{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import threading\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "jumlah_index = 1\n",
    "threads_link = []\n",
    "links = []\n",
    "results = []\n",
    "threads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(page_number):\n",
    "    url = f\"https://jabar.antaranews.com/jabar-terkini/{page_number}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all('article',{\"class\": \"simple-post simple-big clearfix\"})\n",
    "    \n",
    "    page_links = []\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        page_links.append(link)\n",
    "    \n",
    "    print(f\"Scraped {len(page_links)} links from page {page_number}\")\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 9 links from page 1\n",
      "Total Links: 9\n"
     ]
    }
   ],
   "source": [
    "for page_number in range(1, jumlah_index + 1):\n",
    "    thread = threading.Thread(target=lambda p=page_number: links.extend(scrape_links(p)))\n",
    "    thread.start()\n",
    "    threads_link.append(thread)\n",
    "\n",
    "for thread in threads_link:\n",
    "    thread.join()\n",
    "print(\"Total Links:\", len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')        \n",
    "            # Judul Berita\n",
    "            title_elem = soup.find('h1',{\"class\": \"post-title\"})\n",
    "            if title_elem:\n",
    "                title_text = title_elem.text.strip()\n",
    "            else:\n",
    "                title_text = \"Title not found\"\n",
    "            # Author berita\n",
    "            author_elem = soup.find('div', {\"class\": \"tags-wrapper\"})\n",
    "            if author_elem:\n",
    "                author_text = author_elem.find('span')\n",
    "                author_text = author_text.text.split(':')[1]\n",
    "            else:\n",
    "                author_text = \"Author not found\"     \n",
    "            # tanggal berita\n",
    "            date_elem = soup.find('span', {\"class\": \"article-date\"})\n",
    "            if date_elem:\n",
    "                date_text = date_elem.text.strip()\n",
    "            else:\n",
    "                date_text = \"Date not found\"\n",
    "            #     # Category berita\n",
    "            # category_elements = soup.find('ul', {\"class\": \"breadcrumb\"})\n",
    "            # if category_elements:\n",
    "            #     category_text = category_elements.find_all('li')\n",
    "            #     category_text= category_text[2].get_text()\n",
    "            # else:\n",
    "            category_text = \"Category not found\"\n",
    "            #     # Content Berita\n",
    "            body_elem = soup.find('div', {\"class\": \"post-content clearfix\"})\n",
    "            \n",
    "            if body_elem:\n",
    "                content_elem = body_elem.find_all('div')\n",
    "                content_text = \"\"\n",
    "                for p in content_elem:\n",
    "                    content_text += p.text.strip() + \"\\n\"\n",
    "                \n",
    "                if content_text.strip():\n",
    "                    content_text=content_text\n",
    "                else:\n",
    "                    content_text =\"Content not found\"\n",
    "            else:\n",
    "                content_text =\"Content not found\"\n",
    "\n",
    "            results.append({'title': title_text,\n",
    "                            'author' : author_text,\n",
    "                            'category':category_text,\n",
    "                            'date': date_text,\n",
    "                            'content' : content_text,\n",
    "                            'region':'banten',\n",
    "                            'link' : url})\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data from {url}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL '{url}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL '{url}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in links:\n",
    "    thread = threading.Thread(target=scrape_url, args=(url,))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>region</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dibutuhkan Rp1 triliun untuk bangun flyover di...</td>\n",
       "      <td>M Fikri Setiawan</td>\n",
       "      <td>Category not found</td>\n",
       "      <td>Rabu, 27 September 2023 8:30 WIB</td>\n",
       "      <td>Content not found</td>\n",
       "      <td>banten</td>\n",
       "      <td>https://jabar.antaranews.com/berita/470544/dib...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pemprov Jabar terus mengupayakan basmi NII Pan...</td>\n",
       "      <td>Ricky Prayoga</td>\n",
       "      <td>Category not found</td>\n",
       "      <td>Selasa, 26 September 2023 23:00 WIB</td>\n",
       "      <td>12Tampilkan Semua\\n</td>\n",
       "      <td>banten</td>\n",
       "      <td>https://jabar.antaranews.com/berita/470523/pem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Selatan Jawa dan sejumlah wilayah berpotensi a...</td>\n",
       "      <td>Zubi Mahrofi</td>\n",
       "      <td>Category not found</td>\n",
       "      <td>Rabu, 27 September 2023 8:47 WIB</td>\n",
       "      <td>Content not found</td>\n",
       "      <td>banten</td>\n",
       "      <td>https://jabar.antaranews.com/berita/470562/sel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dinas Pertanian Garut minta masyarakat terapka...</td>\n",
       "      <td>Feri Purnama</td>\n",
       "      <td>Category not found</td>\n",
       "      <td>Selasa, 26 September 2023 23:18 WIB</td>\n",
       "      <td>123Tampilkan Semua\\n</td>\n",
       "      <td>banten</td>\n",
       "      <td>https://jabar.antaranews.com/berita/470520/din...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BMKG imbau masyarakat selatan Jawa waspada pot...</td>\n",
       "      <td>Zubi Mahrofi</td>\n",
       "      <td>Category not found</td>\n",
       "      <td>Rabu, 27 September 2023 8:51 WIB</td>\n",
       "      <td>Content not found</td>\n",
       "      <td>banten</td>\n",
       "      <td>https://jabar.antaranews.com/berita/470568/bmk...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title             author  \\\n",
       "0  Dibutuhkan Rp1 triliun untuk bangun flyover di...   M Fikri Setiawan   \n",
       "1  Pemprov Jabar terus mengupayakan basmi NII Pan...      Ricky Prayoga   \n",
       "2  Selatan Jawa dan sejumlah wilayah berpotensi a...       Zubi Mahrofi   \n",
       "3  Dinas Pertanian Garut minta masyarakat terapka...       Feri Purnama   \n",
       "4  BMKG imbau masyarakat selatan Jawa waspada pot...       Zubi Mahrofi   \n",
       "\n",
       "             category                                 date  \\\n",
       "0  Category not found     Rabu, 27 September 2023 8:30 WIB   \n",
       "1  Category not found  Selasa, 26 September 2023 23:00 WIB   \n",
       "2  Category not found     Rabu, 27 September 2023 8:47 WIB   \n",
       "3  Category not found  Selasa, 26 September 2023 23:18 WIB   \n",
       "4  Category not found     Rabu, 27 September 2023 8:51 WIB   \n",
       "\n",
       "                content  region  \\\n",
       "0     Content not found  banten   \n",
       "1   12Tampilkan Semua\\n  banten   \n",
       "2     Content not found  banten   \n",
       "3  123Tampilkan Semua\\n  banten   \n",
       "4     Content not found  banten   \n",
       "\n",
       "                                                link  \n",
       "0  https://jabar.antaranews.com/berita/470544/dib...  \n",
       "1  https://jabar.antaranews.com/berita/470523/pem...  \n",
       "2  https://jabar.antaranews.com/berita/470562/sel...  \n",
       "3  https://jabar.antaranews.com/berita/470520/din...  \n",
       "4  https://jabar.antaranews.com/berita/470568/bmk...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "print(len(results))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# excel_file_name = f'./excel/jabar_antranews_{current_datetime}.xlsx'\n",
    "# df.to_excel(excel_file_name, index=False)\n",
    "\n",
    "# print(f'Data has been saved to {excel_file_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
