{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date, page_number):\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    url = f\"https://jogja.tribunnews.com/index-news?date={current_date}&page={page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    body = soup.find('ul', {\"class\": \"lsi\"})\n",
    "    articles = body.find_all('li', {\"class\": \"ptb15\"})\n",
    "    \n",
    "    links = []\n",
    "    for article in articles:\n",
    "        header = article.find('h3', {\"class\": \"f16 fbo\"})\n",
    "        if header:\n",
    "            link = header.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                link_href = link['href']\n",
    "                links.append(link_href)\n",
    "                \n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    url = f\"https://jogja.tribunnews.com/index-news?date={current_date}&page=\"\n",
    "    response = requests.get(url + \"1\", headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cek = soup.find('div', {\"class\": \"paging\"})\n",
    "    links = cek.find_all('a')\n",
    "    all_link = []\n",
    "    for link in links:\n",
    "        if link and 'href' in link.attrs:\n",
    "            link_href = link['href']\n",
    "            all_link.append(link_href)\n",
    "    last_link = all_link[-1]\n",
    "\n",
    "    # Extract the \"page\" parameter value from the last link\n",
    "    parsed_url = urlparse(last_link)\n",
    "    page_value = parse_qs(parsed_url.query).get('page', [])[0] if 'page' in parse_qs(parsed_url.query) else None\n",
    "    print(page_value)\n",
    "    # Check if page_value is an integer\n",
    "    try:\n",
    "        page_number = int(page_value)\n",
    "    except (TypeError, ValueError):\n",
    "        page_number = 1\n",
    "\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        # Use list comprehension to submit tasks to the thread pool\n",
    "        futures = [executor.submit(scrape_links, current_date, index) for index in range(1, page_number + 1)]\n",
    "\n",
    "        # Collect results from completed futures\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "Scraped 20 links from page 2 url https://jogja.tribunnews.com/index-news?date=2023-11-1&page=2\n",
      "Scraped 20 links from page 5 url https://jogja.tribunnews.com/index-news?date=2023-11-1&page=5\n",
      "Scraped 20 links from page 1 url https://jogja.tribunnews.com/index-news?date=2023-11-1&page=1\n",
      "Scraped 20 links from page 3 url https://jogja.tribunnews.com/index-news?date=2023-11-1&page=3\n",
      "Scraped 20 links from page 4 url https://jogja.tribunnews.com/index-news?date=2023-11-1&page=4\n",
      "Scraped 20 links from page 6 url https://jogja.tribunnews.com/index-news?date=2023-11-1&page=6\n",
      "Scraped 20 links from page 7 url https://jogja.tribunnews.com/index-news?date=2023-11-1&page=7\n",
      "Scraped 20 links from page 8 url https://jogja.tribunnews.com/index-news?date=2023-11-1&page=8\n",
      "Scraped 20 links from page 9 url https://jogja.tribunnews.com/index-news?date=2023-11-1&page=9\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day(\"2023-11-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('h1', {\"id\": \"arttitle\"})\n",
    "                    if title_elem:\n",
    "                        title_text = title_elem.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"   \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('div', {\"class\": \"grey bdr3 pb10 pt10\"})\n",
    "                    if date_elem:\n",
    "                        # date_text = date_elem.text.strip()\n",
    "                        date_text=date_elem.find('time')\n",
    "                        date_text = date_elem.text.strip()\n",
    "                        date_part = ' '.join(date_text.split(',')[1:]).strip()\n",
    "                        date_object = parser.parse(date_part)\n",
    "                        formatted_date = date_object.strftime('%Y-%m-%d')\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"side-article txt-article multi-fontsize editcontent\"})\n",
    "                    \n",
    "                    if body_elem:\n",
    "                        content_elem = body_elem.find_all('p')\n",
    "                        content_text = \"\"\n",
    "                        for p in content_elem:\n",
    "                            content_text += p.text.strip() + \"\\n\"\n",
    "                        \n",
    "                        if content_text.strip():\n",
    "                            content_text=content_text\n",
    "                            content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                            content_text = ' '.join(content_text.split())\n",
    "                        else:\n",
    "                            content_text =\"Content not found\"\n",
    "                    else:\n",
    "                        content_text =\"Content not found\"\n",
    "\n",
    "                    return{\n",
    "                        'title': title_text,\n",
    "                        'date': formatted_date,\n",
    "                        'content':content_text,\n",
    "                        'link' : url}\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://jogja.tribunnews.com/2023/11/23/jalan-panjang-pengungkapan-kasus-pemerasan-syl-hingga-polisi-tetapkan-ketua-kpk-jadi-tersangka'\n",
    "cek=scrape_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Jalan Panjang Pengungkapan Kasus Pemerasan SYL Hingga Polisi Tetapkan Ketua KPK jadi Tersangka', 'date': '2023-11-23', 'content': 'TRIBUNJOGJA.COM, JAKARTA - Setelah melalui sejumlah rangkaian penyelidikan dan penyidikan yang cukup panjang, penyidik Direktorat Kriminal Khusus Polda Metro Jaya akhirnya menetapkan Ketua KPK Firli Bahuri sebagai tersangka kasus dugaan pemerasan terhadap mantan Menteri Pertanian Syahrul Yasin Limpo (SYL).Penetapan tersangka ini diumumkan langsung oleh Direskrimsus Kepolisian Daerah (Polda) Metro Jaya Kombes Ade Safri pada Rabu (22/11/2023) malam.Dalam kasus ini, menurut Ade, polisi telah menyidik dugaan tindak pidana korupsi berupa pemerasan hingga penerimaan gratifikasi di Kementerian Pertanian.\"Berupa penerimaan hadiah atau janji oleh pegawai negeri atau penyelenggara negara terkait dengan penanganan permasalahan hukum di Kementan RI pada kurun waktu 2020-2023,\" ucap Ade di Mapolda Metro Jaya, Rabu (22/11/2023), malam seperti yang dikutip dari Kompas.com.Kasus yang menjerat Ketua KPK ini sebelumnya dilaporkan ke Polda Metro Jaya pada 12 Agustus 2023 silam.Saat itu polisi menerima pengaduan dari masyarakat.Namun hingga saat ini belum diungkapkan siapa sosok yang melaporkan kasus itu ke Polda Metro Jaya.Aduan ini berisi dugaan pemerasan oleh pimpinan KPK, pada perkara korupsi di Kementerian Pertanian (Kementan) pada 2021.Polisi yang menerima laporan kemudian melakukan penyelidikan dan pengumpulan bukti dan meminta keterangan saksi.Baca juga: Kronologi Suami Minum Racun setelah Aniaya Istri di Gunungkidul, Berawal dari Cekcok di KamarSetelah melakukan penyelidikan cukup panjang, polisi akhirnya menaikan statusnya menjadi penyidikan, tepatnya pada 6 Oktober 2023 lalu.Polisi juga meminta keterangan dari Syahrul Yasin Limpo yang saat itu baru saja pulang dari luar negeri.Selain itu, juga sudah meminta keterangan dari Firli Bahuri sebanyak dua kali.Serta memeriksa puluhan saksi lainnya, baik dari pejabat Kementan maupun pejabat di lingkungan KPK.Total sudah ada 91 saksi yang diperiksa oleh penyidik dalam kasus ini.', 'link': 'https://jogja.tribunnews.com/2023/11/23/jalan-panjang-pengungkapan-kasus-pemerasan-syl-hingga-polisi-tetapkan-ketua-kpk-jadi-tersangka'}\n"
     ]
    }
   ],
   "source": [
    "print(cek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
