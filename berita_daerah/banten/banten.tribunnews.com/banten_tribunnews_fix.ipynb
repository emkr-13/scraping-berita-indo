{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date, page_number):\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    url = f\"https://banten.tribunnews.com/index-news?date={current_date}&page={page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    body = soup.find('ul', {\"class\": \"lsi\"})\n",
    "    articles = body.find_all('li', {\"class\": \"ptb15\"})\n",
    "    \n",
    "    links = []\n",
    "    for article in articles:\n",
    "        header = article.find('h3', {\"class\": \"f16 fbo\"})\n",
    "        if header:\n",
    "            link = header.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                link_href = link['href']\n",
    "                links.append(link_href)\n",
    "                \n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    url = f\"https://banten.tribunnews.com/index-news?date={current_date}&page=\"\n",
    "    response = requests.get(url + \"1\", headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cek = soup.find('div', {\"class\": \"paging\"})\n",
    "    links = cek.find_all('a')\n",
    "    all_link = []\n",
    "    for link in links:\n",
    "        if link and 'href' in link.attrs:\n",
    "            link_href = link['href']\n",
    "            all_link.append(link_href)\n",
    "    last_link = all_link[-1]\n",
    "\n",
    "    # Extract the \"page\" parameter value from the last link\n",
    "    parsed_url = urlparse(last_link)\n",
    "    page_value = parse_qs(parsed_url.query).get('page', [])[0] if 'page' in parse_qs(parsed_url.query) else None\n",
    "    print(page_value)\n",
    "    # Check if page_value is an integer\n",
    "    try:\n",
    "        page_number = int(page_value)\n",
    "    except (TypeError, ValueError):\n",
    "        page_number = 1\n",
    "\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        # Use list comprehension to submit tasks to the thread pool\n",
    "        futures = [executor.submit(scrape_links, current_date, index) for index in range(1, page_number + 1)]\n",
    "\n",
    "        # Collect results from completed futures\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Scraped 20 links from page 1 url https://banten.tribunnews.com/index-news?date=2023-11-1&page=1\n",
      "Scraped 9 links from page 5 url https://banten.tribunnews.com/index-news?date=2023-11-1&page=5\n",
      "Scraped 20 links from page 2 url https://banten.tribunnews.com/index-news?date=2023-11-1&page=2\n",
      "Scraped 20 links from page 4 url https://banten.tribunnews.com/index-news?date=2023-11-1&page=4\n",
      "Scraped 20 links from page 3 url https://banten.tribunnews.com/index-news?date=2023-11-1&page=3\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day(\"2023-11-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('h1', {\"id\": \"arttitle\"})\n",
    "                    if title_elem:\n",
    "                        title_text = title_elem.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"   \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('div', {\"class\": \"grey bdr3 pb10 pt10\"})\n",
    "                    if date_elem:\n",
    "                        # date_text = date_elem.text.strip()\n",
    "                        date_text=date_elem.find('time')\n",
    "                        date_text = date_elem.text.strip()\n",
    "                        date_part = ' '.join(date_text.split(',')[1:]).strip()\n",
    "                        date_object = parser.parse(date_part)\n",
    "                        formatted_date = date_object.strftime('%Y-%m-%d')\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"side-article txt-article multi-fontsize editcontent\"})\n",
    "                    \n",
    "                    if body_elem:\n",
    "                        content_elem = body_elem.find_all('p')\n",
    "                        content_text = \"\"\n",
    "                        for p in content_elem:\n",
    "                            content_text += p.text.strip() + \"\\n\"\n",
    "                        \n",
    "                        if content_text.strip():\n",
    "                            content_text=content_text\n",
    "                            content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                            content_text = ' '.join(content_text.split())\n",
    "                        else:\n",
    "                            content_text =\"Content not found\"\n",
    "                    else:\n",
    "                        content_text =\"Content not found\"\n",
    "\n",
    "                    return{\n",
    "                        'title': title_text,\n",
    "                        'date': formatted_date,\n",
    "                        'content':content_text,\n",
    "                        'link' : url}\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cek=scrape_url('https://banten.tribunnews.com/2023/11/22/sidik-eduard-ungkap-alasan-berjualan-cilok-berawal-dari-sepi-job-nggak-ada-pemasukan-sama-sekali')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': \"Sidik Eduard Ungkap Alasan Berjualan Cilok, Berawal dari Sepi Job: 'Nggak Ada Pemasukan Sama Sekali'\", 'date': '2023-11-22', 'content': 'TRIBUNBANTEN.COM - Dikenal sebagai bintang FTV, siapa sangka kini Sidik Eduard berjualan cilok di pinggir jalan.Nama Sidik Eduard belakangan jadi perbincangan hangat di media sosial.Bagaimana tidak, sosok Sidik Eduard yang selama ini dikenal sebagai artis justru kedapatan berjualan cilok di pinggir jalan bersama sang istri.Rupanya, ada kisah mengharukan di balik keputusan Sidik Eduard yang akhirnya berjualan cilok.Sidik Eduard kini berjualan cilok dengan sang istri imbas tidak ada job untuk kembali syuting.Baca juga: Viral Pedagang Cilok Mirip Al Ghazali, Akui Jadi Ramai Pembeli Hingga Banyak yang Minta Nomor HPUsut punya usut, rupanya permasalahan itu berawal dari adegan FTV di Indosiar yang sempat viral di TikTok beberapa waktu lalu.Dalam adegan yang diperankan Sidik Eduard, ia memerakan seseorang yang memiliki tangan yang buntung.Namun, dalam tayangan tersebut, tangan yang seharusnya buntung ini justru tampak terlihat di dalam bajunya.Dari situlah, tawaran job syuting untuknya mulai berkurang.Pengakuan itu dikatakan Sidik Eduard, saat menjadi bintang tamu dalam podcast YouTube kasisolusi, Rabu (22/11/2023).Diakui Sidik Eduard, permasalahan itu beranjak dari tahun musim wabah Covid-19.Menurut Sidik Eduard, wabah Covid juga berimbas didalam kantornya.\"Oke berawal dari sebenarnya covid, covid itu mempengaruhi juga kan produksi kantor.\"\"Setelah covid sebenarnya sebulan sekali tuh masih ada tuh judul buat gua, terus kayak makin ke sini makin sepi,\" ujar Sidik Eduard.', 'link': 'https://banten.tribunnews.com/2023/11/22/sidik-eduard-ungkap-alasan-berjualan-cilok-berawal-dari-sepi-job-nggak-ada-pemasukan-sama-sekali'}\n"
     ]
    }
   ],
   "source": [
    "print(cek)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
