{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date,page_number):\n",
    "    format=datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    formatted_date_string = format.strftime(\"%d-%m-%Y\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = f\"https://banten.inews.id/indeks/all/{formatted_date_string}/{page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('li', {\"class\": \"padding-10px-all\"})\n",
    "    links = []\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        links.append(link)\n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    page_number = 0\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        futures = []\n",
    "\n",
    "        while True:\n",
    "            future = executor.submit(scrape_links, date, page_number)\n",
    "            futures.append(future)\n",
    "            page_number += 12\n",
    "\n",
    "            # Break the loop if no more articles are found\n",
    "            if not future.result():\n",
    "                break\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 0 links from page 0 url https://banten.inews.id/indeks/all/01-11-2023/0\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day('2023-11-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('div', {\"class\": \"title\"})\n",
    "                    if title_elem:\n",
    "                        title_text=title_elem.find('h1')\n",
    "                        title_text = title_text.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"  \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('div', {\"class\": \"date\"})\n",
    "                    # print(date_elem)\n",
    "                    if date_elem:\n",
    "                        date_text= date_elem.text.strip()\n",
    "                        date_text= date_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                        date_text=' '.join(date_text.split())\n",
    "                        match = re.search(r'\\b(\\d{2} \\w+ \\d{4})', date_text)\n",
    "                        if match:\n",
    "                            extracted_date_str = match.group(1)\n",
    "                        date_object = datetime.strptime(extracted_date_str, '%d %B %Y')\n",
    "                        formatted_date = date_object.strftime('%Y-%m-%d')\n",
    "                        # date_text = date_text\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"caption\"})\n",
    "                        \n",
    "                    if body_elem:\n",
    "                        content_elem = body_elem.find_all('p')\n",
    "                        content_text = \"\"\n",
    "                        for p in content_elem:\n",
    "                                content_text += p.text.strip() + \"\\n\"\n",
    "                            \n",
    "                        if content_text.strip():\n",
    "                            content_text=content_text\n",
    "                            content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                            content_text = ' '.join(content_text.split())\n",
    "                        else:\n",
    "                            content_text=\"Content not found\"\n",
    "                    else:\n",
    "                            content_text=\"Content not found\"\n",
    "\n",
    "                    return{\n",
    "                        'title': title_text,\n",
    "                        'date': formatted_date,\n",
    "                        'content':content_text,\n",
    "                        'link' : url}\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://banten.inews.id/read/365106/lurah-kubangsari-pembangunan-yang-sudah-ada-tolong-dirawat-agar-terus-dirasakan-oleh-masyarakat'\n",
    "data_inews = scrape_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Lurah Kubangsari : Pembangunan yang Sudah Ada Tolong Dirawat Agar Terus Dirasakan Oleh Masyarakat', 'date': '2023-11-01', 'content': 'CILEGON, iNewsBanten - Lurah Kelurahan Kubangsari didampingi Kasie Pembangunan, Tim Pendukung Kotaku, bhabinkamtibmas Polsek Ciwandan dan Koramil Ciwandan melakukan Opname atau pengukuran hasil kegiatan Pokmas Kubangsari termin akhir, Rabu (01/11/2023)\"Untuk kegiatan Pokmas Kubangsari ditermin akhir ini, kita melakukan pembangunan sebanyak empat (4) titik salah satunya adalah Drainase di Lingkungan Kebanjiran RT 03/02, kemudian kita ada pembangunan Tembok Penahan Tanah (TPT) dan Drainase ditempat yang sama yaitu di Pagebangan RT 01/03 dan yang terakhir pembangunan Drainase di Lingkungan Kubang Welut RT 04/04 Kata Fadila Ketua Pokmas Kubangsari saat diwawancarai iNewsBanten.\"Saya Pokmas mewakili masyarakat Kelurahan Kubangsari sangat berterima kasih kepada (Pemkot) Pemerintah Kota Cilegon khususnya Pemerintah Kelurahan yang sudah banyak memberikan program Lingkungan, karena dengan adanya kegiatan DPWKel Salira terus terang banyak warga yang merasa sangat terbantu dengan adanya program ini,\" Ucap Fadila.', 'link': 'https://banten.inews.id/read/365106/lurah-kubangsari-pembangunan-yang-sudah-ada-tolong-dirawat-agar-terus-dirasakan-oleh-masyarakat'}\n"
     ]
    }
   ],
   "source": [
    "print(data_inews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
