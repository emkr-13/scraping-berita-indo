{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapping tribunnews Jakarta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date, page_number):\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    url = f\"https://jakarta.tribunnews.com/index-news?date={current_date}&page={page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    body = soup.find('ul', {\"class\": \"lsi\"})\n",
    "    articles = body.find_all('li', {\"class\": \"ptb15\"})\n",
    "    \n",
    "    links = []\n",
    "    for article in articles:\n",
    "        header = article.find('h3', {\"class\": \"f16 fbo\"})\n",
    "        if header:\n",
    "            link = header.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                link_href = link['href']\n",
    "                links.append(link_href)\n",
    "                \n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    url = f\"https://jakarta.tribunnews.com/index-news?date={current_date}&page=\"\n",
    "    response = requests.get(url + \"1\", headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cek = soup.find('div', {\"class\": \"paging\"})\n",
    "    links = cek.find_all('a')\n",
    "    all_link = []\n",
    "    for link in links:\n",
    "        if link and 'href' in link.attrs:\n",
    "            link_href = link['href']\n",
    "            all_link.append(link_href)\n",
    "    last_link = all_link[-1]\n",
    "\n",
    "    # Extract the \"page\" parameter value from the last link\n",
    "    parsed_url = urlparse(last_link)\n",
    "    page_value = parse_qs(parsed_url.query).get('page', [])[0] if 'page' in parse_qs(parsed_url.query) else None\n",
    "    print(page_value)\n",
    "    # Check if page_value is an integer\n",
    "    try:\n",
    "        page_number = int(page_value)\n",
    "    except (TypeError, ValueError):\n",
    "        page_number = 1\n",
    "\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        # Use list comprehension to submit tasks to the thread pool\n",
    "        futures = [executor.submit(scrape_links, current_date, index) for index in range(1, page_number + 1)]\n",
    "\n",
    "        # Collect results from completed futures\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "Scraped 20 links from page 2 url https://jakarta.tribunnews.com/index-news?date=2023-11-1&page=2\n",
      "Scraped 20 links from page 3 url https://jakarta.tribunnews.com/index-news?date=2023-11-1&page=3\n",
      "Scraped 20 links from page 1 url https://jakarta.tribunnews.com/index-news?date=2023-11-1&page=1\n",
      "Scraped 20 links from page 4 url https://jakarta.tribunnews.com/index-news?date=2023-11-1&page=4\n",
      "Scraped 3 links from page 5 url https://jakarta.tribunnews.com/index-news?date=2023-11-1&page=5\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day(\"2023-11-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('h1', {\"id\": \"arttitle\"})\n",
    "                    if title_elem:\n",
    "                        title_text = title_elem.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"   \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('div', {\"class\": \"grey bdr3 pb10 pt10\"})\n",
    "                    if date_elem:\n",
    "                        # date_text = date_elem.text.strip()\n",
    "                        date_text=date_elem.find('time')\n",
    "                        date_text = date_elem.text.strip()\n",
    "                        date_part = ' '.join(date_text.split(',')[1:]).strip()\n",
    "                        date_object = parser.parse(date_part)\n",
    "                        formatted_date = date_object.strftime('%Y-%m-%d')\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"side-article txt-article multi-fontsize editcontent\"})\n",
    "                    \n",
    "                    if body_elem:\n",
    "                        content_elem = body_elem.find_all('p')\n",
    "                        content_text = \"\"\n",
    "                        for p in content_elem:\n",
    "                            content_text += p.text.strip() + \"\\n\"\n",
    "                        \n",
    "                        if content_text.strip():\n",
    "                            content_text=content_text\n",
    "                            content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                            content_text = ' '.join(content_text.split())\n",
    "                        else:\n",
    "                            content_text =\"Content not found\"\n",
    "                    else:\n",
    "                        content_text =\"Content not found\"\n",
    "\n",
    "                    return{\n",
    "                        'title': title_text,\n",
    "                        'date': formatted_date,\n",
    "                        'content':content_text,\n",
    "                        'link' : url}\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cek=scrape_url('https://jakarta.tribunnews.com/2023/11/01/kronologi-mertua-bunuh-menantu-hamil-7-bulan-suami-korban-syok-pulang-interview-buka-pintu-kamar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Kronologi Mertua Bunuh Menantu Hamil 7 Bulan, Suami Korban Syok Pulang Interview Buka Pintu Kamar', 'date': '2023-11-01', 'content': 'TRIBUNJAKARTA.COM - Seorang suami bernama Sueb syok saat melihat istrinya Fitria Almuniroh Hafidloh Diana (23) tewas bersimbah darah di dalam kamarnya.Fitria yang tengah hamil 7 bulan ternyata dibunuh oleh mertuanya sendiri Khoiri alias Satir (53).Wakapolres Pasuruan Kompol Hari Aziz membeberkan kronologi pembunuhan sadis tersebut.Hari awalnya menjelaskan Sueb, Khoiri, dan Fitria, tinggal serumah di Dusun Blimbing, Desa Parerejo, Kecamatan Purwodadi, Kabupaten Pasuruan, Jawa Timur.Khoiri sendiri merupakan duda, istrinya meninggal dunia beberapa waktu yang lalu.Saat peristiwa pembunuhan terjadi Khoiri, dan Fitria hanya berdua di rumah, sementara Sueb sedang interview kerja.\"Mereka itu tinggal satu rumah, lalu saat putranya mencari pekerjaan atau interview,\" ucap Hari dikutip TribunJakarta dari YouTube Tv One, pada Rabu (1/11/2023).\"Korban di rumah dengan tersangka,\" imbuhnya.Kapolsek Purwodadi, AKP Pujianto mengatakan saat Fitria berada di kamarnya pelaku mendadak mendatangi menantunya tersebut.Lalu dengan menggunakan sebilah pisau dapur ia melukai leher korban.\"Kejadian pembunuhan di dalam kamar rumah suami korban dengan menggunakan sebilah pisau dapur dengan cara menggorok leher korban,\" tutur Kapolsek Purwodadi AKP Pujianto.Saat selesai interview Sueb pulang ke rumah.Di rumah, ia mendapati kamar istrinya terkunci.Merasa panik, Sueb langsung mendobrak pintu kamarnya.', 'link': 'https://jakarta.tribunnews.com/2023/11/01/kronologi-mertua-bunuh-menantu-hamil-7-bulan-suami-korban-syok-pulang-interview-buka-pintu-kamar'}\n"
     ]
    }
   ],
   "source": [
    "print(cek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
