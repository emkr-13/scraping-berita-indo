{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date,page_number):\n",
    "    format=datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    formatted_date_string = format.strftime(\"%Y-%m-%d\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = f\"https://surabaya.inews.id/indeks/all/{formatted_date_string}/{page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', {\"class\": \"box-list-news\"})\n",
    "    links = []\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        links.append(link)\n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "    \n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    page_number = 0\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        futures = []\n",
    "\n",
    "        while True:\n",
    "            future = executor.submit(scrape_links, date, page_number)\n",
    "            futures.append(future)\n",
    "            page_number += 12\n",
    "\n",
    "            # Break the loop if no more articles are found\n",
    "            if not future.result():\n",
    "                break\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 12 links from page 0 url https://surabaya.inews.id/indeks/all/2023-11-01/0\n",
      "Scraped 10 links from page 12 url https://surabaya.inews.id/indeks/all/2023-11-01/12\n",
      "Scraped 0 links from page 24 url https://surabaya.inews.id/indeks/all/2023-11-01/24\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day('2023-11-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('div', {\"class\": \"title\"})\n",
    "                    if title_elem:\n",
    "                        title_text=title_elem.find('h1')\n",
    "                        title_text = title_text.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"  \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('div', {\"class\": \"date\"})\n",
    "                    # print(date_elem)\n",
    "                    if date_elem:\n",
    "                        date_text= date_elem.text.strip()\n",
    "                        date_text= date_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                        date_text=' '.join(date_text.split())\n",
    "                        match = re.search(r'\\b(\\d{2} \\w+ \\d{4})', date_text)\n",
    "                        if match:\n",
    "                            extracted_date_str = match.group(1)\n",
    "                        date_object = datetime.strptime(extracted_date_str, '%d %B %Y')\n",
    "                        formatted_date = date_object.strftime('%Y-%m-%d')\n",
    "                        # date_text = date_text\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"caption\"})\n",
    "                        \n",
    "                    if body_elem:\n",
    "                        content_elem = body_elem.find_all('p')\n",
    "                        content_text = \"\"\n",
    "                        for p in content_elem:\n",
    "                                content_text += p.text.strip() + \"\\n\"\n",
    "                            \n",
    "                        if content_text.strip():\n",
    "                            content_text=content_text\n",
    "                            content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                            content_text = ' '.join(content_text.split())\n",
    "                        else:\n",
    "                            content_text=\"Content not found\"\n",
    "                    else:\n",
    "                            content_text=\"Content not found\"\n",
    "\n",
    "                    return{\n",
    "                        'title': title_text,\n",
    "                        'date': formatted_date,\n",
    "                        'content':content_text,\n",
    "                        'link' : url}\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adj|ust the delay as needed\n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://surabaya.inews.id/read/365197/bpjamsostek-surabaya-karimunjawa-serahkan-jkm-kepada-ahli-waris-pegawai-non-asn-ppls'\n",
    "data_inews = scrape_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'BPJamsostek Surabaya Karimunjawa Serahkan JKM Kepada Ahli Waris Pegawai Non ASN PPLS', 'date': '2023-11-01', 'content': 'SURABAYA, iNews.id – BPJS Ketenagakerjaan (BPJamsostek) Surabaya Karimunjawa menyerahkan santunan klaim Jaminan Kematian (JKM) kepada ahli waris pegawai Non ASN Pusat Pengendalian Lumpur Sidoarjo (PPLS). Simbolis penyerahan dilakukan bersamaan dengan kegiatan Sosialisasi Manfaat Program BPJS Ketenagakerjaan, di Kantor BA Porong PPLS, Rabu (01/11/2023).Selain penyerahan klaim JKM, pada momen ini BPJamsostek Surabaya Karimunjawa juga menyerahkan simbolis kartu peserta BPJS Ketenagakerjaan kepada pegawai Swakelola Pusat Pengendalian Lumpur Sidoarjo.Hadir dalam kegiatan ini Kepala Cabang BPJS Ketenagakerjaan Surabaya Karimunjawa, Adventus Edison Souhuwat, Kepala Bagian Tata Usaha PPLS, Hikmad Batara Reza dan seluruh pekerja Swakelola PPLS.Sonny panggilan akrab Adventus Edison Souhuwat menyampaikan duka cita atas meninggalnya salah satu pegawai Non ASN PPLS“Semoga keluarga yang ditinggalkan diberikan penghiburan dan juga santunan yang diberikan dapat berguna bagi keluarga,” ucapnya.Sonny bilang, semua pekerja di PPLS sudah dilindungi program Jaminan Kecelakaan Kerja(JKK), dan Jaminan Kematian (JKM) BPJS Ketenagakerjaan.Namun ia berharap, tidak hanya pekerja di PPLS saja yang terlidungi program BPJamsosten, tapi istri atau suami maupun saudara yang bekerja bisa didaftarkan pula program BPJS Ketenagakerjaan.Cara daftarnya, kata Sonny tidak sulit. Cukup datang ke kantor cabang terdekat atau melalui aplikasi JMO (Jamsostek Mobile) setelah mengunduh aplikasi tersebut.“Karena sudah banyak bukti, negara hadir melalui BPJS Ketenagakerjaan. Ketika peserta mengalami kecelakaan kerja akan mendapatkan pengobatan sampai dengan sembuh. Dan ketika meninggal dunia di luar aktivitas kerja, akan mendapatkan santunan jaminan kematian sebesar Rp42 juta,” terang Sonny.Sementara menurut Kepala Bagian Tata Usaha PPLS, Hikmad Batara Reza, BPJS Ketenagakerjaan sangat perlu bagi semua pekerja di lingkup Pusat pengendalian Lumpur Sidoarjo. Untuk itu, sosialisasi dilakukan agar mereka tahu manfaat program dari BPJS Ketenagakerjaan.Dalam kesempatan ini, selain dilakukan sosialisasi terkait program dan manfaaat, juga dilakukan diskusi tanya jawab terkait ruang lingkup manfaat dan program BPJS Ketenagakerjaan.', 'link': 'https://surabaya.inews.id/read/365197/bpjamsostek-surabaya-karimunjawa-serahkan-jkm-kepada-ahli-waris-pegawai-non-asn-ppls'}\n"
     ]
    }
   ],
   "source": [
    "print(data_inews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
