{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date, page_number):\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    url = f\"https://jatim.tribunnews.com/index-news?date={current_date}&page={page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    body = soup.find('ul', {\"class\": \"lsi\"})\n",
    "    articles = body.find_all('li', {\"class\": \"ptb15\"})\n",
    "    \n",
    "    links = []\n",
    "    for article in articles:\n",
    "        header = article.find('h3', {\"class\": \"f16 fbo\"})\n",
    "        if header:\n",
    "            link = header.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                link_href = link['href']\n",
    "                links.append(link_href)\n",
    "                \n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    url = f\"https://jatim.tribunnews.com/index-news?date={current_date}&page=\"\n",
    "    response = requests.get(url + \"1\", headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cek = soup.find('div', {\"class\": \"paging\"})\n",
    "    links = cek.find_all('a')\n",
    "    all_link = []\n",
    "    for link in links:\n",
    "        if link and 'href' in link.attrs:\n",
    "            link_href = link['href']\n",
    "            all_link.append(link_href)\n",
    "    last_link = all_link[-1]\n",
    "\n",
    "    # Extract the \"page\" parameter value from the last link\n",
    "    parsed_url = urlparse(last_link)\n",
    "    page_value = parse_qs(parsed_url.query).get('page', [])[0] if 'page' in parse_qs(parsed_url.query) else None\n",
    "    print(page_value)\n",
    "    # Check if page_value is an integer\n",
    "    try:\n",
    "        page_number = int(page_value)\n",
    "    except (TypeError, ValueError):\n",
    "        page_number = 1\n",
    "\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        # Use list comprehension to submit tasks to the thread pool\n",
    "        futures = [executor.submit(scrape_links, current_date, index) for index in range(1, page_number + 1)]\n",
    "\n",
    "        # Collect results from completed futures\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "Scraped 20 links from page 1 url https://jatim.tribunnews.com/index-news?date=2023-11-1&page=1\n",
      "Scraped 20 links from page 2 url https://jatim.tribunnews.com/index-news?date=2023-11-1&page=2\n",
      "Scraped 20 links from page 5 url https://jatim.tribunnews.com/index-news?date=2023-11-1&page=5\n",
      "Scraped 20 links from page 4 url https://jatim.tribunnews.com/index-news?date=2023-11-1&page=4\n",
      "Scraped 20 links from page 3 url https://jatim.tribunnews.com/index-news?date=2023-11-1&page=3\n",
      "Scraped 20 links from page 6 url https://jatim.tribunnews.com/index-news?date=2023-11-1&page=6\n",
      "Scraped 20 links from page 7 url https://jatim.tribunnews.com/index-news?date=2023-11-1&page=7\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day(\"2023-11-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('h1', {\"id\": \"arttitle\"})\n",
    "                    if title_elem:\n",
    "                        title_text = title_elem.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"   \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('div', {\"class\": \"grey bdr3 pb10 pt10\"})\n",
    "                    if date_elem:\n",
    "                        # date_text = date_elem.text.strip()\n",
    "                        date_text=date_elem.find('time')\n",
    "                        date_text = date_elem.text.strip()\n",
    "                        date_part = ' '.join(date_text.split(',')[1:]).strip()\n",
    "                        date_object = parser.parse(date_part)\n",
    "                        formatted_date = date_object.strftime('%Y-%m-%d')\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"side-article txt-article multi-fontsize editcontent\"})\n",
    "                    \n",
    "                    if body_elem:\n",
    "                        content_elem = body_elem.find_all('p')\n",
    "                        content_text = \"\"\n",
    "                        for p in content_elem:\n",
    "                            content_text += p.text.strip() + \"\\n\"\n",
    "                        \n",
    "                        if content_text.strip():\n",
    "                            content_text=content_text\n",
    "                            content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                            content_text = ' '.join(content_text.split())\n",
    "                        else:\n",
    "                            content_text =\"Content not found\"\n",
    "                    else:\n",
    "                        content_text =\"Content not found\"\n",
    "\n",
    "                    return{\n",
    "                        'title': title_text,\n",
    "                        'date': formatted_date,\n",
    "                        'content':content_text,\n",
    "                        'link' : url}\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://jatim.tribunnews.com/2023/11/23/tiktok-shop-bakal-kembali-di-indonesia-putuskan-gabung-dengan-goto-demi-regulasi'\n",
    "cek=scrape_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'TikTok Shop Bakal Kembali di Indonesia, Putuskan Gabung dengan GoTo Demi Regulasi', 'date': '2023-11-23', 'content': 'TRIBUNJATIM.COM - TikTok Shop akan kembali membuka layanannya di Indonesia.Namun kali ini, TikTok disebut akan bergabung dengan PT GoTo Gojek Tokopedia Tbk (GOTO) atau GoTo.Diketahui, TikTok Shop sempat dilarang beroperasi akibat regulasi di Indonesia.Keputusan untuk bergabung dengan GoTo itu dilakukan TikTok Shop untuk mengatasi aturan itu.Baca juga: Nasib TikTok Shop, Dikabarkan Buka Kembali di Indonesia, Kemenkop UKM Sebut Kemungkinan, Gabung?Dikutip dari Bloomberg, Kamis (23/11/2023), rencana merger itu akan diumumkan dalam waktu yang dekat.Aksi merger itu pun dilakukan untuk mengatasi hambatan regulasi yang memungkinkan TikTok menghidupkan kembali layanan belanja online di arena ritel terbesar di Asia Tenggara.Adapun Kementerian Perdagangan resmi mengeluarkan Permendag Nomor 31 Tahun 2023 tentang aturan penjualan online. Dalam baleid itu, social commerce dalam hal ini adalah TikTok dilarang membuka usaha dagangnya.Dengan larangan itu pun mau tak mau TikTok langsung menghentikan belanja online-nya di Indonesia untuk mematuhi pembatasan tersebut.\"Pertimbangan untuk mencapai kesepakatan merger itu masih bisa gagal,\" kata sumber Bloomberg.TikTok tidak menanggapi permintaan komentar, sedangkan perwakilan GoTo menolak berkomentar.Dimiliki oleh ByteDance yang berbasis di Beijing, TikTok telah mencoba memetakan jalur baru untuk fitur dengan pertumbuhan tercepat, TikTok Shop, di negara berpenduduk 278 juta jiwa yang seharusnya menjadi contoh ekspansi global dari AS hingga Eropa.Bagi GoTo, perusahaan internet terbesar di Indonesia, kesepakatan dengan TikTok bisa berisiko karena akan membantu pesaing ritel online terbesarnya untuk tetap beroperasi di negara tersebut.Namun, hal ini juga akan memberikan GoTo mitra media sosial global yang kuat dalam sebuah perjanjian yang dapat meningkatkan volume belanja dan pembayaran untuk keduanya TikTok telah berupaya melibatkan pejabat pemerintah dan perusahaan media sosial lainnya untuk mencari cara memulai kembali operasi e-commerce di negara tersebut.Diberitakan sebelumnya, sinyal dibukanya layanan dagangnya TikTok yakni TikTok Shop di Tanah Air semakin jelas.', 'link': 'https://jatim.tribunnews.com/2023/11/23/tiktok-shop-bakal-kembali-di-indonesia-putuskan-gabung-dengan-goto-demi-regulasi'}\n"
     ]
    }
   ],
   "source": [
    "print(cek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
