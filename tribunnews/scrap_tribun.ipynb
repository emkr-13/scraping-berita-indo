{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from urllib.request import Request, urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Link 20\n"
     ]
    }
   ],
   "source": [
    "links=[]\n",
    "# jumlah Page yang diambil \n",
    "jumlah_index = 1\n",
    "for index in range(jumlah_index):\n",
    "    url = f\"https://www.tribunnews.com/2023/09?page={index+1}\"\n",
    "    # print(url)  \n",
    "    hdr = {'User-Agent': 'Chrome/117.0.0.0'}\n",
    "    req = Request(url,headers=hdr)\n",
    "    page = urlopen(req)\n",
    "    soup = BeautifulSoup(page)\n",
    "    articles = soup.find_all('li', {\"class\": \"ptb15\"})\n",
    "    for article in articles:\n",
    "            header = article.find('h3', {\"class\": \"fbo f16\"})\n",
    "            # print(header)   \n",
    "            link = header.find('a')['href']\n",
    "            links.append(link)\n",
    "    \n",
    "print(\"Jumlah Link\",len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = []\n",
    "author = []\n",
    "editor =[]\n",
    "date = []\n",
    "content = []\n",
    "\n",
    "for link in links:\n",
    "    url = link \n",
    "    hdr = {'User-Agent': 'Chrome/117.0.0.0'}\n",
    "    req = Request(url,headers=hdr)\n",
    "    page = urlopen(req)  \n",
    "    soup = BeautifulSoup(page)\n",
    "    \n",
    "    # Title Berita\n",
    "    title_elem = soup.find('h1', {\"id\": \"arttitle\"})\n",
    "    if title_elem:\n",
    "        \n",
    "        title_text = title_elem.text.strip()\n",
    "        # print(title_text)\n",
    "    else:\n",
    "        title_text = \"Title not found\"\n",
    "    title.append(title_text)\n",
    "    \n",
    "    # Author Berita\n",
    "    author_elem = soup.find('div', {\"id\": \"penulis\"})\n",
    "    if author_elem:\n",
    "        # print(author_elem)\n",
    "        penulis = author_elem.find('a')\n",
    "        author_text =penulis.text.strip()\n",
    "    else:\n",
    "        author_text = \"Author not found\"\n",
    "    author.append(author_text)\n",
    "    \n",
    "    # Editor Berita\n",
    "    editor_elem = soup.find('div', {\"id\": \"editor\"})\n",
    "    # editors_elem = editor_elem.find('a')\n",
    "    if editor_elem:\n",
    "        editor = editor_elem.find('a')\n",
    "        editor_text = editor.text.strip()\n",
    "    else:\n",
    "        editor_text = \"editor not found\"\n",
    "    editor.append(editor_text)\n",
    "    \n",
    "    # date berita\n",
    "    date_elem = soup.find('time', {\"class\": \"grey\"})\n",
    "    if date_elem:\n",
    "        date_text = date_elem.text.strip()\n",
    "    else:\n",
    "        date_text = \"Date not found\"\n",
    "    date.append(date_text)\n",
    "    \n",
    "    \n",
    "    # content berita\n",
    "    body_elem = soup.find('div', {\"class\": \"side-article txt-article multi-fontsize\"})\n",
    "\n",
    "    if body_elem:\n",
    "        content_elem = body_elem.find_all('p')\n",
    "        content_text = \"\"\n",
    "        for p in content_elem:\n",
    "            content_text += p.text.strip() + \"\\n\"\n",
    "            \n",
    "        if content_text.strip():\n",
    "            content.append(content_text)\n",
    "        else:\n",
    "            content.append(\"Content not found\")\n",
    "    else:\n",
    "        content.append(\"Content not found\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "20\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(title))\n",
    "print(len(author))\n",
    "print(len(date))\n",
    "print(len(content))\n",
    "print(len(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (2) does not match length of index (20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/krisna/Crawling/crawler_berita_indo/tribunnews/scrap_tribun.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/tribunnews/scrap_tribun.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/tribunnews/scrap_tribun.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m: title,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/tribunnews/scrap_tribun.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mauthor\u001b[39m\u001b[39m'\u001b[39m : author,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/tribunnews/scrap_tribun.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m : links\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/tribunnews/scrap_tribun.ipynb#W5sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         }\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/tribunnews/scrap_tribun.ipynb#W5sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/tribunnews/scrap_tribun.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m: index, \u001b[39m\"\u001b[39m\u001b[39mcolumns\u001b[39m\u001b[39m\"\u001b[39m: columns}, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[39m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[39m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39mcopy, typ\u001b[39m=\u001b[39mmanager)\n\u001b[1;32m    710\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ma\u001b[39m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mma\u001b[39;00m \u001b[39mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[39m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(x, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39melse\u001b[39;00m x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[39mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[39m=\u001b[39mdtype, typ\u001b[39m=\u001b[39mtyp, consolidate\u001b[39m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:120\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    117\u001b[0m         index \u001b[39m=\u001b[39m ensure_index(index)\n\u001b[1;32m    119\u001b[0m     \u001b[39m# don't force copy because getting jammed in an ndarray anyway\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     arrays, refs \u001b[39m=\u001b[39m _homogenize(arrays, index, dtype)\n\u001b[1;32m    121\u001b[0m     \u001b[39m# _homogenize ensures\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[39m#  - all(len(x) == len(index) for x in arrays)\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[39m#  - all(x.ndim == 1 for x in arrays)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m \n\u001b[1;32m    127\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     index \u001b[39m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/construction.py:608\u001b[0m, in \u001b[0;36m_homogenize\u001b[0;34m(data, index, dtype)\u001b[0m\n\u001b[1;32m    605\u001b[0m         val \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mfast_multiget(val, oindex\u001b[39m.\u001b[39m_values, default\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mnan)\n\u001b[1;32m    607\u001b[0m     val \u001b[39m=\u001b[39m sanitize_array(val, index, dtype\u001b[39m=\u001b[39mdtype, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m--> 608\u001b[0m     com\u001b[39m.\u001b[39mrequire_length_match(val, index)\n\u001b[1;32m    609\u001b[0m     refs\u001b[39m.\u001b[39mappend(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    611\u001b[0m homogenized\u001b[39m.\u001b[39mappend(val)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/common.py:576\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 576\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    577\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    578\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (2) does not match length of index (20)"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "        'title': title,\n",
    "        'author' : author,\n",
    "        'editor' : editor,\n",
    "        'date': date,\n",
    "        'content' : content,\n",
    "        'link' : links\n",
    "        }\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tempat save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to ./excel/tribunews_2023-09-25_11-55-20.xlsx\n"
     ]
    }
   ],
   "source": [
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "excel_file_name = f'./excel/tribunews_{current_datetime}.xlsx'\n",
    "df.to_excel(excel_file_name, index=False)\n",
    "\n",
    "print(f'Data has been saved to {excel_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Buat Latihan dan pengecek\n",
    "# links=[]\n",
    "# # jumlah Page yang diambil \n",
    "# jumlah_index = 2\n",
    "# for index in range(jumlah_index):\n",
    "#     url = f\"https://www.tribunnews.com/2023/09?page={index+1}\"\n",
    "#     # print(url)  \n",
    "#     hdr = {'User-Agent': 'Chrome/117.0.0.0'}\n",
    "#     req = Request(url,headers=hdr)\n",
    "#     page = urlopen(req)\n",
    "#     soup = BeautifulSoup(page)\n",
    "#     articles = soup.find_all('li', {\"class\": \"ptb15\"})\n",
    "#     # print(articles)\n",
    "#     for article in articles:\n",
    "#             header = article.find('h3', {\"class\": \"fbo f16\"})\n",
    "#             link = header.find('a')['href']\n",
    "#             links.append(link)\n",
    "    \n",
    "# print(len(links))\n",
    "# print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rinanda DwiYuliawati\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.tribunnews.com/seleb/2023/09/25/lama-tak-muncul-di-layar-kaca-andi-arsyil-mengaku-tak-takut-dilupakan-oleh-fans-gak-butuh-validasi\"\n",
    "    # print(url)  \n",
    "hdr = {'User-Agent': 'Chrome/117.0.0.0'}\n",
    "req = Request(url,headers=hdr)\n",
    "page = urlopen(req)\n",
    "soup = BeautifulSoup(page)\n",
    "\n",
    "# title berita\n",
    "# articles = soup.find('h1', {\"id\": \"arttitle\"})\n",
    "# print(articles.text.strip())\n",
    "\n",
    "# authors berita\n",
    "articles = soup.find('div', {\"id\": \"penulis\"})\n",
    "penulis = articles.find('a')\n",
    "print(penulis.text.strip())\n",
    "\n",
    "# editor berita\n",
    "# articles = soup.find('div', {\"id\": \"editor\"})\n",
    "# penulis = articles.find('a')\n",
    "# print(penulis.text.strip())\n",
    "\n",
    "# isi berita \n",
    "# articles = soup.find('div', {\"class\": \"side-article txt-article multi-fontsize\"})\n",
    "# body_elem = soup.find('div', {\"class\": \"side-article txt-article multi-fontsize\"})\n",
    "\n",
    "# if body_elem:\n",
    "#     content_elem = body_elem.find_all('p')\n",
    "#     content_text = \"\"\n",
    "#     for p in content_elem:\n",
    "#         content_text += p.text.strip() + \"\\n\"\n",
    "        \n",
    "#     if content_text.strip():\n",
    "#         content.append(content_text)\n",
    "#     else:\n",
    "#         content.append(\"Content not found\")\n",
    "# else:\n",
    "#     content.append(\"Content not found\")\n",
    "\n",
    "# print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
