{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "jumlah_index = 200\n",
    "threads_link = []\n",
    "links = []\n",
    "results = []\n",
    "threads = []\n",
    "keywords='anies'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(page_number,keywords):\n",
    "    url = f\"https://www.cnbcindonesia.com/search?query={keywords}&p={page_number}&kanal=&tipe=&date=\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all('article')\n",
    "    \n",
    "    page_links = []\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        page_links.append(link)\n",
    "    \n",
    "    print(f\"Scraped {len(page_links)} links from page {page_number}\")\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 12 links from page 26\n",
      "Scraped 12 links from page 31\n",
      "Scraped 12 links from page 17\n",
      "Scraped 12 links from page 29\n",
      "Scraped 12 links from page 25\n",
      "Scraped 12 links from page 23\n",
      "Scraped 12 links from page 20\n",
      "Scraped 12 links from page 35\n",
      "Scraped 12 links from page 15\n",
      "Scraped 12 links from page 37\n",
      "Scraped 12 links from page 36\n",
      "Scraped 12 links from page 30\n",
      "Scraped 12 links from page 24\n",
      "Scraped 12 links from page 45\n",
      "Scraped 12 links from page 28\n",
      "Scraped 12 links from page 4\n",
      "Scraped 12 links from page 43\n",
      "Scraped 12 links from page 21\n",
      "Scraped 12 links from page 18\n",
      "Scraped 12 links from page 52\n",
      "Scraped 12 links from page 57\n",
      "Scraped 12 links from page 64\n",
      "Scraped 12 links from page 86\n",
      "Scraped 12 links from page 94\n",
      "Scraped 12 links from page 75\n",
      "Scraped 12 links from page 59\n",
      "Scraped 12 links from page 90\n",
      "Scraped 12 links from page 83\n",
      "Scraped 12 links from page 67\n",
      "Scraped 12 links from page 88\n",
      "Scraped 12 links from page 77\n",
      "Scraped 12 links from page 53\n",
      "Scraped 12 links from page 78\n",
      "Scraped 12 links from page 22\n",
      "Scraped 12 links from page 66\n",
      "Scraped 12 links from page 72\n",
      "Scraped 12 links from page 33\n",
      "Scraped 12 links from page 81\n",
      "Scraped 12 links from page 42\n",
      "Scraped 12 links from page 34\n",
      "Scraped 12 links from page 131\n",
      "Scraped 12 links from page 27\n",
      "Scraped 12 links from page 60\n",
      "Scraped 12 links from page 8\n",
      "Scraped 12 links from page 89\n",
      "Scraped 12 links from page 40\n",
      "Scraped 12 links from page 41\n",
      "Scraped 12 links from page 76\n",
      "Scraped 12 links from page 3\n",
      "Scraped 12 links from page 62\n",
      "Scraped 12 links from page 50\n",
      "Scraped 12 links from page 55\n",
      "Scraped 12 links from page 12\n",
      "Scraped 12 links from page 101\n",
      "Scraped 12 links from page 48\n",
      "Scraped 12 links from page 38\n",
      "Scraped 12 links from page 47\n",
      "Scraped 12 links from page 2\n",
      "Scraped 12 links from page 13\n",
      "Scraped 12 links from page 6\n",
      "Scraped 12 links from page 63\n",
      "Scraped 12 links from page 61\n",
      "Scraped 12 links from page 54\n",
      "Scraped 12 links from page 11\n",
      "Scraped 12 links from page 7\n",
      "Scraped 12 links from page 14\n",
      "Scraped 12 links from page 44\n",
      "Scraped 12 links from page 107\n",
      "Scraped 12 links from page 80\n",
      "Scraped 12 links from page 105\n",
      "Scraped 12 links from page 19\n",
      "Scraped 12 links from page 51\n",
      "Scraped 12 links from page 113\n",
      "Scraped 12 links from page 74\n",
      "Scraped 12 links from page 1\n",
      "Scraped 12 links from page 111\n",
      "Scraped 12 links from page 102\n",
      "Scraped 12 links from page 56\n",
      "Scraped 12 links from page 120\n",
      "Scraped 12 links from page 135\n",
      "Scraped 12 links from page 46\n",
      "Scraped 12 links from page 9\n",
      "Scraped 12 links from page 5\n",
      "Scraped 12 links from page 133\n",
      "Scraped 12 links from page 70\n",
      "Scraped 12 links from page 125\n",
      "Scraped 12 links from page 175\n",
      "Scraped 12 links from page 49\n",
      "Scraped 12 links from page 122\n",
      "Scraped 12 links from page 10\n",
      "Scraped 12 links from page 39\n",
      "Scraped 12 links from page 84\n",
      "Scraped 12 links from page 123\n",
      "Scraped 12 links from page 79\n",
      "Scraped 12 links from page 189\n",
      "Scraped 12 links from page 58\n",
      "Scraped 12 links from page 126\n",
      "Scraped 12 links from page 144\n",
      "Scraped 12 links from page 71\n",
      "Scraped 12 links from page 181\n",
      "Scraped 12 links from page 32\n",
      "Scraped 12 links from page 16\n",
      "Scraped 12 links from page 141\n",
      "Scraped 12 links from page 128\n",
      "Scraped 12 links from page 103\n",
      "Scraped 12 links from page 121\n",
      "Scraped 12 links from page 147\n",
      "Scraped 12 links from page 167\n",
      "Scraped 12 links from page 114\n",
      "Scraped 12 links from page 136\n",
      "Scraped 12 links from page 73\n",
      "Scraped 12 links from page 119\n",
      "Scraped 12 links from page 137\n",
      "Scraped 12 links from page 112\n",
      "Scraped 12 links from page 68\n",
      "Scraped 12 links from page 166\n",
      "Scraped 12 links from page 127\n",
      "Scraped 12 links from page 65\n",
      "Scraped 12 links from page 163\n",
      "Scraped 12 links from page 196\n",
      "Scraped 12 links from page 173\n",
      "Scraped 12 links from page 140\n",
      "Scraped 12 links from page 139\n",
      "Scraped 12 links from page 110\n",
      "Scraped 12 links from page 156\n",
      "Scraped 12 links from page 195\n",
      "Scraped 12 links from page 159\n",
      "Scraped 12 links from page 198\n",
      "Scraped 12 links from page 180\n",
      "Scraped 12 links from page 158\n",
      "Scraped 12 links from page 186\n",
      "Scraped 12 links from page 155\n",
      "Scraped 12 links from page 142\n",
      "Scraped 12 links from page 109\n",
      "Scraped 12 links from page 104\n",
      "Scraped 12 links from page 106\n",
      "Scraped 12 links from page 118\n",
      "Scraped 12 links from page 168\n",
      "Scraped 12 links from page 178\n",
      "Scraped 12 links from page 124\n",
      "Scraped 12 links from page 172\n",
      "Scraped 12 links from page 134\n",
      "Scraped 12 links from page 154\n",
      "Scraped 12 links from page 193\n",
      "Scraped 12 links from page 174\n",
      "Scraped 12 links from page 95\n",
      "Scraped 12 links from page 170\n",
      "Scraped 12 links from page 117\n",
      "Scraped 12 links from page 194\n",
      "Scraped 12 links from page 182\n",
      "Scraped 12 links from page 151\n",
      "Scraped 12 links from page 157\n",
      "Scraped 12 links from page 191\n",
      "Scraped 12 links from page 116\n",
      "Scraped 12 links from page 190\n",
      "Scraped 12 links from page 184\n",
      "Scraped 12 links from page 161\n",
      "Scraped 12 links from page 185\n",
      "Scraped 12 links from page 108\n",
      "Scraped 12 links from page 145\n",
      "Scraped 12 links from page 152\n",
      "Scraped 12 links from page 130\n",
      "Scraped 12 links from page 192\n",
      "Scraped 12 links from page 129\n",
      "Scraped 12 links from page 176\n",
      "Scraped 12 links from page 171\n",
      "Scraped 12 links from page 146\n",
      "Scraped 12 links from page 199\n",
      "Scraped 12 links from page 82\n",
      "Scraped 12 links from page 132\n",
      "Scraped 12 links from page 160\n",
      "Scraped 12 links from page 97\n",
      "Scraped 12 links from page 69\n",
      "Scraped 12 links from page 92\n",
      "Scraped 12 links from page 162\n",
      "Scraped 12 links from page 169\n",
      "Scraped 12 links from page 99\n",
      "Scraped 12 links from page 96\n",
      "Scraped 12 links from page 148\n",
      "Scraped 12 links from page 87\n",
      "Scraped 12 links from page 187\n",
      "Scraped 12 links from page 98\n",
      "Scraped 12 links from page 138\n",
      "Scraped 12 links from page 91\n",
      "Scraped 12 links from page 179\n",
      "Scraped 12 links from page 100\n",
      "Scraped 12 links from page 165\n",
      "Scraped 12 links from page 153\n",
      "Scraped 12 links from page 149\n",
      "Scraped 12 links from page 188\n",
      "Scraped 12 links from page 183\n",
      "Scraped 12 links from page 150\n",
      "Scraped 12 links from page 85\n",
      "Scraped 12 links from page 197\n",
      "Scraped 12 links from page 143\n",
      "Scraped 12 links from page 93\n",
      "Scraped 12 links from page 115\n",
      "Scraped 12 links from page 200\n",
      "Scraped 12 links from page 177\n",
      "Scraped 12 links from page 164\n",
      "Total Links: 2400\n"
     ]
    }
   ],
   "source": [
    "for page_number in range(1, jumlah_index + 1):\n",
    "    thread = threading.Thread(target=lambda p=page_number: links.extend(scrape_links(p,keywords)))\n",
    "    thread.start()\n",
    "    threads_link.append(thread)\n",
    "\n",
    "for thread in threads_link:\n",
    "    thread.join()\n",
    "print(\"Total Links:\", len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "def scrape_url(url,keywords):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            \n",
    "            # Judul Berita\n",
    "            title_elem = soup.find('h1')\n",
    "            if title_elem:\n",
    "                title_text = title_elem.text.strip()\n",
    "            else:\n",
    "                title_text = \"Title not found\"\n",
    "            # Author berita\n",
    "            author_elem = soup.find('div', {\"class\": \"author\"})\n",
    "            if author_elem:\n",
    "                author_text = author_elem.get_text()\n",
    "                author_text = author_text.split('-')[1].strip()\n",
    "                author_text = author_text.split(',')[0].strip()\n",
    "            else:\n",
    "                author_text = \"Author not found\"     \n",
    "            # tanggal berita\n",
    "            date_elem = soup.find('div', {\"class\": \"date\"})\n",
    "            if date_elem:\n",
    "                date_text = date_elem.text.strip()\n",
    "            else:\n",
    "                date_text = \"Date not found\"\n",
    "            #     # Category berita\n",
    "            category_elements = soup.find('ul', {\"class\": \"breadcrumb\"})\n",
    "            if category_elements:\n",
    "                category_text = category_elements.find_all('li')\n",
    "                category_text= category_text[2].get_text()\n",
    "            else:\n",
    "                category_text = \"Category not found\"\n",
    "            #     # Content Berita\n",
    "            body_elem = soup.find('div', {\"class\": \"detail_text\"})\n",
    "            \n",
    "            if body_elem:\n",
    "                content_elem = body_elem.find_all('p')\n",
    "                content_text = \"\"\n",
    "                for p in content_elem:\n",
    "                    content_text += p.text.strip() + \"\\n\"\n",
    "                \n",
    "                if content_text.strip():\n",
    "                    content_text=content_text\n",
    "                else:\n",
    "                    content_text =\"Content not found\"\n",
    "            else:\n",
    "              content_text =\"Content not found\"\n",
    "\n",
    "            results.append({'title': title_text,\n",
    "                            'keywords': keywords,\n",
    "                            'author' : author_text,\n",
    "                            'category':category_text,\n",
    "                            'date': date_text,\n",
    "                            'content' : content_text,\n",
    "                            'link' : url})\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data from {url}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching URL '{url}': {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL '{url}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in links:\n",
    "    thread = threading.Thread(target=scrape_url, args=(url,keywords))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil scrapping 2400\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>keywords</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Capres Ini Punya Elektabilitas Tinggi dan Raji...</td>\n",
       "      <td>anies</td>\n",
       "      <td>Robertus Andrianto</td>\n",
       "      <td>Berita MyMoney</td>\n",
       "      <td>14 August 2023 14:05</td>\n",
       "      <td>Jakarta, CNBC Indonesia -  Prabowo Subianto di...</td>\n",
       "      <td>https://www.cnbcindonesia.com/mymoney/20230814...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Media Asing Soroti Pilpres RI, Heboh Capres In...</td>\n",
       "      <td>anies</td>\n",
       "      <td>Tommy Patrio Sorongan</td>\n",
       "      <td>Berita</td>\n",
       "      <td>14 August 2023 08:00</td>\n",
       "      <td>Jakarta, CNBC Indonesia - Pemilihan Presiden (...</td>\n",
       "      <td>https://www.cnbcindonesia.com/news/20230814072...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Media Asing Sorot Pilpres RI, Sebut Capres Dik...</td>\n",
       "      <td>anies</td>\n",
       "      <td>sef</td>\n",
       "      <td>Berita</td>\n",
       "      <td>03 August 2023 10:04</td>\n",
       "      <td>Jakarta, CNBC Indonesia - Media asing menyorot...</td>\n",
       "      <td>https://www.cnbcindonesia.com/news/20230803100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bukan Anies-Ganjar-Prabowo, Asing Sorot Pilpre...</td>\n",
       "      <td>anies</td>\n",
       "      <td>Novina Putri Bestari &amp; redaksi</td>\n",
       "      <td>Berita</td>\n",
       "      <td>12 August 2023 07:30</td>\n",
       "      <td>Jakarta, CNBC Indonesia - Pemilihan presiden (...</td>\n",
       "      <td>https://www.cnbcindonesia.com/news/20230812062...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Asing Sorot Pilpres RI, Bukan Anies-Prabowo-Ga...</td>\n",
       "      <td>anies</td>\n",
       "      <td>Thea Fathanah Arbar</td>\n",
       "      <td>Berita</td>\n",
       "      <td>11 August 2023 12:00</td>\n",
       "      <td>Jakarta, CNBC Indonesia - Media asing kembali ...</td>\n",
       "      <td>https://www.cnbcindonesia.com/news/20230811102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Cawapres Anies Diajak Gabung Jadi Timses Ganja...</td>\n",
       "      <td>anies</td>\n",
       "      <td>Wiji Nur Hayat</td>\n",
       "      <td>Berita</td>\n",
       "      <td>03 August 2023 06:15</td>\n",
       "      <td>Jakarta, CNBC Indonesia - Salah satu kandidat ...</td>\n",
       "      <td>https://www.cnbcindonesia.com/news/20230802195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Top! Ini Capres Pilihan 6 Juta Peternak Ayam S...</td>\n",
       "      <td>anies</td>\n",
       "      <td>linda hasibuan</td>\n",
       "      <td>Berita</td>\n",
       "      <td>13 August 2023 13:00</td>\n",
       "      <td>Jakarta, CNBC Indonesia - Menjelang Pemilu Pre...</td>\n",
       "      <td>https://www.cnbcindonesia.com/news/20230813123...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JIS Dipakai Piala Dunia U-17 Hingga Media Asin...</td>\n",
       "      <td>anies</td>\n",
       "      <td>cnbc indonesia</td>\n",
       "      <td>Video News</td>\n",
       "      <td>02 August 2023 20:30</td>\n",
       "      <td>Jakarta, CNBC Indonesia - Mantan Gubernur DKI ...</td>\n",
       "      <td>https://www.cnbcindonesia.com/news/20230802182...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Peta Koalisi Terkini Partai Pendukung Prabowo,...</td>\n",
       "      <td>anies</td>\n",
       "      <td>Tim Redaksi</td>\n",
       "      <td>Berita</td>\n",
       "      <td>14 August 2023 13:25</td>\n",
       "      <td>Jakarta, CNBC Indonesia - Partai Golongan Kary...</td>\n",
       "      <td>https://www.cnbcindonesia.com/news/20230814111...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Kandidat Cawapres Anies Diajak Gabung Timses G...</td>\n",
       "      <td>anies</td>\n",
       "      <td>Wiji Nur Hayat</td>\n",
       "      <td>Berita</td>\n",
       "      <td>02 August 2023 12:16</td>\n",
       "      <td>Jakarta, CNBC Indonesia - Salah satu kandidat ...</td>\n",
       "      <td>https://www.cnbcindonesia.com/news/20230802121...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title keywords  \\\n",
       "0  Capres Ini Punya Elektabilitas Tinggi dan Raji...    anies   \n",
       "1  Media Asing Soroti Pilpres RI, Heboh Capres In...    anies   \n",
       "2  Media Asing Sorot Pilpres RI, Sebut Capres Dik...    anies   \n",
       "3  Bukan Anies-Ganjar-Prabowo, Asing Sorot Pilpre...    anies   \n",
       "4  Asing Sorot Pilpres RI, Bukan Anies-Prabowo-Ga...    anies   \n",
       "5  Cawapres Anies Diajak Gabung Jadi Timses Ganja...    anies   \n",
       "6  Top! Ini Capres Pilihan 6 Juta Peternak Ayam S...    anies   \n",
       "7  JIS Dipakai Piala Dunia U-17 Hingga Media Asin...    anies   \n",
       "8  Peta Koalisi Terkini Partai Pendukung Prabowo,...    anies   \n",
       "9  Kandidat Cawapres Anies Diajak Gabung Timses G...    anies   \n",
       "\n",
       "                           author        category                  date  \\\n",
       "0              Robertus Andrianto  Berita MyMoney  14 August 2023 14:05   \n",
       "1           Tommy Patrio Sorongan          Berita  14 August 2023 08:00   \n",
       "2                             sef          Berita  03 August 2023 10:04   \n",
       "3  Novina Putri Bestari & redaksi          Berita  12 August 2023 07:30   \n",
       "4             Thea Fathanah Arbar          Berita  11 August 2023 12:00   \n",
       "5                  Wiji Nur Hayat          Berita  03 August 2023 06:15   \n",
       "6                  linda hasibuan          Berita  13 August 2023 13:00   \n",
       "7                  cnbc indonesia      Video News  02 August 2023 20:30   \n",
       "8                     Tim Redaksi          Berita  14 August 2023 13:25   \n",
       "9                  Wiji Nur Hayat          Berita  02 August 2023 12:16   \n",
       "\n",
       "                                             content  \\\n",
       "0  Jakarta, CNBC Indonesia -  Prabowo Subianto di...   \n",
       "1  Jakarta, CNBC Indonesia - Pemilihan Presiden (...   \n",
       "2  Jakarta, CNBC Indonesia - Media asing menyorot...   \n",
       "3  Jakarta, CNBC Indonesia - Pemilihan presiden (...   \n",
       "4  Jakarta, CNBC Indonesia - Media asing kembali ...   \n",
       "5  Jakarta, CNBC Indonesia - Salah satu kandidat ...   \n",
       "6  Jakarta, CNBC Indonesia - Menjelang Pemilu Pre...   \n",
       "7  Jakarta, CNBC Indonesia - Mantan Gubernur DKI ...   \n",
       "8  Jakarta, CNBC Indonesia - Partai Golongan Kary...   \n",
       "9  Jakarta, CNBC Indonesia - Salah satu kandidat ...   \n",
       "\n",
       "                                                link  \n",
       "0  https://www.cnbcindonesia.com/mymoney/20230814...  \n",
       "1  https://www.cnbcindonesia.com/news/20230814072...  \n",
       "2  https://www.cnbcindonesia.com/news/20230803100...  \n",
       "3  https://www.cnbcindonesia.com/news/20230812062...  \n",
       "4  https://www.cnbcindonesia.com/news/20230811102...  \n",
       "5  https://www.cnbcindonesia.com/news/20230802195...  \n",
       "6  https://www.cnbcindonesia.com/news/20230813123...  \n",
       "7  https://www.cnbcindonesia.com/news/20230802182...  \n",
       "8  https://www.cnbcindonesia.com/news/20230814111...  \n",
       "9  https://www.cnbcindonesia.com/news/20230802121...  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "print('hasil scrapping',len(results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to ./excel/cnbc_anies_2023-09-27_07-06-53.xlsx\n"
     ]
    }
   ],
   "source": [
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "excel_file_name = f'./excel/cnbc_{keywords}_{current_datetime}.xlsx'\n",
    "df.to_excel(excel_file_name, index=False)\n",
    "\n",
    "print(f'Data has been saved to {excel_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tempat Searching dan cek web\n",
    "# url = \"https://www.cnbcindonesia.com/news/20230924184706-4-475109/kota-hantu-china-bikin-pening-ada-72-juta-rumah-tak-laku\"  \n",
    "# response = requests.get(url)\n",
    "# soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "# # author_elem = soup.find('ul', {\"class\": \"breadcrumb\"})\n",
    "# author_elem = soup.find('ul', {\"class\": \"breadcrumb\"})\n",
    "\n",
    "# author_text = author_elem.find_all('li')\n",
    "# # author_text = author_text.split('-')[1].strip()\n",
    "# # author_text = author_text.split(',')[0].strip()\n",
    "# print(author_text[2].get_text())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
