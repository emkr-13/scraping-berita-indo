{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Tribunnews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"keywords\": \"anies\",\n",
    "    \"since_time\": \"2023-11-01\",\n",
    "    \"until_time\": \"2023-11-10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page value: 23\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "url = f\"https://www.tribunnews.com/index-news?date=2023-9-2&page=\"\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "cek= soup.find('div', {\"class\": \"paging\"})\n",
    "links= cek.find_all('a')\n",
    "all_link=[]\n",
    "for link in links:\n",
    "        if link and 'href' in link.attrs:\n",
    "            link_href = link['href']\n",
    "            all_link.append(link_href)\n",
    "link=all_link[-1]\n",
    "parsed_url = urlparse(link)\n",
    "\n",
    "# Extract the \"page\" parameter value\n",
    "page_value = parse_qs(parsed_url.query).get('page', [])[0] if 'page' in parse_qs(parsed_url.query) else None\n",
    "\n",
    "print(\"Page value:\", page_value)\n",
    "# body = soup.find('ul', {\"class\": \"lsi\"})\n",
    "# print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date, page_number):\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    url = f\"https://www.tribunnews.com/index-news?date={current_date}&page={page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    body = soup.find('ul', {\"class\": \"lsi\"})\n",
    "    articles = body.find_all('li', {\"class\": \"ptb15\"})\n",
    "    \n",
    "    links = []\n",
    "    for article in articles:\n",
    "        header = article.find('h3', {\"class\": \"f16 fbo\"})\n",
    "        if header:\n",
    "            link = header.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                link_href = link['href']\n",
    "                links.append(link_href)\n",
    "                \n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    url = f\"https://www.tribunnews.com/index-news?date={current_date}&page=\"\n",
    "    response = requests.get(url + \"1\", headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cek = soup.find('div', {\"class\": \"paging\"})\n",
    "    links = cek.find_all('a')\n",
    "    all_link = []\n",
    "    for link in links:\n",
    "        if link and 'href' in link.attrs:\n",
    "            link_href = link['href']\n",
    "            all_link.append(link_href)\n",
    "    last_link = all_link[-1]\n",
    "\n",
    "    # Extract the \"page\" parameter value from the last link\n",
    "    parsed_url = urlparse(last_link)\n",
    "    page_value = parse_qs(parsed_url.query).get('page', [])[0] if 'page' in parse_qs(parsed_url.query) else None\n",
    "    print(page_value)\n",
    "    # Check if page_value is an integer\n",
    "    try:\n",
    "        page_number = int(page_value)\n",
    "    except (TypeError, ValueError):\n",
    "        page_number = 1\n",
    "\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        # Use list comprehension to submit tasks to the thread pool\n",
    "        futures = [executor.submit(scrape_links, current_date, index) for index in range(1, page_number + 1)]\n",
    "\n",
    "        # Collect results from completed futures\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "Scraped 20 links from page 1 url https://www.tribunnews.com/index-news?date=2023-11-1&page=1\n",
      "Scraped 20 links from page 5 url https://www.tribunnews.com/index-news?date=2023-11-1&page=5\n",
      "Scraped 20 links from page 2 url https://www.tribunnews.com/index-news?date=2023-11-1&page=2\n",
      "Scraped 20 links from page 3 url https://www.tribunnews.com/index-news?date=2023-11-1&page=3\n",
      "Scraped 20 links from page 4 url https://www.tribunnews.com/index-news?date=2023-11-1&page=4\n",
      "Scraped 20 links from page 7 url https://www.tribunnews.com/index-news?date=2023-11-1&page=7\n",
      "Scraped 20 links from page 6 url https://www.tribunnews.com/index-news?date=2023-11-1&page=6\n",
      "Scraped 20 links from page 9 url https://www.tribunnews.com/index-news?date=2023-11-1&page=9\n",
      "Scraped 20 links from page 8 url https://www.tribunnews.com/index-news?date=2023-11-1&page=8\n",
      "Scraped 20 links from page 10 url https://www.tribunnews.com/index-news?date=2023-11-1&page=10\n",
      "Scraped 20 links from page 11 url https://www.tribunnews.com/index-news?date=2023-11-1&page=11\n",
      "Scraped 20 links from page 12 url https://www.tribunnews.com/index-news?date=2023-11-1&page=12\n",
      "Scraped 20 links from page 14 url https://www.tribunnews.com/index-news?date=2023-11-1&page=14\n",
      "Scraped 20 links from page 13 url https://www.tribunnews.com/index-news?date=2023-11-1&page=13\n",
      "Scraped 20 links from page 15 url https://www.tribunnews.com/index-news?date=2023-11-1&page=15\n",
      "Scraped 20 links from page 16 url https://www.tribunnews.com/index-news?date=2023-11-1&page=16\n",
      "Scraped 20 links from page 17 url https://www.tribunnews.com/index-news?date=2023-11-1&page=17\n",
      "Scraped 20 links from page 18 url https://www.tribunnews.com/index-news?date=2023-11-1&page=18\n",
      "Scraped 20 links from page 19 url https://www.tribunnews.com/index-news?date=2023-11-1&page=19\n",
      "Scraped 20 links from page 20 url https://www.tribunnews.com/index-news?date=2023-11-1&page=20\n",
      "Scraped 20 links from page 22 url https://www.tribunnews.com/index-news?date=2023-11-1&page=22\n",
      "Scraped 20 links from page 23 url https://www.tribunnews.com/index-news?date=2023-11-1&page=23\n",
      "Scraped 20 links from page 21 url https://www.tribunnews.com/index-news?date=2023-11-1&page=21\n",
      "Scraped 20 links from page 25 url https://www.tribunnews.com/index-news?date=2023-11-1&page=25\n",
      "Scraped 20 links from page 24 url https://www.tribunnews.com/index-news?date=2023-11-1&page=24\n",
      "Scraped 20 links from page 26 url https://www.tribunnews.com/index-news?date=2023-11-1&page=26\n",
      "Scraped 20 links from page 28 url https://www.tribunnews.com/index-news?date=2023-11-1&page=28\n",
      "Scraped 20 links from page 27 url https://www.tribunnews.com/index-news?date=2023-11-1&page=27\n",
      "Scraped 20 links from page 29 url https://www.tribunnews.com/index-news?date=2023-11-1&page=29\n",
      "Scraped 20 links from page 30 url https://www.tribunnews.com/index-news?date=2023-11-1&page=30\n",
      "Scraped 20 links from page 31 url https://www.tribunnews.com/index-news?date=2023-11-1&page=31\n",
      "Scraped 20 links from page 33 url https://www.tribunnews.com/index-news?date=2023-11-1&page=33\n",
      "Scraped 20 links from page 32 url https://www.tribunnews.com/index-news?date=2023-11-1&page=32\n",
      "Scraped 20 links from page 34 url https://www.tribunnews.com/index-news?date=2023-11-1&page=34\n",
      "Scraped 20 links from page 35 url https://www.tribunnews.com/index-news?date=2023-11-1&page=35\n",
      "Scraped 20 links from page 36 url https://www.tribunnews.com/index-news?date=2023-11-1&page=36\n",
      "Scraped 20 links from page 37 url https://www.tribunnews.com/index-news?date=2023-11-1&page=37\n",
      "Scraped 20 links from page 38 url https://www.tribunnews.com/index-news?date=2023-11-1&page=38\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day(data[\"since_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760\n"
     ]
    }
   ],
   "source": [
    "print(len(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('h1', {\"id\": \"arttitle\"})\n",
    "                    if title_elem:\n",
    "                        title_text = title_elem.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"   \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('time', {\"class\": \"grey\"})\n",
    "                    if date_elem:\n",
    "                        date_text = date_elem.text.strip()\n",
    "                        date_part = ' '.join(date_text.split(',')[1:]).strip()\n",
    "                        date_object = parser.parse(date_part)\n",
    "                        formatted_date = date_object.strftime('%Y/%m/%d')\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"side-article txt-article multi-fontsize\"})\n",
    "                    \n",
    "                    if body_elem:\n",
    "                        content_elem = body_elem.find_all('p')\n",
    "                        content_text = \"\"\n",
    "                        for p in content_elem:\n",
    "                            content_text += p.text.strip() + \"\\n\"\n",
    "                        \n",
    "                        if content_text.strip():\n",
    "                            content_text=content_text\n",
    "                            content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                            content_text = ' '.join(content_text.split())\n",
    "                        else:\n",
    "                            content_text =\"Content not found\"\n",
    "                    else:\n",
    "                        content_text =\"Content not found\"\n",
    "\n",
    "                    return{\n",
    "                        'title': title_text,\n",
    "                        'date': formatted_date,\n",
    "                        'content':content_text,\n",
    "                        'link' : url}\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.tribunnews.com/nasional/2023/11/01/wapres-maruf-amin-dan-panglima-tni-buka-gerakan-nasional-ketahanan-pangan-2023'\n",
    "data_tribunnews=scrape_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': \"Wapres Ma'ruf Amin dan Panglima TNI Buka Gerakan Nasional Ketahanan Pangan 2023\", 'date': '2023/11/01', 'content': 'TRIBUNNEWS.COM, JAKARTA - Wakil Presiden RI Ma’ruf Amin dan Menteri Pertanian Amran Sulaiman menghadiri pembukaan Gerakan Nasional Ketahanan Pangan 2023 di Taman Pancasila, Cibitung, Kabupaten Bekasi, Rabu (1/11/2023).Gerakan yang diinisiasi oleh TNI ini dibuka oleh Wapres Ma’ruf Amin dan Panglima TNI Laksamana Yudo Margono.Selain Wapres dan Panglima, turut hadir pula Kapolri Jenderal Listyo Sigit dan kepala staf tiga matra, yakni Kepala Staf TNI AD (KSAD) Jenderal Agus Subiyanto, Kepala Staf TNI AL (KSAL) Laksamana Muhammad Ali, dan Kepala Staf TNI AU (KSAU) Marsekal Fadjar Prasetyo serta Menteri Pertanian Amran Sulaiman.Berdasarkan pantauan Tribunnews.com di lokasi, Wapres Ma’ruf Amin, Mentan Amran, Panglima TNI Yudo, dan Kapolri Listyo Sigit meninjau lokasi penggemukan sapi yang dikerjakan kelompok tani.Selain lokasi penggemukan sapi, para pejabat tinggi itu juga melaksanakan tabur benih ikan ke area embung yang juga terdapat di lokasi tersebut.“Kegiatan Gerakan Nasional Ketahanan Pangan ini mengandung makna sebagai upaya TNI dalam rangka membantu meningkatkan perekonomian masyarakat dan sebagai bentuk dukungan kepada pemerintah terhadap program ketahanan pangan dalam mencegah terjadinya krisis pangan di Indonesia,” tulis siaran pers Puspen TNI, Rabu.Selain meninjau lokasi ketahanan pangan di Cibitung, Wapres dan Panglima TNI juga turut memonitor kegiatan yang sama di lokasi lain melalui video zoom.Adapun kegiatan Gerakan Ketahanan Pangan Nasional ini digelar secara serentak di 385 lokasi di seluruh Indonesia.Terkait kegiatan ini sebelumnya, Yudo Margono telah mengungkapkan bahwa dirinya bakal mengerahkan prajuritnya untuk membantu ketahanan pangan nasional.Hal itu disampaikan usai Presiden Joko Widodo (Jokowi) meminta agar TNI peka terhadap krisis pangan.Baca juga: Perbandingan Program 3 Capres-cawapres Bidang Ekonomi dan Ketahanan Pangan“Ke depan ini akan saya adakan kegiatan gerakan nasional ketahanan pangan. Nanti saya akan sesuaikan jadwal Pak Presiden, tanggal 17 Oktober (2023) akan kami gerakkan seluruh Indonesia,” ujar Yudo usai perayaan HUT ke-78 TNI di Monumen Nasional (Monas), Jakarta Pusat, Kamis (5/10/2023) lalu.', 'link': 'https://www.tribunnews.com/nasional/2023/11/01/wapres-maruf-amin-dan-panglima-tni-buka-gerakan-nasional-ketahanan-pangan-2023'}\n"
     ]
    }
   ],
   "source": [
    "print(data_tribunnews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for url in link:\n",
    "#     data_tribunnews = scrape_url(url)\n",
    "#     print(data[\"keywords\"])\n",
    "#     keywords=data[\"keywords\"]\n",
    "#     if keywords.lower() in data_tribunnews['title'].lower() or keywords.lower() in data_tribunnews['content'].lower():\n",
    "#         print(\"Data contains keywords:\", data_tribunnews)\n",
    "#     else:\n",
    "#         print(\"News does not contain the specified keywords and will not be inserted into the database. URL:\", data_tribunnews['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
