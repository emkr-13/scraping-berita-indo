{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapping Tribunnews "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from urllib.parse import urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"keywords\": \"anies\",\n",
    "    \"since_time\": \"2023-09-01\",\n",
    "    \"until_time\": \"2023-11-10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page value: 23\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "url = f\"https://www.tribunnews.com/index-news?date=2023-9-2&page=\"\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "cek= soup.find('div', {\"class\": \"paging\"})\n",
    "links= cek.find_all('a')\n",
    "all_link=[]\n",
    "for link in links:\n",
    "        if link and 'href' in link.attrs:\n",
    "            link_href = link['href']\n",
    "            all_link.append(link_href)\n",
    "link=all_link[-1]\n",
    "parsed_url = urlparse(link)\n",
    "\n",
    "# Extract the \"page\" parameter value\n",
    "page_value = parse_qs(parsed_url.query).get('page', [])[0] if 'page' in parse_qs(parsed_url.query) else None\n",
    "\n",
    "print(\"Page value:\", page_value)\n",
    "# body = soup.find('ul', {\"class\": \"lsi\"})\n",
    "# print(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date, page_number):\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    url = f\"https://www.tribunnews.com/index-news?date={current_date}&page={page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    body = soup.find('ul', {\"class\": \"lsi\"})\n",
    "    articles = body.find_all('li', {\"class\": \"ptb15\"})\n",
    "    \n",
    "    links = []\n",
    "    for article in articles:\n",
    "        header = article.find('h3', {\"class\": \"f16 fbo\"})\n",
    "        if header:\n",
    "            link = header.find('a')\n",
    "            if link and 'href' in link.attrs:\n",
    "                link_href = link['href']\n",
    "                links.append(link_href)\n",
    "                \n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\").strftime(\"%Y-%-m-%-d\")\n",
    "    url = f\"https://www.tribunnews.com/index-news?date={current_date}&page=\"\n",
    "    response = requests.get(url + \"1\", headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cek = soup.find('div', {\"class\": \"paging\"})\n",
    "    links = cek.find_all('a')\n",
    "    all_link = []\n",
    "    for link in links:\n",
    "        if link and 'href' in link.attrs:\n",
    "            link_href = link['href']\n",
    "            all_link.append(link_href)\n",
    "    last_link = all_link[-1]\n",
    "\n",
    "    # Extract the \"page\" parameter value from the last link\n",
    "    parsed_url = urlparse(last_link)\n",
    "    page_value = parse_qs(parsed_url.query).get('page', [])[0] if 'page' in parse_qs(parsed_url.query) else None\n",
    "    print(page_value)\n",
    "    # Check if page_value is an integer\n",
    "    try:\n",
    "        page_number = int(page_value)\n",
    "    except (TypeError, ValueError):\n",
    "        page_number = 1\n",
    "\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        # Use list comprehension to submit tasks to the thread pool\n",
    "        futures = [executor.submit(scrape_links, current_date, index) for index in range(1, page_number + 1)]\n",
    "\n",
    "        # Collect results from completed futures\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n",
      "Scraped 20 links from page 1 url https://www.tribunnews.com/index-news?date=2023-9-1&page=1\n",
      "Scraped 20 links from page 2 url https://www.tribunnews.com/index-news?date=2023-9-1&page=2\n",
      "Scraped 20 links from page 5 url https://www.tribunnews.com/index-news?date=2023-9-1&page=5\n",
      "Scraped 20 links from page 3 url https://www.tribunnews.com/index-news?date=2023-9-1&page=3\n",
      "Scraped 20 links from page 4 url https://www.tribunnews.com/index-news?date=2023-9-1&page=4\n",
      "Scraped 20 links from page 10 url https://www.tribunnews.com/index-news?date=2023-9-1&page=10\n",
      "Scraped 20 links from page 7 url https://www.tribunnews.com/index-news?date=2023-9-1&page=7\n",
      "Scraped 20 links from page 9 url https://www.tribunnews.com/index-news?date=2023-9-1&page=9\n",
      "Scraped 20 links from page 6 url https://www.tribunnews.com/index-news?date=2023-9-1&page=6\n",
      "Scraped 20 links from page 8 url https://www.tribunnews.com/index-news?date=2023-9-1&page=8\n",
      "Scraped 20 links from page 12 url https://www.tribunnews.com/index-news?date=2023-9-1&page=12\n",
      "Scraped 20 links from page 13 url https://www.tribunnews.com/index-news?date=2023-9-1&page=13\n",
      "Scraped 20 links from page 15 url https://www.tribunnews.com/index-news?date=2023-9-1&page=15\n",
      "Scraped 20 links from page 14 url https://www.tribunnews.com/index-news?date=2023-9-1&page=14\n",
      "Scraped 20 links from page 11 url https://www.tribunnews.com/index-news?date=2023-9-1&page=11\n",
      "Scraped 20 links from page 16 url https://www.tribunnews.com/index-news?date=2023-9-1&page=16\n",
      "Scraped 20 links from page 17 url https://www.tribunnews.com/index-news?date=2023-9-1&page=17\n",
      "Scraped 20 links from page 18 url https://www.tribunnews.com/index-news?date=2023-9-1&page=18\n",
      "Scraped 20 links from page 20 url https://www.tribunnews.com/index-news?date=2023-9-1&page=20\n",
      "Scraped 20 links from page 19 url https://www.tribunnews.com/index-news?date=2023-9-1&page=19\n",
      "Scraped 20 links from page 21 url https://www.tribunnews.com/index-news?date=2023-9-1&page=21\n",
      "Scraped 20 links from page 22 url https://www.tribunnews.com/index-news?date=2023-9-1&page=22\n",
      "Scraped 20 links from page 23 url https://www.tribunnews.com/index-news?date=2023-9-1&page=23Scraped 20 links from page 25 url https://www.tribunnews.com/index-news?date=2023-9-1&page=25\n",
      "\n",
      "Scraped 20 links from page 24 url https://www.tribunnews.com/index-news?date=2023-9-1&page=24\n",
      "Scraped 20 links from page 26 url https://www.tribunnews.com/index-news?date=2023-9-1&page=26\n",
      "Scraped 20 links from page 27 url https://www.tribunnews.com/index-news?date=2023-9-1&page=27\n",
      "Scraped 20 links from page 28 url https://www.tribunnews.com/index-news?date=2023-9-1&page=28Scraped 20 links from page 30 url https://www.tribunnews.com/index-news?date=2023-9-1&page=30\n",
      "\n",
      "Scraped 20 links from page 29 url https://www.tribunnews.com/index-news?date=2023-9-1&page=29\n",
      "Scraped 20 links from page 32 url https://www.tribunnews.com/index-news?date=2023-9-1&page=32\n",
      "Scraped 20 links from page 31 url https://www.tribunnews.com/index-news?date=2023-9-1&page=31\n",
      "Scraped 20 links from page 33 url https://www.tribunnews.com/index-news?date=2023-9-1&page=33\n",
      "Scraped 20 links from page 34 url https://www.tribunnews.com/index-news?date=2023-9-1&page=34\n",
      "Scraped 20 links from page 35 url https://www.tribunnews.com/index-news?date=2023-9-1&page=35\n",
      "Scraped 20 links from page 37 url https://www.tribunnews.com/index-news?date=2023-9-1&page=37\n",
      "Scraped 20 links from page 36 url https://www.tribunnews.com/index-news?date=2023-9-1&page=36\n",
      "Scraped 20 links from page 38 url https://www.tribunnews.com/index-news?date=2023-9-1&page=38\n",
      "Scraped 20 links from page 40 url https://www.tribunnews.com/index-news?date=2023-9-1&page=40\n",
      "Scraped 20 links from page 39 url https://www.tribunnews.com/index-news?date=2023-9-1&page=39\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day(data[\"since_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "print(len(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('h1', {\"id\": \"arttitle\"})\n",
    "                    if title_elem:\n",
    "                        title_text = title_elem.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"   \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('time', {\"class\": \"grey\"})\n",
    "                    if date_elem:\n",
    "                        date_text = date_elem.text.strip()\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"side-article txt-article multi-fontsize\"})\n",
    "                    \n",
    "                    if body_elem:\n",
    "                        content_elem = body_elem.find_all('p')\n",
    "                        content_text = \"\"\n",
    "                        for p in content_elem:\n",
    "                            content_text += p.text.strip() + \"\\n\"\n",
    "                        \n",
    "                        if content_text.strip():\n",
    "                            content_text=content_text\n",
    "                        else:\n",
    "                            content_text =\"Content not found\"\n",
    "                    else:\n",
    "                        content_text =\"Content not found\"\n",
    "\n",
    "                    return{\n",
    "                        'title': title_text,\n",
    "                        'date': date_text,\n",
    "                        'content':content_text,\n",
    "                        'link' : url}\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/mata-lokal-memilih/2023/01/01/ganjar-erick-dinilai-mirip-seperti-jokowi-di-2014-yang-muncul-dengan-segudang-prestasi\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/seleb/2023/01/01/agensi-mengonfirmasi-shindong-super-junior-pacaran-dengan-non-selebriti\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/pendidikan/2023/01/01/kunci-jawaban-bahasa-indonesia-kelas-10-halaman-237-penggunaan-kata-ganti-pada-teks-biografi\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/sport/2023/01/01/jadwal-motogp-2023-mulai-bulan-maret-di-portugal-motogp-mandalika-bulan-oktober\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/superskor/2023/01/01/rekor-pertemuan-timnas-indonesia-vs-filipina-garuda-pernah-bantai-12-0-the-azkals\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/metropolitan/2023/01/01/pelaku-penculik-bocah-di-gunung-sahari-merupakan-residivis-pencabulan-dan-pernah-dipenjara-7-tahun\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/regional/2023/01/01/pamit-beli-petasan-2-bocah-laki-laki-ditemukan-tewas-di-kubangan-galian-proyek-tol-cijago-depok\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/sport/2023/01/01/stefan-bradl-bicara-soal-peluang-marc-marquez-di-kejuaraan-dunia-motogp-2023\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/bisnis/2023/01/01/usai-merugi-39-triliun-dolar-as-pasar-saham-di-china-diproyeksikan-rebound-pada-2023\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/nasional/2023/01/01/gelombang-tinggi-di-laut-panglima-tni-perintahkan-jajaran-tetap-siaga-sar-dan-tanggap-darurat\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/tribunners/2023/01/01/situs-goa-liang-bangkai-salah-satu-pesona-geopark-meratus\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/bisnis/2023/01/01/perjalanan-kereta-api-terganggu-banjir-semarang-kai-kembalikan-biaya-tiket-yang-lakukan-pembatalan\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/internasional/2023/01/01/korea-utara-akan-kembangkan-rudal-balistik-antar-benua-saingi-as-dan-korea-selatan\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/superskor/2023/01/01/habis-cody-gakpo-terbitlah-sofyan-amrabat-agresifitas-transfer-liverpool-di-musim-dingin\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/parapuan/2023/01/01/tayang-januari-2023-di-bioskop-simak-deretan-film-horor-indonesia-yang-siap-menghantuimu\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/regional/2023/01/01/imbas-banjir-di-semarang-waktu-tempuh-naik-kereta-dari-surabaya-ke-jakarta-tembus-22-jam\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/seleb/2023/01/01/sinopsis-film-cj7-kisah-kehidupan-manusia-dan-alien-tayang-malam-ini-di-trans-tv\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/metropolitan/2023/01/01/polres-metro-jakarta-pusat-naikan-status-kasus-penculikan-bocah-di-gunung-sahari-ke-penyidikan\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/bisnis/2023/01/01/kai-akibat-banjir-di-semarang-12-perjalanan-kereta-api-telat-berangkat-dari-jakarta\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/superskor/2023/01/01/timnas-indonesia-gelar-latihan-di-malam-tahun-baru\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/regional/2023/01/01/ini-alat-yang-dipakai-pelaku-untuk-mutilasi-wanita-di-bekasi-hingga-soal-identitas-terduga-korban\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/nasional/2023/01/01/cuaca-besok-bmkg-21-wilayah-berpotensi-hujan-dan-angin-kencang-2-januari-2023\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/sport/2023/01/01/malaysia-open-2023-ambisi-lee-zii-jia-rengkuh-trofi-juara-demi-patahkan-puasa-gelar\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/kilas-kementerian/2023/01/01/hnw-sistem-pemilu-terbuka-lebih-sesuai-dengan-konstitusi-dan-putusan-mk-sebelumnya\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/nasional/2023/01/01/bantuan-kartu-prakerja-lanjut-di-2023-rp-42-juta-per-penerima\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/lifestyle/2023/01/01/selain-anti-aging-retinol-mampu-tingkatkan-produksi-kolagen\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/seleb/2023/01/01/indra-bekti-akan-dirawat-20-hari-ke-depan-aldilla-jelita-buka-penggalangan-dana-biaya-sangat-besar\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/seleb/2023/01/01/resmi-berpacaran-dengan-iu-lee-jong-suk-akui-sudah-suka-sejak-lama\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/pendidikan/2023/01/01/kunci-jawaban-prakarya-kelas-8-halaman-116-semester-2-lembar-kerja-5-pengolahan-serelia-dan-umbi\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/nasional/2023/01/01/ronny-talapessy-hanya-tertawa-tanggapi-klaim-ferdy-sambo-jadi-yang-pertama-bongkar-kasus-brigadir-j\n",
      "prabowo\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://www.tribunnews.com/lifestyle/2023/01/01/ramalan-zodiak-cinta-senin-2-januari-2023-virgo-salah-paham-sagittarius-tenang-hadapi-masalah\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m re\u001b[39m.\u001b[39msearch(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(re\u001b[39m.\u001b[39mescape(keyword)), text)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m url \u001b[39min\u001b[39;00m link:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     data_tribunnews \u001b[39m=\u001b[39m scrape_url(url)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb#X11sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m# Assuming data_tribunnews is a dictionary with 'title', 'content', and 'link' keys\u001b[39;00m\n",
      "\u001b[1;32m/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, headers\u001b[39m=\u001b[39mheaders)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Judul Berita\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/tribunnews/scrap_tribunnews_fix_fungsi.ipynb#X11sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     title_elem \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mh1\u001b[39m\u001b[39m'\u001b[39m, {\u001b[39m\"\u001b[39m\u001b[39mid\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39marttitle\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bs4/__init__.py:335\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39minitialize_soup(\u001b[39mself\u001b[39m)\n\u001b[1;32m    334\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 335\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_feed()\n\u001b[1;32m    336\u001b[0m     success \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bs4/__init__.py:478\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[39m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mreset()\n\u001b[0;32m--> 478\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mfeed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmarkup)\n\u001b[1;32m    479\u001b[0m \u001b[39m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    480\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bs4/builder/_htmlparser.py:380\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    378\u001b[0m parser\u001b[39m.\u001b[39msoup \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\n\u001b[1;32m    379\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 380\u001b[0m     parser\u001b[39m.\u001b[39mfeed(markup)\n\u001b[1;32m    381\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    382\u001b[0m     \u001b[39m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[1;32m    383\u001b[0m     \u001b[39m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[39m# when there's an error in the doctype declaration.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m     \u001b[39mraise\u001b[39;00m ParserRejectedMarkup(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/html/parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[39mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[39mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrawdata \u001b[39m+\u001b[39m data\n\u001b[0;32m--> 110\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoahead(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/html/parser.py:172\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    170\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_starttag(i)\n\u001b[1;32m    171\u001b[0m \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m</\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[0;32m--> 172\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_endtag(i)\n\u001b[1;32m    173\u001b[0m \u001b[39melif\u001b[39;00m startswith(\u001b[39m\"\u001b[39m\u001b[39m<!--\u001b[39m\u001b[39m\"\u001b[39m, i):\n\u001b[1;32m    174\u001b[0m     k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_comment(i)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/html/parser.py:413\u001b[0m, in \u001b[0;36mHTMLParser.parse_endtag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_data(rawdata[i:gtpos])\n\u001b[1;32m    411\u001b[0m         \u001b[39mreturn\u001b[39;00m gtpos\n\u001b[0;32m--> 413\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_endtag(elem)\n\u001b[1;32m    414\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclear_cdata_mode()\n\u001b[1;32m    415\u001b[0m \u001b[39mreturn\u001b[39;00m gtpos\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bs4/builder/_htmlparser.py:176\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_endtag\u001b[0;34m(self, name, check_already_closed)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malready_closed_empty_element\u001b[39m.\u001b[39mremove(name)\n\u001b[1;32m    175\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 176\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoup\u001b[39m.\u001b[39mhandle_endtag(name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bs4/__init__.py:771\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_endtag\u001b[0;34m(self, name, nsprefix)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[39m#print(\"End tag: \" + name)\u001b[39;00m\n\u001b[1;32m    770\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mendData()\n\u001b[0;32m--> 771\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popToTag(name, nsprefix)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/bs4/__init__.py:710\u001b[0m, in \u001b[0;36mBeautifulSoup._popToTag\u001b[0;34m(self, name, nsprefix, inclusivePop)\u001b[0m\n\u001b[1;32m    707\u001b[0m most_recently_popped \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    709\u001b[0m stack_size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtagStack)\n\u001b[0;32m--> 710\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(stack_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    711\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mopen_tag_counter\u001b[39m.\u001b[39mget(name):\n\u001b[1;32m    712\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for url in link:\n",
    "    data_tribunnews = scrape_url(url)\n",
    "    print(data[\"keywords\"])\n",
    "    keywords=data[\"keywords\"]\n",
    "    if keywords.lower() in data_tribunnews['title'].lower() or keywords.lower() in data_tribunnews['content'].lower():\n",
    "        print(\"Data contains keywords:\", data_tribunnews['link'])\n",
    "    else:\n",
    "        print(\"News does not contain the specified keywords and will not be inserted into the database. URL:\", data_tribunnews['link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
