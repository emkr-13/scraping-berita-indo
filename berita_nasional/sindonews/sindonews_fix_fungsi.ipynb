{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from dateutil import parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"keywords\": \"anies\",\n",
    "    \"since_time\": \"2023-09-01\",\n",
    "    \"until_time\": \"2023-11-10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "url = f\"https://index.sindonews.com/index/0?t=2023-09-01\"\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# print(soup)\n",
    "cek = soup.find('div', {\"class\": \"pagination\"})\n",
    "links=cek.find_all('a')\n",
    "all_link=[]\n",
    "for link in links:\n",
    "        if link and 'href' in link.attrs:\n",
    "            link_href = link['href']\n",
    "            all_link.append(link_href)\n",
    "link=all_link[-1]\n",
    "# print(link)\n",
    "parsed_url = urlparse(link)\n",
    "number = parsed_url.path.split(\"/\")[-1]\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date, page_number):\n",
    "    input_date_str = datetime.strftime(date, \"%Y-%m-%d\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = f\"https://index.sindonews.com/index/5/{page_number}?t={input_date_str}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', {\"class\": \"indeks-title\"})\n",
    "    \n",
    "    links = []\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        links.append(link)\n",
    "        \n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    url = f\"https://index.sindonews.com/index/5/0?t={current_date}\"\n",
    "    response = requests.get(url + \"1\", headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cek = soup.find('div', {\"class\": \"pagination\"})\n",
    "    links = cek.find_all('a')\n",
    "    all_link = [link['href'] for link in links if link and 'href' in link.attrs]\n",
    "\n",
    "    if not all_link:\n",
    "        return []\n",
    "\n",
    "    last_link = all_link[-1]\n",
    "\n",
    "    # Extract the \"page\" parameter value from the last link\n",
    "    parsed_url = urlparse(last_link)\n",
    "    page_value = parsed_url.path.split(\"/\")[-1]\n",
    "    print(page_value)\n",
    "    # Check if page_value is an integer\n",
    "    try:\n",
    "        page_number = int(page_value)\n",
    "    except (TypeError, ValueError):\n",
    "        page_number = 0\n",
    "\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        # Use list comprehension to submit tasks to the thread pool\n",
    "        futures = [executor.submit(scrape_links, current_date, index) for index in range(0, page_number + 20, 20)]\n",
    "\n",
    "        # Collect results from completed futures\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n",
      "Scraped 20 links from page 20 url https://index.sindonews.com/index/5/20?t=2023-09-01\n",
      "Scraped 20 links from page 40 url https://index.sindonews.com/index/5/40?t=2023-09-01\n",
      "Scraped 2 links from page 80 url https://index.sindonews.com/index/5/80?t=2023-09-01\n",
      "Scraped 20 links from page 60 url https://index.sindonews.com/index/5/60?t=2023-09-01\n",
      "Scraped 20 links from page 0 url https://index.sindonews.com/index/5/0?t=2023-09-01\n",
      "Scraped 0 links from page 120 url https://index.sindonews.com/index/5/120?t=2023-09-01\n",
      "Scraped 0 links from page 100 url https://index.sindonews.com/index/5/100?t=2023-09-01\n",
      "Scraped 0 links from page 140 url https://index.sindonews.com/index/5/140?t=2023-09-01\n",
      "Scraped 0 links from page 160 url https://index.sindonews.com/index/5/160?t=2023-09-01\n",
      "Scraped 0 links from page 180 url https://index.sindonews.com/index/5/180?t=2023-09-01\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day(\"2023-09-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "print(len(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('div', {\"class\": \"detail-title\"})\n",
    "                    if title_elem:\n",
    "                        title_text = title_elem.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"   \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('div', {\"class\": \"detail-date-artikel\"})\n",
    "                    if date_elem:\n",
    "                        date_text = date_elem.text.strip()\n",
    "                        date_part = ' '.join(date_text.split(',')[1:]).strip()\n",
    "                        date_object = parser.parse(date_part)\n",
    "                        formatted_date = date_object.strftime('%Y-%m-%d')\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"detail-desc\"})\n",
    "                    \n",
    "                    if body_elem:\n",
    "                        content_text = body_elem.text\n",
    "                        content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                        content_text = ' '.join(content_text.split())\n",
    "                    else:\n",
    "                        content_text =\"Content not found\"\n",
    "\n",
    "                    return{\n",
    "                        'title': title_text,\n",
    "                        'date': formatted_date,\n",
    "                        'content':content_text,\n",
    "                        'link' : url}\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cek = scrape_url('https://nasional.sindonews.com/read/1257753/14/lanjutkan-program-yudo-margono-panglima-tni-akan-naikkan-tunjangan-lauk-pauk-prajurit-1700629885')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Lanjutkan Program Yudo Margono, Panglima TNI Akan Naikkan Tunjangan Lauk-pauk Prajurit', 'date': '2023-11-22', 'content': 'JAKARTA - Panglima TNI Jenderal TNI Agus Subiyanto akan melanjutkan program-program Panglima TNI sebelumnya Laksamana TNI Yudo Margono . Ia berjanji akan menaikkan tunjangan lauk-pauk prajurit TNI.\"Mungkin saya juga akan melanjutkan program-program Panglima TNI yang lalu (Laksamana Yudo Margono) dan sesuai visi-misi waktu fit and proper test. Jadi ada visi dan misi saya adalah TNI yang prima yaitu profesional, responsif, integratif, modern dan adaptif,\" ujar Agus dalam keterangannya usai pelantikan di Istana Negara, Jakarta, Rabu (22/11/2023). Baca JugaSiapa Calon KSAD? Panglima TNI Agus Subiyanto: Kita Lihat Saja Bintang 3 Eligible Agus juga akan menaikkan tunjangan lauk-pauk prajurit TNI. Hal tersebut telah disampaikannya kepada Kementerian Pertahanan (Kemhan).\"Jadi tunjangannya harus dinaikkan. Nanti secara button up saya akan mengajukan kepada Kemhan. Dan Kemhan sudah ACC untuk tunjangan uang lauk pauk (bagi) pasukan yang melaksanakan tugas operasi. Sehingga tidak terlalu jomplang dengan instansi lain,\" jelasnya.Agus juga akan menggandeng industri pertahanan (Inhan) Indonesia untuk melengkapi perlengkapan prajurit TNI baik yang digunakan secara perorangan ataupun satuan.\"Seperti senjata, drone, dan juga kita akan mereorganisasi satuan-satuan drone. Kemudian satuan siber disesuaikan dengan perkembangan lingkungan strategis yang sekarang sedang terjadi,\" papar Agus.Mantan KSAD ini menambahkan bahwa TNI berhasil meraih juara pada ASEAN Armies Rifle Meet (AARM) di Thailand tahun ini. Dalam pergelaran tersebut, kata Agus, TNI menggunakan senjata dan alutsista buatan dalam negeri.Baca JugaDilantik Jadi Panglima TNI, Agus Subiyanto Dapat Arahan Khusus dari Jokowi \"Dan Alhamdulilah dengan menggunakan senjata yang dibuat oleh Pindad, AARM di Thailand yang sedang berlangsung dan kemarin sudah berlangsung Alhamdulillah personel TNI bisa meraih juara 1 AARM di Thailand. Ini biasanya AARM di negara-negara ASEAN secara bergantian dan tahun ini (di Thailand). Itu semua menggunakan alutsista buatan dalam negeri. Jadi kita harus berbangga produk dalam negeri,\" pungkasnya. Lihat Juga: Kunker 3 Negara, Wapres Dipastikan Absen Pelantikan Panglima TNI Jenderal Agus Subiyanto(kri)', 'link': 'https://nasional.sindonews.com/read/1257753/14/lanjutkan-program-yudo-margono-panglima-tni-akan-naikkan-tunjangan-lauk-pauk-prajurit-1700629885'}\n"
     ]
    }
   ],
   "source": [
    "print(cek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for url in link:\n",
    "#     data_tribunnews = scrape_url(url)\n",
    "#     print(data[\"keywords\"])\n",
    "#     keywords=data[\"keywords\"]\n",
    "#     if keywords.lower() in data_tribunnews['title'].lower() or keywords.lower() in data_tribunnews['content'].lower():\n",
    "#         print(\"Data contains keywords:\", data_tribunnews['link'])\n",
    "#     else:\n",
    "#         print(\"News does not contain the specified keywords and will not be inserted into the database. URL:\", data_tribunnews['link'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
