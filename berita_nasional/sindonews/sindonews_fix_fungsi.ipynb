{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from urllib.parse import urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"keywords\": \"anies\",\n",
    "    \"since_time\": \"2023-09-01\",\n",
    "    \"until_time\": \"2023-11-10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "440\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "url = f\"https://index.sindonews.com/index/0?t=2023-09-01\"\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# print(soup)\n",
    "cek = soup.find('div', {\"class\": \"pagination\"})\n",
    "links=cek.find_all('a')\n",
    "all_link=[]\n",
    "for link in links:\n",
    "        if link and 'href' in link.attrs:\n",
    "            link_href = link['href']\n",
    "            all_link.append(link_href)\n",
    "link=all_link[-1]\n",
    "# print(link)\n",
    "parsed_url = urlparse(link)\n",
    "number = parsed_url.path.split(\"/\")[-1]\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date, page_number):\n",
    "    input_date_str = datetime.strftime(date, \"%Y-%m-%d\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = f\"https://index.sindonews.com/index/5/{page_number}?t={input_date_str}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', {\"class\": \"indeks-title\"})\n",
    "    \n",
    "    links = []\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        links.append(link)\n",
    "        \n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    url = f\"https://index.sindonews.com/index/5/0?t={current_date}\"\n",
    "    response = requests.get(url + \"1\", headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cek = soup.find('div', {\"class\": \"pagination\"})\n",
    "    links = cek.find_all('a')\n",
    "    all_link = [link['href'] for link in links if link and 'href' in link.attrs]\n",
    "\n",
    "    if not all_link:\n",
    "        return []\n",
    "\n",
    "    last_link = all_link[-1]\n",
    "\n",
    "    # Extract the \"page\" parameter value from the last link\n",
    "    parsed_url = urlparse(last_link)\n",
    "    page_value = parsed_url.path.split(\"/\")[-1]\n",
    "    print(page_value)\n",
    "    # Check if page_value is an integer\n",
    "    try:\n",
    "        page_number = int(page_value)\n",
    "    except (TypeError, ValueError):\n",
    "        page_number = 0\n",
    "\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        # Use list comprehension to submit tasks to the thread pool\n",
    "        futures = [executor.submit(scrape_links, current_date, index) for index in range(0, page_number + 20, 20)]\n",
    "\n",
    "        # Collect results from completed futures\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "260\n",
      "Scraped 2 links from page 80 url https://index.sindonews.com/index/5/80?t=2023-09-01\n",
      "Scraped 20 links from page 40 url https://index.sindonews.com/index/5/40?t=2023-09-01\n",
      "Scraped 20 links from page 20 url https://index.sindonews.com/index/5/20?t=2023-09-01\n",
      "Scraped 20 links from page 60 url https://index.sindonews.com/index/5/60?t=2023-09-01\n",
      "Scraped 0 links from page 120 url https://index.sindonews.com/index/5/120?t=2023-09-01\n",
      "Scraped 20 links from page 0 url https://index.sindonews.com/index/5/0?t=2023-09-01\n",
      "Scraped 0 links from page 140 url https://index.sindonews.com/index/5/140?t=2023-09-01\n",
      "Scraped 0 links from page 160 url https://index.sindonews.com/index/5/160?t=2023-09-01\n",
      "Scraped 0 links from page 200 url https://index.sindonews.com/index/5/200?t=2023-09-01\n",
      "Scraped 0 links from page 180 url https://index.sindonews.com/index/5/180?t=2023-09-01\n",
      "Scraped 0 links from page 220 url https://index.sindonews.com/index/5/220?t=2023-09-01\n",
      "Scraped 0 links from page 260 url https://index.sindonews.com/index/5/260?t=2023-09-01\n",
      "Scraped 0 links from page 100 url https://index.sindonews.com/index/5/100?t=2023-09-01\n",
      "Scraped 0 links from page 240 url https://index.sindonews.com/index/5/240?t=2023-09-01\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day(data[\"since_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "print(len(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('div', {\"class\": \"detail-title\"})\n",
    "                    if title_elem:\n",
    "                        title_text = title_elem.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"   \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('div', {\"class\": \"detail-date-artikel\"})\n",
    "                    if date_elem:\n",
    "                        date_text = date_elem.text.strip()\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"detail-desc\"})\n",
    "                    \n",
    "                    if body_elem:\n",
    "                        content_text = body_elem.text\n",
    "                        content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                        content_text = ' '.join(content_text.split())\n",
    "                    else:\n",
    "                        content_text =\"Content not found\"\n",
    "\n",
    "                    return{\n",
    "                        'title': title_text,\n",
    "                        'date': date_text,\n",
    "                        'content':content_text,\n",
    "                        'link' : url}\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anies\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://nasional.sindonews.com/read/1190259/15/sederet-tokoh-terima-penghargaan-indonesia-awards-2023-hary-tanoe-mereka-layak-1693501764\n",
      "anies\n",
      "Data contains keywords: https://nasional.sindonews.com/read/1190255/12/pkb-merapat-ke-nasdem-pengamat-prediksi-suara-prabowo-bakal-gembos-1693501782\n",
      "anies\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://nasional.sindonews.com/read/1190677/12/daftar-lengkap-10-penjabat-gubernur-yang-segera-dilantik-gantikan-ganjar-hingga-ridwan-kamil-1693552153\n",
      "anies\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://nasional.sindonews.com/read/1190673/12/partai-perindo-unity-kesejahteraan-dan-masa-depan-anak-muda-1693552179\n",
      "anies\n",
      "Data contains keywords: https://nasional.sindonews.com/read/1190663/12/diisukan-jadi-cawapres-anies-ini-perbandingan-elektabilitas-cak-imin-dengan-ahy-1693552185\n",
      "anies\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://nasional.sindonews.com/read/1190647/12/cawapres-ganjar-dibahas-megawati-mardiono-sandiaga-uno-ibarat-menikah-orang-tua-harus-sepakat-1693552193\n",
      "anies\n",
      "News does not contain the specified keywords and will not be inserted into the database. URL: https://nasional.sindonews.com/read/1190971/15/unggul-tata-kelola-dan-manajemen-risiko-perhutani-terima-4-penghargaan-1693570197\n",
      "anies\n",
      "Data contains keywords: https://nasional.sindonews.com/read/1190633/12/anies-berduet-dengan-cak-imin-demokrat-bakal-gabung-dukung-ganjar-1693548552\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m url \u001b[39min\u001b[39;00m link:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     data_tribunnews \u001b[39m=\u001b[39m scrape_url(url)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(data[\u001b[39m\"\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     keywords\u001b[39m=\u001b[39mdata[\u001b[39m\"\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[1;32m/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb Cell 9\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     headers \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mUser-Agent\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     }\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url, headers\u001b[39m=\u001b[39mheaders)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb#X12sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m200\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/sindonews/sindonews_fix_fungsi.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:73\u001b[0m, in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(url, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     63\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \n\u001b[1;32m     65\u001b[0m \u001b[39m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m    :rtype: requests.Response\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m request(\u001b[39m\"\u001b[39m\u001b[39mget\u001b[39m\u001b[39m\"\u001b[39m, url, params\u001b[39m=\u001b[39mparams, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[39mwith\u001b[39;00m sessions\u001b[39m.\u001b[39mSession() \u001b[39mas\u001b[39;00m session:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m session\u001b[39m.\u001b[39mrequest(method\u001b[39m=\u001b[39mmethod, url\u001b[39m=\u001b[39murl, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msend(prep, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/sessions.py:747\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n\u001b[0;32m--> 747\u001b[0m     r\u001b[39m.\u001b[39mcontent\n\u001b[1;32m    749\u001b[0m \u001b[39mreturn\u001b[39;00m r\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/models.py:899\u001b[0m, in \u001b[0;36mResponse.content\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    898\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 899\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_content(CONTENT_CHUNK_SIZE)) \u001b[39mor\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    901\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_content_consumed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    902\u001b[0m \u001b[39m# don't need to release the connection; that's been handled by urllib3\u001b[39;00m\n\u001b[1;32m    903\u001b[0m \u001b[39m# since we exhausted the data.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:933\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    917\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    918\u001b[0m \u001b[39mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m    919\u001b[0m \u001b[39m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[39m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    932\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunked \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m--> 933\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread_chunked(amt, decode_content\u001b[39m=\u001b[39mdecode_content)\n\u001b[1;32m    934\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp) \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoded_buffer) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:1073\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1073\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_chunk_length()\n\u001b[1;32m   1074\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1075\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/urllib3/response.py:1001\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    999\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_left \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1001\u001b[0m line \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline()  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1003\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for url in link:\n",
    "    data_tribunnews = scrape_url(url)\n",
    "    print(data[\"keywords\"])\n",
    "    keywords=data[\"keywords\"]\n",
    "    if keywords.lower() in data_tribunnews['title'].lower() or keywords.lower() in data_tribunnews['content'].lower():\n",
    "        print(\"Data contains keywords:\", data_tribunnews['link'])\n",
    "    else:\n",
    "        print(\"News does not contain the specified keywords and will not be inserted into the database. URL:\", data_tribunnews['link'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
