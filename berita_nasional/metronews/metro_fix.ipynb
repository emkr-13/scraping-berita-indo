{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from dateutil import parser,tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.metrotvnews.com/read/kewCaRZl-kpu-hapus-debat-khusus-cawapres', 'https://www.metrotvnews.com/read/bmRCepjy-saka-disebut-butuh-istirahat', 'https://www.metrotvnews.com/read/bzGCzLAX-pelatih-prancis-anggap-final-piala-dunia-u-17-seperti-laga-biasa', 'https://www.metrotvnews.com/read/bw6CoP4y-striker-prancis-minta-dukungan-lebih-besar-di-final-piala-dunia-u-17', 'https://www.metrotvnews.com/read/bD2C1qPa-kemenpora-dorong-pencak-silat-masuk-olimpiade', 'https://www.metrotvnews.com/read/NLMC2x69-jubir-amin-sebut-contract-farming-upayakan-hak-milik-lahan-petani', 'https://www.metrotvnews.com/read/b2lCVzjE-firli-singgung-ham-dan-serangan-balik-koruptor-usai-diperiksa-sebagai-tersangka', 'https://www.metrotvnews.com/read/NrWC5Pvd-format-baru-debat-cawapres-kini-bakal-didampingi-capres', 'https://www.metrotvnews.com/read/KRXC5VvV-sinergitas-jadi-kunci-sukses-piala-dunia-u-17', 'https://www.metrotvnews.com/read/NxGCzR4o-firli-bahuri-saling-pandang-dengan-alex-tirta-saat-diperiksa', 'https://www.metrotvnews.com/read/NA0CjpD6-penampilan-odegaard-musim-ini-merosot', 'https://www.metrotvnews.com/read/K5nCLYqQ-amin-berkomitmen-memperbaiki-kekurangan-pembangunan-nasional', 'https://www.metrotvnews.com/read/KZmCdja1-timnas-jerman-u-17-berambisi-kalahkan-prancis-demi-cetak-sejarah', 'https://www.metrotvnews.com/read/kj2CnP2z-fraksi-pkb-tolak-percepatan-pilkada-2024', 'https://www.metrotvnews.com/read/KXyCAdxz-3-skenario-dishub-surakarta-dalam-menjaga-kenyamanan-final-piala-dunia-u-17', 'https://www.metrotvnews.com/play/kpLCWZY5-ksad-maruli-siap-tegakkan-netralitas-tni', 'https://www.metrotvnews.com/read/kELCxg4m-jesus-rileks-dengan-rumor-toney', 'https://www.metrotvnews.com/read/N4ECJ1Dz-cak-imin-harapan-soal-perubahan-tak-terbendung', 'https://www.metrotvnews.com/play/bJECarw3-gus-imin-hadiri-undangan-masyarakat-nonton-voli-kampung', 'https://www.metrotvnews.com/read/Ky6CPMJ0-balas-serangan-sipil-palestina-alasan-brigade-qassam-serang-israel', 'https://www.metrotvnews.com/play/NG9C3AG1-prabowo-ingatkan-bahaya-pertahanan-negara-yang-lemah', 'https://www.metrotvnews.com/read/N0BCv7pd-firli-dicecar-40-pertanyaan-seputar-pemerasan-syl', 'https://www.metrotvnews.com/read/KdZCWvLe-polisi-berpangkat-brigjen-ikut-diperiksa-kasus-pemerasan-syl', 'https://www.metrotvnews.com/play/b7WCYlnJ-diperiksa-10-jam-firli-bahuri-lolos-dari-penahanan', 'https://www.metrotvnews.com/read/kM6Ca1jo-alasan-polri-tak-menahan-firli', 'https://www.metrotvnews.com/play/NP6Cpe5v-prabowo-gibran-hadiri-rakornas-tkn', 'https://www.metrotvnews.com/read/N9nCnqL5-firli-bahuri-siap-jalani-proses-hukum', 'https://www.metrotvnews.com/play/N6GCgv25-pemkot-cimahi-gandeng-pemkab-garut-kendalikan-inflasi', 'https://www.metrotvnews.com/read/kqYCxOR9-firli-tak-ditahan-usai-diperiksa-sebagai-tersangka-pemerasan', 'https://www.metrotvnews.com/read/b1oC9zMn-ulama-dan-tokoh-agama-banten-deklarasi-damai-untuk-hindari-perpecahan']\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "url = f\"https://www.metrotvnews.com/index/2023-12-01/0\"\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "body= soup.find('div',{\"class\":\"item-list pt-20\"})\n",
    "articles = body.findAll('div', {\"class\": \"text\"})\n",
    "links = []\n",
    "for article in articles:\n",
    "    link = article.find('a')['href']\n",
    "    full_link = f\"https://www.metrotvnews.com{link}\"\n",
    "    links.append(full_link)\n",
    "    \n",
    "print(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date, page_number):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = f\"https://www.metrotvnews.com/index/{date}/{page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    body= soup.find('div',{\"class\":\"item-list pt-20\"})\n",
    "    articles =  body.find_all('div', {\"class\": \"text\"})\n",
    "    \n",
    "    links = []\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        full_link = f\"https://www.metrotvnews.com{link}\"\n",
    "        links.append(full_link)\n",
    "        \n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    page_number = 0\n",
    "    page_links = []\n",
    "    # Adjust the date format here\n",
    "    # current_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    # formatted_date = current_date.strftime(\"%Y/%m/%d\")\n",
    "    # print(formatted_date)\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        futures = []\n",
    "\n",
    "        while True:\n",
    "            future = executor.submit(scrape_links, date, page_number)\n",
    "            futures.append(future)\n",
    "            page_number += 1\n",
    "\n",
    "            # Break the loop if no more articles are found\n",
    "            if not future.result():\n",
    "                break\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 30 links from page 0 url https://www.metrotvnews.com/index/2023-11-02/0\n",
      "Scraped 30 links from page 1 url https://www.metrotvnews.com/index/2023-11-02/1\n",
      "Scraped 30 links from page 2 url https://www.metrotvnews.com/index/2023-11-02/2\n",
      "Scraped 30 links from page 3 url https://www.metrotvnews.com/index/2023-11-02/3\n",
      "Scraped 30 links from page 4 url https://www.metrotvnews.com/index/2023-11-02/4\n",
      "Scraped 30 links from page 5 url https://www.metrotvnews.com/index/2023-11-02/5\n",
      "Scraped 30 links from page 6 url https://www.metrotvnews.com/index/2023-11-02/6\n",
      "Scraped 23 links from page 7 url https://www.metrotvnews.com/index/2023-11-02/7\n",
      "Scraped 0 links from page 8 url https://www.metrotvnews.com/index/2023-11-02/8\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day('2023-11-02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233\n"
     ]
    }
   ],
   "source": [
    "print(len(link))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url, max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # Judul Berita\n",
    "                title_elem = soup.find('div', {\"class\": \"title\"})\n",
    "                if title_elem:\n",
    "                    title_text = title_elem.text.strip()\n",
    "                else:\n",
    "                    title_text = \"Title not found\"\n",
    "\n",
    "                # Tanggal berita\n",
    "                date_elem = soup.find('div', {\"class\": \"reporter\"})\n",
    "                if date_elem:\n",
    "                    date_text = date_elem.text.strip()\n",
    "                    # Define a custom timezone information for WIB\n",
    "                    def wib_tz(offset):\n",
    "                        return tz.tzoffset('WIB', offset)\n",
    "\n",
    "                    # Parse the date string with the custom timezone information\n",
    "                    date_object = parser.parse(date_text, tzinfos={'WIB': 7*60}, fuzzy=True)\n",
    "\n",
    "                    # Format the datetime object to the desired format\n",
    "                    formatted_date = date_object.strftime('%Y-%m-%d')\n",
    "\n",
    "                else:\n",
    "                    date_text = \"Date not found\"\n",
    "\n",
    "                # Content Berita\n",
    "                body_elem = soup.find('div', {\"class\": \"read\"})\n",
    "\n",
    "                if body_elem:\n",
    "                    content_text = body_elem.text\n",
    "                    content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                    content_text = ' '.join(content_text.split())\n",
    "                else:\n",
    "                    content_text = \"Content not found\"\n",
    "\n",
    "                return {\n",
    "                    'title': title_text,\n",
    "                    'date': formatted_date,\n",
    "                    'content': content_text,\n",
    "                    'link': url\n",
    "                }\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching URL '{url}': {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL '{url}': {e}\")\n",
    "        retries += 1\n",
    "        if retries < max_retries:\n",
    "            print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "            time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url, max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            headers = {\n",
    "                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "            }\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                # Judul Berita\n",
    "                title_elem = soup.find('h1')\n",
    "                if title_elem:\n",
    "                    title_text = title_elem.text.strip()\n",
    "                else:\n",
    "                    title_text = \"Title not found\"\n",
    "\n",
    "                # Tanggal berita\n",
    "                date_elem = soup.find('p', {\"class\": \"pt-20 date\"})\n",
    "                if date_elem:\n",
    "                    date_text = date_elem.text.strip()\n",
    "                    parsed_date = datetime.strptime(date_text, 'Media Indonesia • %d %B %Y %H:%M')\n",
    "\n",
    "                    # Format the date as 'yyyy-mm-dd'\n",
    "                    formatted_date = parsed_date.strftime('%Y-%m-%d')\n",
    "\n",
    "                else:\n",
    "                    date_text = \"Date not found\"\n",
    "\n",
    "                # Content Berita\n",
    "                body_elem = soup.find('div', {\"class\": \"news-text\"})\n",
    "\n",
    "                if body_elem:\n",
    "                    content_text = body_elem.text\n",
    "                    content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                    content_text = ' '.join(content_text.split())\n",
    "                else:\n",
    "                    content_text = \"Content not found\"\n",
    "\n",
    "                return {\n",
    "                    'title': title_text,\n",
    "                    'date': formatted_date,\n",
    "                    'content': content_text,\n",
    "                    'link': url\n",
    "                }\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching URL '{url}': {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing URL '{url}': {e}\")\n",
    "        retries += 1\n",
    "        if retries < max_retries:\n",
    "            print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "            time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cek = scrape_url('https://www.metrotvnews.com/read/kewCaRZl-kpu-hapus-debat-khusus-cawapres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'KPU Hapus Debat Khusus Cawapres',\n",
       " 'date': '2023-12-01',\n",
       " 'content': 'Jakarta: Komisi Pemilihan Umum (KPU) membuat format baru Debat Pemilihan Presiden (Pilpres) 2024. Format baru tersebut membuat debat khusus antarcawapres ditiadakan karena didampingi calon presiden (capres).Ketua KPU RI Hasyim Asy\\'ari menjelaskan kebijakan itu dibuat. Penyelenggara pemilu ingin pemilih melihat sejauh mana kerja sama yang dibangun antara capres dan cawapresnya.\"Sehingga kemudian publik makin yakin lah teamwork antara capres dan cawapres dalam penampilan di debat,\" kata Hasyim saat dikonfirmasi, Jumat, 1 Desember 2023.Eks komisioner KPU periode 2027-2022 itu menyampaikan lima rangkaian debat itu diikuti setiap pasangan calon (paslon) Pilpres 2024. Namun, dibagi menjadi debat capres sebanyak tiga kali dan cawapres dua kali. Baca juga: Format Baru, Debat Cawapres Kini Bakal Didampingi CapresPada debat capres, porsi bicara akan lebih banyak diberikan kepada calon RI 1. Sedangkan debat cawapres, waktu bicara akan diberikan kepada calon RI 2.\"Pada saat debat capres, maka proporsinya capres untuk bicara lebih banyak. Ketika debat cawapres, proporsinya maka untuk cawapres lebih banyak,\" jelasnya.Format debat pada Pilpres 2024 berbeda dengan Pilpres 2019. Saat itu, KPU membagi lima kegiatan debat dalam komposisi dua kali debat khusus capres, satu kali debat khusus cawapres, dan dua kali debat capres-cawapres.Direktur Democracy and Electoral Empowerment Partnership (DEEP) Indonesia Neni Nur Hayati menilai KPU memang diberikan kewenangan untuk mengatur teknis pelaksanaan debat capres-cawapres. Namun, ia mengingatkan KPU untuk bersikap adil dalam memperlakukan setiap pasangan calon.\"Jangan sampai ada tindakan yang merugikan dan menguntungkan pasangan calon lainnya,\" kata Neni. (MI/Tri Subarkah)',\n",
       " 'link': 'https://www.metrotvnews.com/read/kewCaRZl-kpu-hapus-debat-khusus-cawapres'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cek"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
