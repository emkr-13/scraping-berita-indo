{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures\n",
    "from urllib.parse import urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"keywords\": \"anies\",\n",
    "    \"since_time\": \"2023-09-01\",\n",
    "    \"until_time\": \"2023-11-10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "descriptor 'strftime' for 'datetime.date' objects doesn't apply to a 'str' object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/krisna/Crawling/crawler_berita_indo/berita_nasional/okezone/okezone_fix.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/okezone/okezone_fix.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m input_date_str \u001b[39m=\u001b[39m datetime\u001b[39m.\u001b[39mstrftime(data[\u001b[39m'\u001b[39m\u001b[39msince_time\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mY-\u001b[39m\u001b[39m%\u001b[39m\u001b[39mm-\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/krisna/Crawling/crawler_berita_indo/berita_nasional/okezone/okezone_fix.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(input_date_str)\n",
      "\u001b[0;31mTypeError\u001b[0m: descriptor 'strftime' for 'datetime.date' objects doesn't apply to a 'str' object"
     ]
    }
   ],
   "source": [
    "# input_date_str = datetime.strftime(data['since_time'], \"%Y-%m-%d\")\n",
    "input_date_str=data['since_time']\n",
    "print(input_date_str)\n",
    "# headers = {\n",
    "#         'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "#     }\n",
    "# url = f\"https://index.okezone.com/bydate/channel/2023/09/01/1\"\n",
    "# response = requests.get(url, headers=headers)\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# response = requests.get(url, headers=headers)\n",
    "# soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# articles = soup.find_all('h4', {\"class\": \"f17\"})\n",
    "# for article in articles:\n",
    "#     link = article.find('a')['href']\n",
    "#     print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='https', netloc='index.okezone.com', path='/bydate/channel/2023/09/01/1/240/', params='', query='', fragment='')\n",
      "240\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "url = f\"https://index.okezone.com/bydate/channel/2023/09/01/1\"\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# print(soup)\n",
    "cek = soup.find('div', {\"class\": \"time r1 fl bg1 b1\"})\n",
    "links=cek.find_all('a')\n",
    "all_link=[]\n",
    "for link in links:\n",
    "        if link and 'href' in link.attrs:\n",
    "            link_href = link['href']\n",
    "            all_link.append(link_href)\n",
    "link=all_link[-1]\n",
    "# print(link)\n",
    "parsed_url = urlparse(link)\n",
    "print(parsed_url)\n",
    "number = parsed_url.path.split(\"/\")[-2]\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date, page_number):\n",
    "    input_date_str = datetime.strftime(date, \"%Y-%m-%d\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = f\"https://index.sindonews.com/index/5/{page_number}?t={input_date_str}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    articles = soup.find_all('div', {\"class\": \"indeks-title\"})\n",
    "    \n",
    "    links = []\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        links.append(link)\n",
    "        \n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    current_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    url = f\"https://index.sindonews.com/index/5/0?t={current_date}\"\n",
    "    response = requests.get(url + \"1\", headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    cek = soup.find('div', {\"class\": \"pagination\"})\n",
    "    links = cek.find_all('a')\n",
    "    all_link = [link['href'] for link in links if link and 'href' in link.attrs]\n",
    "\n",
    "    if not all_link:\n",
    "        return []\n",
    "\n",
    "    last_link = all_link[-1]\n",
    "\n",
    "    # Extract the \"page\" parameter value from the last link\n",
    "    parsed_url = urlparse(last_link)\n",
    "    page_value = parsed_url.path.split(\"/\")[-2]\n",
    "    print(page_value)\n",
    "    # Check if page_value is an integer\n",
    "    try:\n",
    "        page_number = int(page_value)\n",
    "    except (TypeError, ValueError):\n",
    "        page_number = 0\n",
    "\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        # Use list comprehension to submit tasks to the thread pool\n",
    "        futures = [executor.submit(scrape_links, current_date, index) for index in range(0, page_number + 20, 20)]\n",
    "\n",
    "        # Collect results from completed futures\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
