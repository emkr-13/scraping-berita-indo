{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import concurrent.futures\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"keywords\": \"jokowi\",\n",
    "    \"since_time\": \"2023-10-16\",\n",
    "    \"until_time\": \"2023-10-19\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pagination(keywords, since_time, until_time):\n",
    "    driver = webdriver.Chrome()\n",
    "    current_date = datetime.strptime(since_time, \"%Y-%m-%d\")\n",
    "    until_date = datetime.strptime(until_time, \"%Y-%m-%d\")\n",
    "    result_data = []  # Membuat list kosong untuk menyimpan data\n",
    "    \n",
    "    while current_date <= until_date:\n",
    "        date_str = current_date.strftime('%Y/%m/%d')\n",
    "        url = f\"https://www.cnnindonesia.com/search?query={keywords}&date={date_str}\"\n",
    "        driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            total_search_element = WebDriverWait(driver, 300).until(\n",
    "                EC.presence_of_element_located((By.ID, \"total-search\"))\n",
    "            )\n",
    "\n",
    "            total_search_text = total_search_element.text\n",
    "            match = re.search(r'\\d+', total_search_text)\n",
    "            if match:\n",
    "                numeric_value = int(match.group())\n",
    "                hasil_divided = numeric_value / 10\n",
    "                page_index = round(hasil_divided)\n",
    "                result_data.append({\n",
    "                        'keywords':keywords,\n",
    "                        'tanggal_berita': date_str,\n",
    "                        'jumlah_index': page_index\n",
    "                 })\n",
    "            else:\n",
    "                print(f\"No numeric value found in total_search_text for date {date_str}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for date {date_str}: {str(e)}\")\n",
    "            \n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    driver.quit()\n",
    "    return result_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'keywords': 'jokowi', 'tanggal_berita': '2023/10/16', 'jumlah_index': 4}, {'keywords': 'jokowi', 'tanggal_berita': '2023/10/17', 'jumlah_index': 5}, {'keywords': 'jokowi', 'tanggal_berita': '2023/10/18', 'jumlah_index': 4}, {'keywords': 'jokowi', 'tanggal_berita': '2023/10/19', 'jumlah_index': 4}]\n"
     ]
    }
   ],
   "source": [
    "data_tanggal = scrape_pagination(data[\"keywords\"], data[\"since_time\"], data[\"until_time\"])\n",
    "print(data_tanggal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(page_number, keywords, date, link_list):\n",
    "    driver = webdriver.Chrome()\n",
    "    try:\n",
    "        url = f\"https://www.cnnindonesia.com/search?query={keywords}&date={date}&page={page_number}\"\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            div_badan = WebDriverWait(driver, 300).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".flex.flex-col.gap-5\"))\n",
    "            )\n",
    "\n",
    "            if div_badan:\n",
    "                page_links = []\n",
    "                articles = div_badan.find_elements(By.CSS_SELECTOR, \".flex-grow\")\n",
    "                for article in articles:\n",
    "                    # Wrap this element locating code in a try block\n",
    "                    try:\n",
    "                        link = WebDriverWait(article, 300).until(\n",
    "                            EC.presence_of_element_located((By.TAG_NAME, 'a'))\n",
    "                        )\n",
    "                        href = link.get_attribute('href')\n",
    "                        page_links.append(href)\n",
    "                    except StaleElementReferenceException:\n",
    "                        # Handle StaleElementReferenceException by re-locating the element\n",
    "                        link = WebDriverWait(article, 300).until(\n",
    "                            EC.presence_of_element_located((By.TAG_NAME, 'a'))\n",
    "                        )\n",
    "                        href = link.get_attribute('href')\n",
    "                        page_links.append(href)\n",
    "        except StaleElementReferenceException:\n",
    "            # Handle StaleElementReferenceException by re-locating the div_badan element\n",
    "            div_badan = WebDriverWait(driver, 300).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".flex.flex-col.gap-5\"))\n",
    "            )\n",
    "            page_links = []\n",
    "            articles = div_badan.find_elements(By.CSS_SELECTOR, \".flex-grow\")\n",
    "            for article in articles:\n",
    "                link = WebDriverWait(article, 60).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, 'a'))\n",
    "                )\n",
    "                href = link.get_attribute('href')\n",
    "                page_links.append(href)\n",
    "\n",
    "        print(f\"Scraped {len(page_links)} links from page {page_number} date {date}\")\n",
    "        link_list.extend(page_links)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scraping page {page_number}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_links(data_tanggal):\n",
    "    link_list = []  # Create an empty list to store all the links\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # You can adjust the number of threads as needed\n",
    "        for item in data_tanggal:\n",
    "            keywords = item['keywords']\n",
    "            date = item['tanggal_berita']\n",
    "            max_pages = item['jumlah_index']\n",
    "            for page_number in range(1, max_pages+1):  # Specify the range of pages you want to scrape\n",
    "                executor.submit(scrape_links, page_number, keywords, date, link_list)\n",
    "    \n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 10 links from page 2 date 2023/10/16\n",
      "Scraped 10 links from page 1 date 2023/10/16\n",
      "Scraped 8 links from page 4 date 2023/10/16\n",
      "Scraped 10 links from page 3 date 2023/10/16\n",
      "Scraped 10 links from page 3 date 2023/10/17\n",
      "Scraped 10 links from page 1 date 2023/10/17\n",
      "Scraped 10 links from page 2 date 2023/10/17\n",
      "Scraped 10 links from page 4 date 2023/10/17\n",
      "Scraped 10 links from page 2 date 2023/10/18\n",
      "Scraped 10 links from page 1 date 2023/10/18\n",
      "Scraped 8 links from page 5 date 2023/10/17\n",
      "Scraped 10 links from page 3 date 2023/10/18\n",
      "Scraped 10 links from page 1 date 2023/10/19\n",
      "Scraped 10 links from page 2 date 2023/10/19\n",
      "Scraped 10 links from page 4 date 2023/10/18\n",
      "Scraped 5 links from page 4 date 2023/10/19\n",
      "Scraped 10 links from page 3 date 2023/10/19\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "all_links = scrape_all_links(data_tanggal)\n",
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('h1', {\"class\": \"mb-2 text-[28px] leading-9 text-cnn_black\"})\n",
    "                    if title_elem:\n",
    "                        title_text = title_elem.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"\n",
    "                    # Author berita\n",
    "                    author_elem = soup.find('span', {\"class\": \"text-cnn_red\"})\n",
    "                    if author_elem:\n",
    "                        author_text = author_elem.get_text()\n",
    "                        author_text = author_text.split('-')[0].strip()\n",
    "                    else:\n",
    "                        author_text = \"Author not found\"     \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('div', {\"class\": \"text-cnn_grey text-sm mb-4\"})\n",
    "                    if date_elem:\n",
    "                        date_text = date_elem.text.strip()\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Category berita\n",
    "                    category_elements = soup.find('a', {\"dtr-sec\": \"subkanal\"})\n",
    "                    if category_elements:\n",
    "                        category_text= category_elements.text.strip()\n",
    "                    else:\n",
    "                        category_text = \"Category not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"detail-text text-cnn_black text-sm grow min-w-0\"})\n",
    "                    \n",
    "                    if body_elem:\n",
    "                        content_elem = body_elem.find_all('p')\n",
    "                        content_text = \"\"\n",
    "                        for p in content_elem:\n",
    "                            content_text += p.text.strip() + \"\\n\"\n",
    "                        \n",
    "                        if content_text.strip():\n",
    "                            content_text=content_text\n",
    "                        else:\n",
    "                            content_text =\"Content not found\"\n",
    "                    else:\n",
    "                        content_text =\"Content not found\"\n",
    "\n",
    "                    nama_berita_match = re.search(r'https://www\\.(\\w+)\\.com/', url)\n",
    "                    if nama_berita_match:\n",
    "                        nama_berita = nama_berita_match.group(1)\n",
    "                    else:\n",
    "                        nama_berita = \"Nama_berita not found\"\n",
    "                    results.append({\n",
    "                        'title': title_text,\n",
    "                                    'category':category_text,\n",
    "                                    'link' : url})\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None         "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
