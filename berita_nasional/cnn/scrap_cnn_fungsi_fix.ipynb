{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import concurrent.futures\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"keywords\": \"jokowi\",\n",
    "    \"since_time\": \"2023-10-16\",\n",
    "    \"until_time\": \"2023-10-19\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pagination(keywords, since_time, until_time):\n",
    "    driver = webdriver.Chrome()\n",
    "    current_date = datetime.strptime(since_time, \"%Y-%m-%d\")\n",
    "    until_date = datetime.strptime(until_time, \"%Y-%m-%d\")\n",
    "    result_data = []  # Membuat list kosong untuk menyimpan data\n",
    "    \n",
    "    while current_date <= until_date:\n",
    "        date_str = current_date.strftime('%Y/%m/%d')\n",
    "        url = f\"https://www.cnnindonesia.com/search?query={keywords}&date={date_str}\"\n",
    "        driver.get(url)\n",
    "        \n",
    "        try:\n",
    "            total_search_element = WebDriverWait(driver, 300).until(\n",
    "                EC.presence_of_element_located((By.ID, \"total-search\"))\n",
    "            )\n",
    "\n",
    "            total_search_text = total_search_element.text\n",
    "            match = re.search(r'\\d+', total_search_text)\n",
    "            if match:\n",
    "                numeric_value = int(match.group())\n",
    "                hasil_divided = numeric_value / 10\n",
    "                page_index = round(hasil_divided)\n",
    "                result_data.append({\n",
    "                        'keywords':keywords,\n",
    "                        'tanggal_berita': date_str,\n",
    "                        'jumlah_index': page_index\n",
    "                 })\n",
    "            else:\n",
    "                print(f\"No numeric value found in total_search_text for date {date_str}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for date {date_str}: {str(e)}\")\n",
    "            \n",
    "        current_date += timedelta(days=1)\n",
    "\n",
    "    driver.quit()\n",
    "    return result_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'keywords': 'jokowi', 'tanggal_berita': '2023/10/16', 'jumlah_index': 4}, {'keywords': 'jokowi', 'tanggal_berita': '2023/10/17', 'jumlah_index': 5}, {'keywords': 'jokowi', 'tanggal_berita': '2023/10/18', 'jumlah_index': 4}, {'keywords': 'jokowi', 'tanggal_berita': '2023/10/19', 'jumlah_index': 4}]\n"
     ]
    }
   ],
   "source": [
    "data_tanggal = scrape_pagination(data[\"keywords\"], data[\"since_time\"], data[\"until_time\"])\n",
    "print(data_tanggal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(page_number, keywords, date, link_list):\n",
    "    driver = webdriver.Chrome()\n",
    "    try:\n",
    "        url = f\"https://www.cnnindonesia.com/search?query={keywords}&date={date}&page={page_number}\"\n",
    "        driver.get(url)\n",
    "\n",
    "        try:\n",
    "            div_badan = WebDriverWait(driver, 300).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".flex.flex-col.gap-5\"))\n",
    "            )\n",
    "\n",
    "            if div_badan:\n",
    "                page_links = []\n",
    "                articles = div_badan.find_elements(By.CSS_SELECTOR, \".flex-grow\")\n",
    "                for article in articles:\n",
    "                    # Wrap this element locating code in a try block\n",
    "                    try:\n",
    "                        link = WebDriverWait(article, 300).until(\n",
    "                            EC.presence_of_element_located((By.TAG_NAME, 'a'))\n",
    "                        )\n",
    "                        href = link.get_attribute('href')\n",
    "                        page_links.append(href)\n",
    "                    except StaleElementReferenceException:\n",
    "                        # Handle StaleElementReferenceException by re-locating the element\n",
    "                        link = WebDriverWait(article, 300).until(\n",
    "                            EC.presence_of_element_located((By.TAG_NAME, 'a'))\n",
    "                        )\n",
    "                        href = link.get_attribute('href')\n",
    "                        page_links.append(href)\n",
    "        except StaleElementReferenceException:\n",
    "            # Handle StaleElementReferenceException by re-locating the div_badan element\n",
    "            div_badan = WebDriverWait(driver, 300).until(\n",
    "                EC.presence_of_element_located((By.CSS_SELECTOR, \".flex.flex-col.gap-5\"))\n",
    "            )\n",
    "            page_links = []\n",
    "            articles = div_badan.find_elements(By.CSS_SELECTOR, \".flex-grow\")\n",
    "            for article in articles:\n",
    "                link = WebDriverWait(article, 60).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, 'a'))\n",
    "                )\n",
    "                href = link.get_attribute('href')\n",
    "                page_links.append(href)\n",
    "\n",
    "        print(f\"Scraped {len(link_list)} links from page {page_number} date {date}\")\n",
    "        link_list.extend(page_links)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while scraping page {page_number}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_links(data_tanggal):\n",
    "    link_list = []  # Create an empty list to store all the links\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # You can adjust the number of threads as needed\n",
    "        for item in data_tanggal:\n",
    "            keywords = item['keywords']\n",
    "            date = item['tanggal_berita']\n",
    "            max_pages = item['jumlah_index']\n",
    "            for page_number in range(1, max_pages+1):  # Specify the range of pages you want to scrape\n",
    "                executor.submit(scrape_links, page_number, keywords, date, link_list)\n",
    "    \n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 0 links from page 4 date 2023/10/16\n",
      "Scraped 8 links from page 1 date 2023/10/16\n",
      "Scraped 18 links from page 3 date 2023/10/16\n",
      "Scraped 28 links from page 1 date 2023/10/17\n",
      "Scraped 38 links from page 2 date 2023/10/16\n",
      "Scraped 48 links from page 2 date 2023/10/17\n",
      "Scraped 58 links from page 3 date 2023/10/17\n",
      "Scraped 68 links from page 4 date 2023/10/17\n",
      "Scraped 78 links from page 5 date 2023/10/17\n",
      "Scraped 86 links from page 1 date 2023/10/18\n",
      "Scraped 96 links from page 4 date 2023/10/18\n",
      "Scraped 106 links from page 2 date 2023/10/18\n",
      "Scraped 116 links from page 3 date 2023/10/18\n",
      "Scraped 126 links from page 4 date 2023/10/19\n",
      "Scraped 131 links from page 3 date 2023/10/19\n",
      "Scraped 141 links from page 1 date 2023/10/19\n",
      "Scraped 151 links from page 2 date 2023/10/19\n",
      "161\n"
     ]
    }
   ],
   "source": [
    "all_links = scrape_all_links(data_tanggal)\n",
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
