{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Detik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mengecek Link status Ok\n",
    "url = \"https://news.detik.com/index\"\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(page_number):\n",
    "    url = f\"https://news.detik.com/indeks/{page_number}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all('article', {\"class\": \"list-content__item\"})\n",
    "    \n",
    "    page_links = []\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        page_links.append(link)\n",
    "    \n",
    "    print(f\"Scraped {len(page_links)} links from page {page_number}\")\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 20 links from page 11\n",
      "Scraped 20 links from page 2\n",
      "Scraped 20 links from page 4\n",
      "Scraped 20 links from page 12\n",
      "Scraped 20 links from page 10\n",
      "Scraped 20 links from page 9\n",
      "Scraped 20 links from page 20\n",
      "Scraped 20 links from page 19\n",
      "Scraped 20 links from page 15\n",
      "Scraped 20 links from page 18\n",
      "Scraped 20 links from page 14\n",
      "Scraped 20 links from page 7\n",
      "Scraped 20 links from page 3\n",
      "Scraped 20 links from page 13\n",
      "Scraped 20 links from page 46\n",
      "Scraped 20 links from page 35\n",
      "Scraped 20 links from page 45\n",
      "Scraped 20 links from page 34\n",
      "Scraped 20 links from page 49\n",
      "Scraped 20 links from page 42\n",
      "Scraped 20 links from page 28\n",
      "Scraped 20 links from page 48\n",
      "Scraped 20 links from page 25\n",
      "Scraped 20 links from page 38\n",
      "Scraped 20 links from page 44\n",
      "Scraped 20 links from page 5\n",
      "Scraped 20 links from page 32\n",
      "Scraped 20 links from page 37\n",
      "Scraped 20 links from page 16\n",
      "Scraped 20 links from page 8\n",
      "Scraped 20 links from page 29\n",
      "Scraped 20 links from page 17\n",
      "Scraped 20 links from page 33\n",
      "Scraped 20 links from page 6\n",
      "Scraped 20 links from page 22\n",
      "Scraped 20 links from page 43\n",
      "Scraped 20 links from page 41\n",
      "Scraped 20 links from page 30\n",
      "Scraped 20 links from page 39\n",
      "Scraped 20 links from page 24\n",
      "Scraped 20 links from page 50\n",
      "Scraped 20 links from page 47\n",
      "Scraped 20 links from page 31\n",
      "Scraped 20 links from page 40\n",
      "Scraped 20 links from page 27\n",
      "Scraped 20 links from page 36\n",
      "Scraped 20 links from page 23\n",
      "Scraped 20 links from page 1\n",
      "Scraped 20 links from page 26\n",
      "Scraped 20 links from page 21\n",
      "Total Links: 1000\n"
     ]
    }
   ],
   "source": [
    "jumlah_index = 50\n",
    "threads_link = []\n",
    "links = []\n",
    "for page_number in range(1, jumlah_index + 1):\n",
    "    thread = threading.Thread(target=lambda p=page_number: links.extend(scrape_links(p)))\n",
    "    thread.start()\n",
    "    threads_link.append(thread)\n",
    "\n",
    "for thread in threads_link:\n",
    "    thread.join()\n",
    "print(\"Total Links:\", len(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "def scrape_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        # title =[]\n",
    "        # author = []\n",
    "        # date = []\n",
    "        # content = []\n",
    "        # category =[]\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        \n",
    "        # Judul Berita\n",
    "        title_elem = soup.find('h1', {\"class\": \"detail__title\"})\n",
    "        if title_elem:\n",
    "            title_text = title_elem.text.strip()\n",
    "        else:\n",
    "            title_text = \"Title not found\"\n",
    "        # Author berita\n",
    "        author_elem = soup.find('div', {\"class\": \"detail__author\"})\n",
    "        if author_elem:\n",
    "            author_text = author_elem.get_text()\n",
    "            author_text = author_text.split('-')[0].strip()\n",
    "        else:\n",
    "            author_text = \"Author not found\"     \n",
    "        # tanggal berita\n",
    "        date_elem = soup.find('div', {\"class\": \"detail__date\"})\n",
    "        if date_elem:\n",
    "            date_text = date_elem.text.strip()\n",
    "        else:\n",
    "            date_text = \"Date not found\"\n",
    "        #     # Category berita\n",
    "        category_elements = soup.find('div',{\"class\": \"page__breadcrumb\"})\n",
    "        if category_elements:\n",
    "            category_text = category_elements.find('a',{\"dtr-sec\": \"breadcrumbkanal\"})\n",
    "            category_text= category_text.text.strip()\n",
    "        else:\n",
    "            category_text = \"Category not found\"\n",
    "        #     # Content Berita\n",
    "        body_elem = soup.find('div', {\"class\": \"detail__body\"})\n",
    "        \n",
    "        if body_elem:\n",
    "            content_elem = body_elem.find_all('p')\n",
    "            content_text = \"\"\n",
    "            for p in content_elem:\n",
    "                content_text += p.text.strip() + \"\\n\"\n",
    "            \n",
    "            if content_text.strip():\n",
    "                content_text=content_text\n",
    "            else:\n",
    "                content_text =\"Content not found\"\n",
    "        else:\n",
    "          content_text =\"Content not found\"\n",
    "\n",
    "        results.append({'title': title_text,\n",
    "                        'author' : author_text,\n",
    "                        'category':category_text,\n",
    "                        'date': date_text,\n",
    "                        'content' : content_text,\n",
    "                        'link' : url})\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve data from https://20.detik.com/detikupdate/20230925-230925073/momen-hangat-silaturahmi-ganjar-dengan-keluarga-besar-transmedia\n"
     ]
    }
   ],
   "source": [
    "threads = []\n",
    "for url in links:\n",
    "    thread = threading.Thread(target=scrape_url, args=(url,))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PSI Angkat Giring Ganesha Jadi Dewan Pembina</td>\n",
       "      <td>Firda Cynthia Anggrainy</td>\n",
       "      <td>Pemilu</td>\n",
       "      <td>Senin, 25 Sep 2023 19:39 WIB</td>\n",
       "      <td>Giring Ganesha diangkat pada jabatan baru seba...</td>\n",
       "      <td>https://news.detik.com/pemilu/d-6950217/psi-an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kaesang Pangarep Jadi Ketum PSI!</td>\n",
       "      <td>Firda Cynthia Anggrainy, Adrial Akbar</td>\n",
       "      <td>Pemilu</td>\n",
       "      <td>Senin, 25 Sep 2023 19:43 WIB</td>\n",
       "      <td>Putra bungsu Presiden Joko Widodo (Jokowi), Ka...</td>\n",
       "      <td>https://news.detik.com/pemilu/d-6950223/kaesan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Diantar Naik Motor, Anies Datangi Baintelkam P...</td>\n",
       "      <td>Adrial Akbar</td>\n",
       "      <td>Pemilu</td>\n",
       "      <td>Senin, 25 Sep 2023 11:58 WIB</td>\n",
       "      <td>Bakal calon presiden (bacapres) Koalisi Peruba...</td>\n",
       "      <td>https://news.detik.com/pemilu/d-6949111/dianta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KPK Puji Pabrik Pengelolaan Sampah Jadi Bahan ...</td>\n",
       "      <td>Jihaan Khoirunnisaa</td>\n",
       "      <td>Berita</td>\n",
       "      <td>Senin, 25 Sep 2023 17:17 WIB</td>\n",
       "      <td>Komisi Pemberantasan Korupsi (KPK) mengapresia...</td>\n",
       "      <td>https://news.detik.com/berita/d-6949939/kpk-pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pemkot Depok Kucurkan Rp 2,7 M untuk Trotoar R...</td>\n",
       "      <td>Devi Puspitasari</td>\n",
       "      <td>Berita</td>\n",
       "      <td>Senin, 25 Sep 2023 11:30 WIB</td>\n",
       "      <td>Dinas Pekerjaan Umum dan Penataan Ruang (DPUPR...</td>\n",
       "      <td>https://news.detik.com/berita/d-6949055/pemkot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kesulitan Air, Warga Perumahan di Bekasi Ambil...</td>\n",
       "      <td>Kurniawan Fadil</td>\n",
       "      <td>Berita</td>\n",
       "      <td>Senin, 25 Sep 2023 19:43 WIB</td>\n",
       "      <td>Warga di Perumahan Bali Indah, Setiamulya, Tar...</td>\n",
       "      <td>https://news.detik.com/berita/d-6950224/kesuli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5 Fakta Demo Tutup Suramadu Protes Truk Garam ...</td>\n",
       "      <td>Tim detikJatim</td>\n",
       "      <td>Berita</td>\n",
       "      <td>Senin, 25 Sep 2023 17:30 WIB</td>\n",
       "      <td>Demo akan dilakukan sejumlah massa dari organi...</td>\n",
       "      <td>https://news.detik.com/berita/d-6949980/5-fakt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Mayat Pria Terbakar Ditemukan di Pos Area Lanu...</td>\n",
       "      <td>Wildan Noviansah</td>\n",
       "      <td>Berita</td>\n",
       "      <td>Senin, 25 Sep 2023 19:02 WIB</td>\n",
       "      <td>Seorang pria ditemukan tewas dalam kondisi ter...</td>\n",
       "      <td>https://news.detik.com/berita/d-6950156/mayat-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Pemotor Lawan Arah Tewaskan Pesepeda di Jakut ...</td>\n",
       "      <td>Wildan Noviansah</td>\n",
       "      <td>Berita</td>\n",
       "      <td>Senin, 25 Sep 2023 18:55 WIB</td>\n",
       "      <td>Ulah pemotor lawan arah di Cilincing, Jakarta ...</td>\n",
       "      <td>https://news.detik.com/berita/d-6950145/pemoto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Penampilan Siskaeee Jalani Pemeriksaan Kasus '...</td>\n",
       "      <td>Author not found</td>\n",
       "      <td>Category not found</td>\n",
       "      <td>14,459 Views  |  Senin, 25 Sep 2023 10:41 WIB</td>\n",
       "      <td>Siskaeee hadiri undangan pemeriksaan penyidik ...</td>\n",
       "      <td>https://20.detik.com/detikupdate/20230925-2309...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0       PSI Angkat Giring Ganesha Jadi Dewan Pembina   \n",
       "1                   Kaesang Pangarep Jadi Ketum PSI!   \n",
       "2  Diantar Naik Motor, Anies Datangi Baintelkam P...   \n",
       "3  KPK Puji Pabrik Pengelolaan Sampah Jadi Bahan ...   \n",
       "4  Pemkot Depok Kucurkan Rp 2,7 M untuk Trotoar R...   \n",
       "5  Kesulitan Air, Warga Perumahan di Bekasi Ambil...   \n",
       "6  5 Fakta Demo Tutup Suramadu Protes Truk Garam ...   \n",
       "7  Mayat Pria Terbakar Ditemukan di Pos Area Lanu...   \n",
       "8  Pemotor Lawan Arah Tewaskan Pesepeda di Jakut ...   \n",
       "9  Penampilan Siskaeee Jalani Pemeriksaan Kasus '...   \n",
       "\n",
       "                                  author            category  \\\n",
       "0                Firda Cynthia Anggrainy              Pemilu   \n",
       "1  Firda Cynthia Anggrainy, Adrial Akbar              Pemilu   \n",
       "2                           Adrial Akbar              Pemilu   \n",
       "3                    Jihaan Khoirunnisaa              Berita   \n",
       "4                       Devi Puspitasari              Berita   \n",
       "5                        Kurniawan Fadil              Berita   \n",
       "6                         Tim detikJatim              Berita   \n",
       "7                       Wildan Noviansah              Berita   \n",
       "8                       Wildan Noviansah              Berita   \n",
       "9                       Author not found  Category not found   \n",
       "\n",
       "                                            date  \\\n",
       "0                   Senin, 25 Sep 2023 19:39 WIB   \n",
       "1                   Senin, 25 Sep 2023 19:43 WIB   \n",
       "2                   Senin, 25 Sep 2023 11:58 WIB   \n",
       "3                   Senin, 25 Sep 2023 17:17 WIB   \n",
       "4                   Senin, 25 Sep 2023 11:30 WIB   \n",
       "5                   Senin, 25 Sep 2023 19:43 WIB   \n",
       "6                   Senin, 25 Sep 2023 17:30 WIB   \n",
       "7                   Senin, 25 Sep 2023 19:02 WIB   \n",
       "8                   Senin, 25 Sep 2023 18:55 WIB   \n",
       "9  14,459 Views  |  Senin, 25 Sep 2023 10:41 WIB   \n",
       "\n",
       "                                             content  \\\n",
       "0  Giring Ganesha diangkat pada jabatan baru seba...   \n",
       "1  Putra bungsu Presiden Joko Widodo (Jokowi), Ka...   \n",
       "2  Bakal calon presiden (bacapres) Koalisi Peruba...   \n",
       "3  Komisi Pemberantasan Korupsi (KPK) mengapresia...   \n",
       "4  Dinas Pekerjaan Umum dan Penataan Ruang (DPUPR...   \n",
       "5  Warga di Perumahan Bali Indah, Setiamulya, Tar...   \n",
       "6  Demo akan dilakukan sejumlah massa dari organi...   \n",
       "7  Seorang pria ditemukan tewas dalam kondisi ter...   \n",
       "8  Ulah pemotor lawan arah di Cilincing, Jakarta ...   \n",
       "9  Siskaeee hadiri undangan pemeriksaan penyidik ...   \n",
       "\n",
       "                                                link  \n",
       "0  https://news.detik.com/pemilu/d-6950217/psi-an...  \n",
       "1  https://news.detik.com/pemilu/d-6950223/kaesan...  \n",
       "2  https://news.detik.com/pemilu/d-6949111/dianta...  \n",
       "3  https://news.detik.com/berita/d-6949939/kpk-pu...  \n",
       "4  https://news.detik.com/berita/d-6949055/pemkot...  \n",
       "5  https://news.detik.com/berita/d-6950224/kesuli...  \n",
       "6  https://news.detik.com/berita/d-6949980/5-fakt...  \n",
       "7  https://news.detik.com/berita/d-6950156/mayat-...  \n",
       "8  https://news.detik.com/berita/d-6950145/pemoto...  \n",
       "9  https://20.detik.com/detikupdate/20230925-2309...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to ./excel/detik_2023-09-25_20-47-39.xlsx\n"
     ]
    }
   ],
   "source": [
    "# current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "# excel_file_name = f'./excel/detik_{current_datetime}.xlsx'\n",
    "# df.to_excel(excel_file_name, index=False)\n",
    "\n",
    "# print(f'Data has been saved to {excel_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links=[]\n",
    "# # jumlah Page yang diambil \n",
    "# jumlah_index = 20\n",
    "\n",
    "# for index in range(jumlah_index):\n",
    "#     url = f\"https://news.detik.com/indeks/{index+1}\"  \n",
    "#     # print(url)\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#     articles = soup.find_all('article', {\"class\": \"list-content__item\"})\n",
    "    \n",
    "#     for article in articles:\n",
    "#         link = article.find('a')['href']\n",
    "#         links.append(link)\n",
    "    \n",
    "# print(\"Total Links\",len(links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = []\n",
    "# author = []\n",
    "# date = []\n",
    "# content = []\n",
    "# category =[]\n",
    "\n",
    "# for link in links:\n",
    "#     url = link\n",
    "#     response = requests.get(url)\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "#     # Judul Berita\n",
    "#     title_elem = soup.find('h1', {\"class\": \"detail__title\"})\n",
    "#     if title_elem:\n",
    "#         title_text = title_elem.text.strip()\n",
    "#     else:\n",
    "#         title_text = \"Title not found\"\n",
    "#     title.append(title_text)\n",
    "    \n",
    "#     # Author berita\n",
    "#     author_elem = soup.find('div', {\"class\": \"detail__author\"})\n",
    "#     if author_elem:\n",
    "#         author_text = author_elem.get_text()\n",
    "#         author_text = author_text.split('-')[0].strip()\n",
    "#     else:\n",
    "#         author_text = \"Author not found\"\n",
    "#     author.append(author_text)\n",
    "    \n",
    "#     # tanggal berita\n",
    "#     date_elem = soup.find('div', {\"class\": \"detail__date\"})\n",
    "#     if date_elem:\n",
    "#         date_text = date_elem.text.strip()\n",
    "#     else:\n",
    "#         date_text = \"Date not found\"\n",
    "#     date.append(date_text)\n",
    "    \n",
    "#     # Category berita\n",
    "#     category_elements = soup.find('div',{\"class\": \"page__breadcrumb\"})\n",
    "#     if category_elements:\n",
    "#         category_text = category_elements.find('a',{\"dtr-sec\": \"breadcrumbkanal\"})\n",
    "#         category_text= category_text.text.strip()\n",
    "#     else:\n",
    "#         category_text = \"Category not found\"\n",
    "#     category.append(category_text)\n",
    "   \n",
    "    \n",
    "#     # Content Berita\n",
    "#     body_elem = soup.find('div', {\"class\": \"detail__body\"})\n",
    "    \n",
    "#     if body_elem:\n",
    "#         content_elem = body_elem.find_all('p')\n",
    "#         content_text = \"\"\n",
    "#         for p in content_elem:\n",
    "#             content_text += p.text.strip() + \"\\n\"\n",
    "        \n",
    "#         if content_text.strip():\n",
    "#             content.append(content_text)\n",
    "#         else:\n",
    "#             content.append(\"Content not found\")\n",
    "#     else:\n",
    "#         content.append(\"Content not found\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = {'title': title,\n",
    "#         'author' : author,\n",
    "#         'category':category,\n",
    "#         'date': date,\n",
    "#         'content' : content,\n",
    "#         'link' : links\n",
    "#         }\n",
    "\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to ./excel/detik_2023-09-25_20-47-40.xlsx\n"
     ]
    }
   ],
   "source": [
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "excel_file_name = f'./excel/detik_{current_datetime}.xlsx'\n",
    "df.to_excel(excel_file_name, index=False)\n",
    "\n",
    "print(f'Data has been saved to {excel_file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Latihan Pengecek dan find element\n",
    "# url = f\"https://news.detik.com/detiknews/d-6948823/siskaeee-penuhi-panggilan-polisi-soal-film-porno-ini-penampilannya\"  \n",
    "# # print(url)\n",
    "# response = requests.get(url)\n",
    "# soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "# Category\n",
    "# category_elements = soup.find('div',{\"class\": \"page__breadcrumb\"})\n",
    "# print(category_elements)\n",
    "# category_text = category_elements.find('a',{\"dtr-sec\": \"breadcrumbkanal\"})\n",
    "# print(category_text.text.strip())\n",
    "\n",
    "# ambil author detik\n",
    "# title_elem = soup.find('div', {\"class\": \"detail__author\"})\n",
    "# author_text = title_elem.get_text()\n",
    "# author_name = author_text.split('-')[0].strip()\n",
    "# print(author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrape_url(url):\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code == 200:\n",
    "#         soup = BeautifulSoup(response.content, 'html.parser')\n",
    "#     else:\n",
    "#         print(f\"Failed to retrieve data from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urls = ['https://kompas.com', 'https://kompas.com', 'https://kompas.com']\n",
    "# threads = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for url in urls:\n",
    "#     thread = threading.Thread(target=scrape_url, args=(url,))\n",
    "#     thread.start()\n",
    "#     threads.append(thread)\n",
    "\n",
    "# print(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for thread in threads:\n",
    "#     thread.join()\n",
    "    \n",
    "# print(\"All threads finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
