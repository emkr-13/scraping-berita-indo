{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Detik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mengecek Link status Ok\n",
    "url = \"https://news.detik.com/index\"\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(page_number):\n",
    "    url = f\"https://sport.detik.com/indeks/{page_number}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all('article', {\"class\": \"list-content__item\"})\n",
    "    \n",
    "    page_links = []\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        page_links.append(link)\n",
    "    \n",
    "    print(f\"Scraped {len(page_links)} links from page {page_number}\")\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 20 links from page 5\n",
      "Scraped 20 links from page 19\n",
      "Scraped 20 links from page 24\n",
      "Scraped 20 links from page 16\n",
      "Scraped 20 links from page 4\n",
      "Scraped 20 links from page 40\n",
      "Scraped 20 links from page 34\n",
      "Scraped 20 links from page 14\n",
      "Scraped 20 links from page 37\n",
      "Scraped 20 links from page 33\n",
      "Scraped 20 links from page 23\n",
      "Scraped 20 links from page 44\n",
      "Scraped 20 links from page 25\n",
      "Scraped 20 links from page 27\n",
      "Scraped 20 links from page 1\n",
      "Scraped 20 links from page 18\n",
      "Scraped 20 links from page 7\n",
      "Scraped 20 links from page 13\n",
      "Scraped 20 links from page 39\n",
      "Scraped 20 links from page 12\n",
      "Scraped 20 links from page 22\n",
      "Scraped 20 links from page 11\n",
      "Scraped 20 links from page 35\n",
      "Scraped 20 links from page 8\n",
      "Scraped 20 links from page 46\n",
      "Scraped 20 links from page 30\n",
      "Scraped 20 links from page 32\n",
      "Scraped 20 links from page 3\n",
      "Scraped 20 links from page 31\n",
      "Scraped 20 links from page 17\n",
      "Scraped 20 links from page 20\n",
      "Scraped 20 links from page 41\n",
      "Scraped 20 links from page 50\n",
      "Scraped 20 links from page 15\n",
      "Scraped 20 links from page 45\n",
      "Scraped 20 links from page 6\n",
      "Scraped 20 links from page 43\n",
      "Scraped 20 links from page 29\n",
      "Scraped 20 links from page 48\n",
      "Scraped 20 links from page 36\n",
      "Scraped 20 links from page 47\n",
      "Scraped 20 links from page 26\n",
      "Scraped 20 links from page 42\n",
      "Scraped 20 links from page 49\n",
      "Scraped 20 links from page 9\n",
      "Scraped 20 links from page 21\n",
      "Scraped 20 links from page 10\n",
      "Scraped 20 links from page 2\n",
      "Scraped 20 links from page 38\n",
      "Scraped 20 links from page 28\n",
      "Total Links: 1000\n"
     ]
    }
   ],
   "source": [
    "jumlah_index = 50\n",
    "threads_link = []\n",
    "links = []\n",
    "for page_number in range(1, jumlah_index + 1):\n",
    "    thread = threading.Thread(target=lambda p=page_number: links.extend(scrape_links(p)))\n",
    "    thread.start()\n",
    "    threads_link.append(thread)\n",
    "\n",
    "for thread in threads_link:\n",
    "    thread.join()\n",
    "print(\"Total Links:\", len(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "# def scrape_url(url):\n",
    "#     response = requests.get(url)\n",
    "#     if response.status_code == 200:\n",
    "#         # title =[]\n",
    "#         # author = []\n",
    "#         # date = []\n",
    "#         # content = []\n",
    "#         # category =[]\n",
    "#         soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        \n",
    "#         # Judul Berita\n",
    "#         title_elem = soup.find('h1', {\"class\": \"detail__title\"})\n",
    "#         if title_elem:\n",
    "#             title_text = title_elem.text.strip()\n",
    "#         else:\n",
    "#             title_text = \"Title not found\"\n",
    "#         # Author berita\n",
    "#         author_elem = soup.find('div', {\"class\": \"detail__author\"})\n",
    "#         if author_elem:\n",
    "#             author_text = author_elem.get_text()\n",
    "#             author_text = author_text.split('-')[0].strip()\n",
    "#         else:\n",
    "#             author_text = \"Author not found\"     \n",
    "#         # tanggal berita\n",
    "#         date_elem = soup.find('div', {\"class\": \"detail__date\"})\n",
    "#         if date_elem:\n",
    "#             date_text = date_elem.text.strip()\n",
    "#         else:\n",
    "#             date_text = \"Date not found\"\n",
    "#         #     # Category berita\n",
    "#         category_elements = soup.find('div',{\"class\": \"page__breadcrumb\"})\n",
    "#         if category_elements:\n",
    "#             category_text = category_elements.find('a',{\"dtr-sec\": \"breadcrumbkanal\"})\n",
    "#             category_text= category_text.text.strip()\n",
    "#         else:\n",
    "#             category_text = \"Category not found\"\n",
    "#         #     # Content Berita\n",
    "#         body_elem = soup.find('div', {\"class\": \"detail__body\"})\n",
    "        \n",
    "#         if body_elem:\n",
    "#             content_elem = body_elem.find_all('p')\n",
    "#             content_text = \"\"\n",
    "#             for p in content_elem:\n",
    "#                 content_text += p.text.strip() + \"\\n\"\n",
    "            \n",
    "#             if content_text.strip():\n",
    "#                 content_text=content_text\n",
    "#             else:\n",
    "#                 content_text =\"Content not found\"\n",
    "#         else:\n",
    "#           content_text =\"Content not found\"\n",
    "\n",
    "#         results.append({'judul_berita': title_text,\n",
    "#                         'penulis' : author_text,\n",
    "#                         'kategori_berita':category_text,\n",
    "#                         'tanggal_berita': date_text,\n",
    "#                         'isi_berita' : content_text,\n",
    "#                         'link_berita' : url})\n",
    "#     else:\n",
    "#         print(f\"Failed to retrieve data from {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url):\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                    # Extract data from the web page\n",
    "                    title_elem = soup.find('h1', {\"class\": \"detail__title\"})\n",
    "                    title_text = title_elem.text.strip() if title_elem else \"Title not found\"\n",
    "\n",
    "                    author_elem = soup.find('div', {\"class\": \"detail__author\"})\n",
    "                    author_text = author_elem.get_text().split('-')[0].strip() if author_elem else \"Author not found\"\n",
    "\n",
    "                    date_elem = soup.find('div', {\"class\": \"detail__date\"})\n",
    "                    date_text = date_elem.text.strip() if date_elem else \"Date not found\"\n",
    "\n",
    "                    category_elements = soup.find('div', {\"class\": \"page__breadcrumb\"})\n",
    "                    category_text = category_elements.find('a', {\"dtr-sec\": \"breadcrumbkanal\"}).text.strip() if category_elements else \"Category not found\"\n",
    "\n",
    "                    body_elem = soup.find('div', {\"class\": \"detail__body\"})\n",
    "                    if body_elem:\n",
    "                        content_elem = body_elem.find_all('p')\n",
    "                        content_text = \"\\n\".join(p.text.strip() for p in content_elem)\n",
    "                        content_text = content_text.replace('\\n', ' ')\n",
    "                    else:\n",
    "                        content_text = \"Content not found\"\n",
    "\n",
    "                    nama_berita_match = re.search(r'https://(?:www\\.)?([a-zA-Z0-9.-]+)\\.com', url)\n",
    "                    nama_berita = nama_berita_match.group(1) if nama_berita_match else \"Nama_berita not found\"\n",
    "\n",
    "                    results.append({'judul_berita': title_text,\n",
    "                                            'penulis' : author_text,\n",
    "                                            'kategori_berita':category_text,\n",
    "                                            'tanggal_berita': date_text,\n",
    "                                            'isi_berita' : content_text,\n",
    "                                            'link_berita' : url})\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = []\n",
    "for url in links:\n",
    "    thread = threading.Thread(target=scrape_url, args=(url,))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "    \n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hasil scrapping 3000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>judul_berita</th>\n",
       "      <th>penulis</th>\n",
       "      <th>kategori_berita</th>\n",
       "      <th>tanggal_berita</th>\n",
       "      <th>isi_berita</th>\n",
       "      <th>link_berita</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Marc Marquez Gagal Finis Lagi di Mandalika</td>\n",
       "      <td>Author not found</td>\n",
       "      <td>Category not found</td>\n",
       "      <td>20,017 Views  |  Minggu, 15 Okt 2023 16:43 WIB</td>\n",
       "      <td>Marc Marquez kembali gagal finis di MotoGP Man...</td>\n",
       "      <td>https://20.detik.com/detikupdate/20231015-2310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Siap Rebut Emas! Ini Daftar Pebulutangkis RI N...</td>\n",
       "      <td>Mercy Raya</td>\n",
       "      <td>Raket</td>\n",
       "      <td>Rabu, 27 Sep 2023 14:10 WIB</td>\n",
       "      <td>PP PBSI merilis skuad Indonesia yang akan berl...</td>\n",
       "      <td>https://sport.detik.com/raket/d-6953098/siap-r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pemerintah Buka Peluang Sirkuit Mandalika Jadi...</td>\n",
       "      <td>Author not found</td>\n",
       "      <td>Category not found</td>\n",
       "      <td>66,517 Views  |  Selasa, 17 Okt 2023 20:20 WIB</td>\n",
       "      <td>Pemerintah tidak menutup kemungkinan sirkuit M...</td>\n",
       "      <td>https://20.detik.com/detikupdate/20231017-2310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kata Marc Marquez Setelah Lagi-lagi Gagal Fini...</td>\n",
       "      <td>Author not found</td>\n",
       "      <td>Category not found</td>\n",
       "      <td>13,750 Views  |  Senin, 16 Okt 2023 06:18 WIB</td>\n",
       "      <td>Marc Marquez gagal finis di MotoGP Mandalika, ...</td>\n",
       "      <td>https://20.detik.com/detikupdate/20231016-2310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Basuki Hadimuljono Kasih Bonus Tambahan ke Per...</td>\n",
       "      <td>Mercy Raya</td>\n",
       "      <td>Sport Lain</td>\n",
       "      <td>Rabu, 27 Sep 2023 10:30 WIB</td>\n",
       "      <td>Chef de Mission Basuki Hadimuljono gembira atl...</td>\n",
       "      <td>https://sport.detik.com/sport-lain/d-6952629/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Harapan Christian Hadinata buat Fajar/Rian: Ma...</td>\n",
       "      <td>Mercy Raya</td>\n",
       "      <td>Raket</td>\n",
       "      <td>Kamis, 12 Okt 2023 14:55 WIB</td>\n",
       "      <td>Performa ganda putra Indonesia Fajar Alfian/Mu...</td>\n",
       "      <td>https://sport.detik.com/raket/d-6978601/harapa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The Daddies dan Pram-Yere Lolos ke Perempat Fi...</td>\n",
       "      <td>Author not found</td>\n",
       "      <td>Category not found</td>\n",
       "      <td>8,691 Views  |  Jumat, 13 Okt 2023 04:20 WIB</td>\n",
       "      <td>Dua pasangan ganda putra Indonesia raih kemena...</td>\n",
       "      <td>https://20.detik.com/detikupdate/20231013-2310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Harris Horatius Sumbang Emas di Asian Games, M...</td>\n",
       "      <td>Randy Prasatya</td>\n",
       "      <td>Sport Lain</td>\n",
       "      <td>Rabu, 27 Sep 2023 01:31 WIB</td>\n",
       "      <td>Harris Horatius menyumbang medali emas untuk I...</td>\n",
       "      <td>https://sport.detik.com/sport-lain/d-6952381/h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Plot Twist Martin di Mandalika: Start Bak Roke...</td>\n",
       "      <td>Author not found</td>\n",
       "      <td>Category not found</td>\n",
       "      <td>12,934 Views  |  Minggu, 15 Okt 2023 16:36 WIB</td>\n",
       "      <td>Jorge Martin start dari grid 6 dan melesat bag...</td>\n",
       "      <td>https://20.detik.com/detikupdate/20231015-2310...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sanggoe Sabet Medali Perak Skateboard Asian Ga...</td>\n",
       "      <td>Mercy Raya</td>\n",
       "      <td>Sport Lain</td>\n",
       "      <td>Rabu, 27 Sep 2023 17:35 WIB</td>\n",
       "      <td>Atlet skateboard Sanggoe Darma Tanjung raih me...</td>\n",
       "      <td>https://sport.detik.com/sport-lain/d-6953625/s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        judul_berita           penulis  \\\n",
       "0         Marc Marquez Gagal Finis Lagi di Mandalika  Author not found   \n",
       "1  Siap Rebut Emas! Ini Daftar Pebulutangkis RI N...        Mercy Raya   \n",
       "2  Pemerintah Buka Peluang Sirkuit Mandalika Jadi...  Author not found   \n",
       "3  Kata Marc Marquez Setelah Lagi-lagi Gagal Fini...  Author not found   \n",
       "4  Basuki Hadimuljono Kasih Bonus Tambahan ke Per...        Mercy Raya   \n",
       "5  Harapan Christian Hadinata buat Fajar/Rian: Ma...        Mercy Raya   \n",
       "6  The Daddies dan Pram-Yere Lolos ke Perempat Fi...  Author not found   \n",
       "7  Harris Horatius Sumbang Emas di Asian Games, M...    Randy Prasatya   \n",
       "8  Plot Twist Martin di Mandalika: Start Bak Roke...  Author not found   \n",
       "9  Sanggoe Sabet Medali Perak Skateboard Asian Ga...        Mercy Raya   \n",
       "\n",
       "      kategori_berita                                  tanggal_berita  \\\n",
       "0  Category not found  20,017 Views  |  Minggu, 15 Okt 2023 16:43 WIB   \n",
       "1               Raket                     Rabu, 27 Sep 2023 14:10 WIB   \n",
       "2  Category not found  66,517 Views  |  Selasa, 17 Okt 2023 20:20 WIB   \n",
       "3  Category not found   13,750 Views  |  Senin, 16 Okt 2023 06:18 WIB   \n",
       "4          Sport Lain                     Rabu, 27 Sep 2023 10:30 WIB   \n",
       "5               Raket                    Kamis, 12 Okt 2023 14:55 WIB   \n",
       "6  Category not found    8,691 Views  |  Jumat, 13 Okt 2023 04:20 WIB   \n",
       "7          Sport Lain                     Rabu, 27 Sep 2023 01:31 WIB   \n",
       "8  Category not found  12,934 Views  |  Minggu, 15 Okt 2023 16:36 WIB   \n",
       "9          Sport Lain                     Rabu, 27 Sep 2023 17:35 WIB   \n",
       "\n",
       "                                          isi_berita  \\\n",
       "0  Marc Marquez kembali gagal finis di MotoGP Man...   \n",
       "1  PP PBSI merilis skuad Indonesia yang akan berl...   \n",
       "2  Pemerintah tidak menutup kemungkinan sirkuit M...   \n",
       "3  Marc Marquez gagal finis di MotoGP Mandalika, ...   \n",
       "4  Chef de Mission Basuki Hadimuljono gembira atl...   \n",
       "5  Performa ganda putra Indonesia Fajar Alfian/Mu...   \n",
       "6  Dua pasangan ganda putra Indonesia raih kemena...   \n",
       "7  Harris Horatius menyumbang medali emas untuk I...   \n",
       "8  Jorge Martin start dari grid 6 dan melesat bag...   \n",
       "9  Atlet skateboard Sanggoe Darma Tanjung raih me...   \n",
       "\n",
       "                                         link_berita  \n",
       "0  https://20.detik.com/detikupdate/20231015-2310...  \n",
       "1  https://sport.detik.com/raket/d-6953098/siap-r...  \n",
       "2  https://20.detik.com/detikupdate/20231017-2310...  \n",
       "3  https://20.detik.com/detikupdate/20231016-2310...  \n",
       "4  https://sport.detik.com/sport-lain/d-6952629/b...  \n",
       "5  https://sport.detik.com/raket/d-6978601/harapa...  \n",
       "6  https://20.detik.com/detikupdate/20231013-2310...  \n",
       "7  https://sport.detik.com/sport-lain/d-6952381/h...  \n",
       "8  https://20.detik.com/detikupdate/20231015-2310...  \n",
       "9  https://sport.detik.com/sport-lain/d-6953625/s...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(results)\n",
    "print('hasil scrapping',len(results))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to ../tempat_simpan_nasional/detik_sport_2023-10-21_11-12-38.xlsx\n"
     ]
    }
   ],
   "source": [
    "current_datetime = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "excel_file_name = f'../tempat_simpan_nasional/detik_sport_{current_datetime}.xlsx'\n",
    "df.to_excel(excel_file_name, index=False)\n",
    "\n",
    "print(f'Data has been saved to {excel_file_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
