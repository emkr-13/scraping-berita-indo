{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import concurrent.futures\n",
    "from selenium.common.exceptions import StaleElementReferenceException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"keywords\": \"jokowi\",\n",
    "    \"since_time\": \"2023-10-16\",\n",
    "    \"until_time\": \"2023-10-19\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pagination(keywords, since_time, until_time):\n",
    "    current_date = datetime.strptime(since_time, \"%Y-%m-%d\")\n",
    "    until_date = datetime.strptime(until_time, \"%Y-%m-%d\")\n",
    "    result_data = []  # Membuat list kosong untuk menyimpan data\n",
    "    \n",
    "    while current_date <= until_date:\n",
    "        formatted_date = current_date.strftime(\"%d/%m/%Y\")\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "        }\n",
    "        url = f\"https://www.detik.com/search/searchall?query={keywords}&siteid=2&sortby=time&fromdatex={formatted_date}&todatex={formatted_date}&page=1\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        cek = soup.find('span', {\"class\": \"fl text\"})\n",
    "        \n",
    "        if cek:\n",
    "            hasil_text = cek.get_text()\n",
    "            result = re.search(r'\\d+', hasil_text)\n",
    "            hasil_number = int(result.group(0))\n",
    "            hasil_divided = hasil_number / 9\n",
    "            page_index = round(hasil_divided)\n",
    "            # print(formatted_date)\n",
    "            result_data.append({\n",
    "                'keywords':keywords,\n",
    "                'tanggal_berita': formatted_date,\n",
    "                'jumlah_index': page_index\n",
    "            })\n",
    "        else:\n",
    "            print(f\"No data found for {formatted_date}\")\n",
    "        current_date += timedelta(days=1)\n",
    "    \n",
    "    return result_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'keywords': 'jokowi', 'tanggal_berita': '16/10/2023', 'jumlah_index': 8}, {'keywords': 'jokowi', 'tanggal_berita': '17/10/2023', 'jumlah_index': 11}, {'keywords': 'jokowi', 'tanggal_berita': '18/10/2023', 'jumlah_index': 11}, {'keywords': 'jokowi', 'tanggal_berita': '19/10/2023', 'jumlah_index': 8}]\n"
     ]
    }
   ],
   "source": [
    "data_tanggal = scrape_pagination(data[\"keywords\"], data[\"since_time\"], data[\"until_time\"])\n",
    "print(data_tanggal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(page_number, keywords, date, link_list):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = f\"https://www.detik.com/search/searchall?query={keywords}&siteid=2&sortby=time&fromdatex={date}&todatex={date}&page={page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    articles = soup.find_all('article')\n",
    "\n",
    "    page_links = []\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        page_links.append(link)\n",
    "\n",
    "    print(f\"Scraped {len(page_links)} links from page {page_number} cek tanggal {date}\")\n",
    "    \n",
    "    # Extend the link_list with the links scraped on this page\n",
    "    link_list.extend(page_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_links(data_tanggal):\n",
    "    link_list = []  # Create an empty list to store all the links\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:  # You can adjust the number of threads as needed\n",
    "        for item in data_tanggal:\n",
    "            keywords = item['keywords']\n",
    "            date = item['tanggal_berita']\n",
    "            max_pages = item['jumlah_index']\n",
    "            for page_number in range(1, max_pages+1):  # Specify the range of pages you want to scrape\n",
    "                executor.submit(scrape_links, page_number, keywords, date, link_list)\n",
    "    \n",
    "    return link_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 9 links from page 2 cek tanggal 16/10/2023\n",
      "Scraped 9 links from page 3 cek tanggal 16/10/2023\n",
      "Scraped 9 links from page 5 cek tanggal 16/10/2023\n",
      "Scraped 9 links from page 4 cek tanggal 16/10/2023\n",
      "Scraped 9 links from page 6 cek tanggal 16/10/2023\n",
      "Scraped 9 links from page 1 cek tanggal 16/10/2023\n",
      "Scraped 9 links from page 1 cek tanggal 17/10/2023\n",
      "Scraped 9 links from page 2 cek tanggal 17/10/2023\n",
      "Scraped 9 links from page 3 cek tanggal 17/10/2023\n",
      "Scraped 9 links from page 5 cek tanggal 17/10/2023\n",
      "Scraped 9 links from page 7 cek tanggal 16/10/2023\n",
      "Scraped 9 links from page 8 cek tanggal 16/10/2023\n",
      "Scraped 9 links from page 8 cek tanggal 17/10/2023\n",
      "Scraped 9 links from page 4 cek tanggal 17/10/2023\n",
      "Scraped 9 links from page 10 cek tanggal 17/10/2023\n",
      "Scraped 9 links from page 6 cek tanggal 17/10/2023\n",
      "Scraped 9 links from page 7 cek tanggal 17/10/2023\n",
      "Scraped 9 links from page 1 cek tanggal 18/10/2023\n",
      "Scraped 9 links from page 3 cek tanggal 18/10/2023\n",
      "Scraped 9 links from page 2 cek tanggal 18/10/2023\n",
      "Scraped 9 links from page 4 cek tanggal 18/10/2023\n",
      "Scraped 9 links from page 9 cek tanggal 17/10/2023\n",
      "Scraped 9 links from page 6 cek tanggal 18/10/2023\n",
      "Scraped 4 links from page 11 cek tanggal 17/10/2023\n",
      "Scraped 9 links from page 8 cek tanggal 18/10/2023\n",
      "Scraped 9 links from page 9 cek tanggal 18/10/2023\n",
      "Scraped 9 links from page 7 cek tanggal 18/10/2023\n",
      "Scraped 9 links from page 11 cek tanggal 18/10/2023\n",
      "Scraped 9 links from page 5 cek tanggal 18/10/2023\n",
      "Scraped 9 links from page 10 cek tanggal 18/10/2023\n",
      "Scraped 9 links from page 1 cek tanggal 19/10/2023\n",
      "Scraped 9 links from page 5 cek tanggal 19/10/2023\n",
      "Scraped 9 links from page 2 cek tanggal 19/10/2023\n",
      "Scraped 9 links from page 6 cek tanggal 19/10/2023\n",
      "Scraped 9 links from page 4 cek tanggal 19/10/2023\n",
      "Scraped 9 links from page 7 cek tanggal 19/10/2023\n",
      "Scraped 9 links from page 3 cek tanggal 19/10/2023\n",
      "Scraped 9 links from page 8 cek tanggal 19/10/2023\n",
      "337\n"
     ]
    }
   ],
   "source": [
    "all_links = scrape_all_links(data_tanggal)\n",
    "print(len(all_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
