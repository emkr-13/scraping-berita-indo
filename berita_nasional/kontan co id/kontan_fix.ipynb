{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import threading\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Day: 01\n",
      "Month: 11\n",
      "Year: 2023\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "date = '2023-11-01'\n",
    "format_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "day = format_date.strftime(\"%d\")\n",
    "month = format_date.strftime(\"%m\")\n",
    "year = format_date.strftime(\"%Y\")\n",
    "\n",
    "print(\"Day:\", day)\n",
    "print(\"Month:\", month)\n",
    "print(\"Year:\", year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find the 'list-berita' div.\n"
     ]
    }
   ],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "url = \"https://www.kontan.co.id/search/indeks?kanal=nasional&tanggal=01&bulan=11&tahun=2023&pos=indeks&per_page=60\"\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Check if 'list-berita' div is found\n",
    "body = soup.find('div', {\"class\": \"list-berita\"})\n",
    "if body:\n",
    "    # If found, proceed with finding 'li' elements\n",
    "    articles = body.find_all('li')\n",
    "\n",
    "    for article in articles:\n",
    "        link = article.find('a')['href']\n",
    "        print(link)\n",
    "else:\n",
    "    print(\"Couldn't find the 'list-berita' div.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(date, page_number):\n",
    "    format_date = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "    day = format_date.strftime(\"%d\")\n",
    "    month = format_date.strftime(\"%m\")\n",
    "    year = format_date.strftime(\"%Y\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "    }\n",
    "    url = f\"https://www.kontan.co.id/search/indeks?kanal=nasional&tanggal={day}&bulan={month}&tahun={year}&pos=indeks&per_page={page_number}\"\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    body = soup.find('div', {\"class\": \"list-berita\"})\n",
    "    links = []  # Initialize links outside the if block\n",
    "\n",
    "    if body:\n",
    "        articles = body.find_all('li')\n",
    "        for article in articles:\n",
    "            link = article.find('a')['href']\n",
    "            links.append(link)\n",
    "    else:\n",
    "        print(\"Couldn't find the 'list-berita' div.\")\n",
    "\n",
    "    print(f\"Scraped {len(links)} links from page {page_number} url {url}\")\n",
    "    return links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_link_per_day(date, max_threads=5):\n",
    "    page_number = 0\n",
    "    page_links = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_threads) as executor:\n",
    "        futures = []\n",
    "\n",
    "        while True:\n",
    "            future = executor.submit(scrape_links, date, page_number)\n",
    "            futures.append(future)\n",
    "            page_number += 20\n",
    "\n",
    "            # Break the loop if no more articles are found\n",
    "            if not future.result():\n",
    "                break\n",
    "\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            page_links.extend(future.result())\n",
    "\n",
    "    return page_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 20 links from page 0 url https://www.kontan.co.id/search/indeks?kanal=nasional&tanggal=01&bulan=11&tahun=2023&pos=indeks&per_page=0\n",
      "Scraped 20 links from page 20 url https://www.kontan.co.id/search/indeks?kanal=nasional&tanggal=01&bulan=11&tahun=2023&pos=indeks&per_page=20\n",
      "Scraped 8 links from page 40 url https://www.kontan.co.id/search/indeks?kanal=nasional&tanggal=01&bulan=11&tahun=2023&pos=indeks&per_page=40\n",
      "Couldn't find the 'list-berita' div.\n",
      "Scraped 0 links from page 60 url https://www.kontan.co.id/search/indeks?kanal=nasional&tanggal=01&bulan=11&tahun=2023&pos=indeks&per_page=60\n"
     ]
    }
   ],
   "source": [
    "link=scrape_link_per_day('2023-11-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(url,max_retries=2):\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "            try:\n",
    "                headers = {\n",
    "                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'\n",
    "                }\n",
    "                response = requests.get(url, headers=headers)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "                    \n",
    "                    \n",
    "                    # Judul Berita\n",
    "                    title_elem = soup.find('h1', {\"class\": \"detail-desk\"})\n",
    "                    if title_elem:\n",
    "                        title_text = title_elem.text.strip()\n",
    "                    else:\n",
    "                        title_text = \"Title not found\"  \n",
    "                    # tanggal berita\n",
    "                    date_elem = soup.find('div', {\"class\": \"fs14 ff-opensans font-gray\"})\n",
    "                    # print(date_elem)\n",
    "                    if date_elem:\n",
    "                        date_text= date_elem.text.strip()\n",
    "                        month_mapping = {\n",
    "                            'Januari': '01',\n",
    "                            'Februari': '02',\n",
    "                            'Maret': '03',\n",
    "                            'April': '04',\n",
    "                            'Mei': '05',\n",
    "                            'Juni': '06',\n",
    "                            'Juli': '07',\n",
    "                            'Agustus': '08',\n",
    "                            'September': '09',\n",
    "                            'Oktober': '10',\n",
    "                            'November': '11',\n",
    "                            'Desember': '12'\n",
    "                        }\n",
    "\n",
    "                        # Memisahkan string tanggal dan waktu\n",
    "                        date_time_parts = date_text.split('/')[0].strip().split()\n",
    "                        day = date_time_parts[1]\n",
    "                        month_name = date_time_parts[2]\n",
    "                        year = date_time_parts[3]\n",
    "\n",
    "                        # Mengonversi nama bulan menjadi angka menggunakan kamus\n",
    "                        month = month_mapping[month_name]\n",
    "\n",
    "                        # Membentuk string tanggal yang diformat\n",
    "                        formatted_date = f'{year}-{month}-{day}'\n",
    "\n",
    "                        # date_text = date_text\n",
    "                    else:\n",
    "                        date_text = \"Date not found\"\n",
    "                    #     # Content Berita\n",
    "                    body_elem = soup.find('div', {\"class\": \"tmpt-desk-kon\"})\n",
    "                        \n",
    "                    if body_elem:\n",
    "                        content_elem = body_elem.find_all('p')\n",
    "                        content_text = \"\"\n",
    "                        for p in content_elem:\n",
    "                                content_text += p.text.strip() + \"\\n\"\n",
    "                            \n",
    "                        if content_text.strip():\n",
    "                            content_text=content_text\n",
    "                            content_text = content_text.replace('\\n', '').replace('\\r', '').replace('\\t', '')\n",
    "                            content_text = ' '.join(content_text.split())\n",
    "                        else:\n",
    "                            content_text=\"Content not found\"\n",
    "                    else:\n",
    "                            content_text=\"Content not found\"\n",
    "\n",
    "                    return{\n",
    "                        'title': title_text,\n",
    "                        'date': formatted_date,\n",
    "                        'content':content_text,\n",
    "                        'link' : url}\n",
    "                elif response.status_code == 429:\n",
    "                    print(f\"Received a 429 error for {url}. Retrying in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    print(f\"Failed to retrieve data from {url}: Status Code {response.status_code}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Error fetching URL '{url}': {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing URL '{url}': {e}\")\n",
    "            retries += 1\n",
    "            if retries < max_retries:\n",
    "                print(f\"Retrying {url} (Attempt {retries}/{max_retries})\")\n",
    "                time.sleep(5)  # You can adjust the delay as needed\n",
    "    return None         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://nasional.kontan.co.id/news/ini-data-dan-informasi-yang-harus-disetorkan-pmse-ke-bps'\n",
    "data_kontan = scrape_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Ini Data dan Informasi yang Harus Disetorkan PMSE ke BPS',\n",
       " 'date': '2023-11-01',\n",
       " 'content': 'Reporter: Bidara Pink | Editor: Khomarul HidayatKONTAN.CO.ID - JAKARTA. Makin berkembangnya transaksi dan ekonomi digital, Badan Pusat Statistik (BPS) pun meluncurkan aturan baru terkait dengan Perdagangan Melalui Sistem Elektronik (PMSE).Ini tertuang dalam Peraturan BPS Nomor 4 Tahun 2023 tentang Penyampaian dan Pengelolaan Data dan/atau Informasi PMSE.Dengan adanya beleid tersebut, maka pelaku PMSE wajib lapor kepada BPS mengenai sejumlah data atau informasi dari perusahaannya secara berkala.Dalam Pasal 4 ayat (1) tertulis, data yang wajib disetor kepada otoritas statistik adalah keterangan umum perusahaan, tenaga kerja, pendapatan dan pengeluaran.Pelaku PMSE juga wajib setor informasi kategori produk, kategori wilayah, transaksi, metode pembayaran, juga jumlah penjual dan pembeli.Dalam Pasal 6 ayat (1) kemudian dijelaskan, bahwa para pelaku PMSE bisa menggunakan moda penyampaian data dalam beberapa bentuk.Seperti, formulir elektronik, mengunggah berkas, pertukaran data menggunakan mesin yang harus disertai metadata, hingga langsung berkunjung ke kantor BPS.Bila para pelaku PMSE tidak melakukan pelaporan sesuai ketentuan, maka akan ada sanksi administratif yang menanti.Baca Juga: BPS Wajibkan E-Commerce Lapor Data, IdEA: Kita Akan punya Data yang Lebih RiilDengan adanya ketentuan tersebut, BPS menjamin kalau data dan informasi yang disampaikan akan dijaga kerahasiaannya sesuai dengan ketentuan peraturan perundang-undangan.Dalam peluncuran beleid tersebut, Pelaksana Tugas (Plt.) Kepala BPS Amalia Adininggar mengungkapkan, langkah otoritas statistik tersebut seiring dengan perkembangan terkini, yaitu transaksi digital yang makin membengkak.Amalia memandang, tren tersebut memberikan sebuah kebutuhan bagi otoritas untuk memiliki data yang akurat terkait dengan transaksi elektronik secara komprehensif.\"Karena penting bagi pemerintah untuk bisa merumuskan berbagai kebijakan, berbasis danta dan fakta yang akurat,\" terang Amalia dalam Sosialisasi Peraturan BPS No. 4 Tahun 2023, Senin (30/10) di Jakarta.Sebagai otoritas statistik, BPS yakin adanya pengumpulan data yang lebih komprehensif ini akan membuat pemerintah bisa menelurkan kebijakan yang lebih konkrit untuk pelaku PMSE juga para konsumen.Dengan demikian, akan bermuara pada penguatan perkembangan ekonomi di masa depan, mengingat kontribusi ekonomi digital pada perekonomian cukup terlihat.Amalia menambahkan, seiring dengan peluncuran Peraturan BPS ini, otoritas statistik akan segera mengeluarkan keputusan Kepala BPS yang berisi petunjuk teknis dalam melaksanakan peraturan tersebut.Untuk saat ini, ada uji coba penyampaian dan pengelolaan data dan/atau informasi PMSE yang dilaksanakan paling lama enam bulan terhitung sejak peraturan tersebut diundangkan.Baca Juga: Tak Setor Data ke BPS, E-commerce Akan Kena Sanksi Sampai PemblokiranCek Berita dan Artikel yang lain di Google News',\n",
       " 'link': 'https://nasional.kontan.co.id/news/ini-data-dan-informasi-yang-harus-disetorkan-pmse-ke-bps'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kontan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
